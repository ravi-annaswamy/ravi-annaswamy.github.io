===============
50 years ago, screwworm flies inspired a new approach to insect control
Screwworms, the first pest to be eliminated on a large scale by the use of the sterile male technique, have shown an alarming increase, according to U.S. and Mexican officials…. The screwworm fly lays its eggs in open wounds on cattle. The maggots live on the flesh of their host, causing damage and death, and economic losses of many millions of dollars.

— Science News, November 23, 1968

Update

Though eradicated in the United States in 1966, screwworms reemerged two years later, probably coming up from Mexico. Outbreaks in southern U.S. states in 1972 and in Florida in 2016 were both handled with the sterile male technique, considered one of the most successful approaches for pest control. Males are sterilized with radiation, then released into a population to breed with wild counterparts; no offspring result. The method has been used with other pests, such as mosquitoes, which were dropped by drones over Brazil this year as a test before the technology is used against outbreaks like the Zika virus.
===============
This huge plant-eater thrived in the age of dinosaurs — but wasn’t one of them
A new species of hulking ancient herbivore would have overshadowed its relatives.

Fossils found in Poland belong to a new species that roamed during the Late Triassic, a period some 237 million to 201 million years ago, researchers report November 22 in Science. But unlike most of the enormous animals who lived during that time period, this new creature isn’t a dinosaur — it’s a dicynodont.

Dicynodonts are a group of ancient four-legged animals that are related to mammals’ ancestors. They’re a diverse group, but the new species is far larger than any other dicynodont found to date. The elephant-sized creature was more than 4.5 meters long and probably weighed about 9 tons, the researchers estimate. Related animals didn’t become that big again until the Eocene, 150 million years later.

“We think it’s one of the most unexpected fossil discoveries from the Triassic of Europe,” says study coauthor Grzegorz Niedzwiedzki, a paleontologist at Uppsala University in Sweden. “Who would have ever thought that there is a fossil record of such a giant, elephant-sized mammal cousin in this part of the world?” He and his team first described some of the bones in 2008; now they’ve made the new species — Lisowicia bojani — official.

The creature had upright forelimbs like today’s rhinoceroses and hippos, instead of the splayed front limbs seen on other Triassic dicynodonts, which were similar to the forelimbs of present-day lizards. That posture would have helped it support its massive bodyweight.
===============
An orbiter glitch may mean some signs of liquid water on Mars aren’t real
Some signs of water on Mars may have just dried up.

Thanks to the way data from NASA’s Mars Reconnaissance Orbiter are handled, the spacecraft may be seeing signs of hydrated salts that aren’t really there, planetary scientists report online November 9 in Geophysical Research Letters.

That lack of salts could mean that certain sites proposed as places where life could exist on Mars today, including purported streaks of liquid water on the walls of Martian craters, are probably dry and lifeless.

“People think these environments might be inhabitable by microbes,” says planetary scientist Ellen Leask of Caltech. But “there might not actually be any real evidence for it,” at least not from orbit.

Leask and her colleagues found the problem while searching for hydrated salts called perchlorates in maps of Mars taken by the orbiter’s Compact Reconnaissance Imaging Spectrometer for Mars, or CRISM. Perchlorates can lower the freezing point of water by up to 80 degrees Celsius, which could be enough to melt ice in the frigid Martian climate.

Both the Phoenix Mars lander (SN: 4/11/09, p. 12) and the Curiosity rover have detected tiny amounts of perchlorates in Martian soil (SN Online: 9/26/13). “Finding perchlorate was a big deal, because it’s a way to really make liquid water on Mars,” says planetary scientist Bethany Ehlmann of Caltech.

To see if the salts showed up in other locations on Mars, scientists turned to CRISM’s chemical maps, which show how light reflects off of the Martian surface in hundreds of wavelengths. The resulting spectra allow scientists to identify specific minerals on the surface based on the ways those minerals absorb or alter the light.

In 2015, planetary scientist Lujendra Ojha, now of Johns Hopkins University, and colleagues made headlines when the team reported spotting perchlorates in ephemeral dark streaks on Martian slopes using CRISM data. The results were widely interpreted as a sign that salty liquid water flows on Mars today (SN: 10/31/15, p. 17).

CRISM’s camera doesn’t work perfectly, though. It can be thrown off by boundaries between light and dark, like a shadowed region at the edge of a cliff. Some pixels in the orbiter’s camera take a fraction of a millisecond to realize the surface color changed, so they record an extra spot of light or dark where it shouldn’t be. Planetary scientists have software to correct for these “spikes” in the spectra and make the data more reliable and easier to read.

But the correction sometimes introduces dips in the spectra at the same wavelengths as perchlorates, Leask and Ehlmann and their colleagues found. “We cleverly created a way to get rid of the spiky noise,” Ehlmann says. “But for 0.05 percent of the pixels, it smooths out in a way that looks like perchlorate.”

The researchers found the glitch while looking for small signs of the salts in CRISM images. Assuming that the orbiter would have already spotted large deposits of perchlorates if they existed, the team wrote an algorithm to find smaller traces that covered fewer than 10 pixels in a CRISM image. And the scientists started seeing perchlorates everywhere, including Jezero crater, which NASA announced was selected as the landing site for the Mars 2020 rover on November 19 (SN Online: 11/19/18).

“We were like, oh my gosh!” Ehlmann says. If Mars 2020 lands near a potentially habitable environment, the rover team would have to sterilize the spacecraft more stringently, to avoid accidentally poisoning the water with Earth microbes (SN: 1/20/18, p. 22). “You don’t want to send your dirty spacecraft and kill all the Mars life.”

But on closer inspection, Leask and her colleagues noticed that perchlorates seemed to be showing up in places where it made no geologic sense for the salts to form — and especially along the boundaries between light and dark surfaces. That made the team suspect that the spike-smoothing strategy might be introducing an error.

For months, Leask painstakingly examined every perchlorate pixel in the raw data, before the spike-removing correction had been applied. “We knew instantly that some of [the signs] were not real,” she says. It turned out that none of them were.

“I think [Leask] is definitely onto something here,” says planetary scientist Jennifer Hanley of the Lowell Observatory in Flagstaff, Ariz., who was a coauthor on the 2015 study but was not involved in the new work. Looking at the processed data and the raw data together, the perchlorate feature “does seem to kind of disappear, which is concerning.”

That doesn’t necessarily mean the perchlorates aren’t there, though, she says — they may just be harder to recognize. Hanley and her colleagues are working on a more reliable way to identify similar salts on Mars based on several lines of evidence, not just a single line in the spectrum.

“We definitely know that these salts are on the surface of Mars,” Hanley says. “They could still be important for habitability. But we have to be more cautious about detecting them.”
===============
Brain implants let paralyzed people use tablets to send texts and stream music
Devices that eavesdrop on neural activity can help paralyzed people command computer tablets to stream music, text friends, check the weather or surf the internet.

Three people with paralysis below the neck were able to navigate off-the-shelf computer tablets using an electrode array system called BrainGate2. The results, published November 21 in PLOS One, are the latest to show that neural signals can be harnessed to directly allow movement (SN: 6/16/12, p. 5).

The two men and one woman had electrode grids implanted over part of the motor cortex, an area of the brain that helps control movement. The brain implants picked up neural activity indicating that the participants were thinking about moving a cursor. Those patterns were then sent to a virtual mouse that was wirelessly paired to the tablet.

Using nothing more than their intentions to move a cursor, the three participants performed seven common digital tasks, including web browsing and sending e-mail. One participant looked up orchid care, ordered groceries online and played a digital piano. “The tablet became second nature to me, very intuitive,” she told the researchers when asked about her experience, according to the study.

Another participant enjoyed texting friends, “especially because I could interject some humor,” he told the scientists. The system even allowed two of the participants to chat with each other in real time.

For the study, the researchers used tablets with standard settings, without installing any shortcuts or features to make typing or navigation easier.
===============
A new airplane uses charged molecules, not propellers or turbines, to fly
A newly designed airplane prototype does away with noisy propellers and turbines.

Instead, it’s powered by ionic wind: charged molecules, or ions, flowing in one direction and pushing the plane in the other. That setup makes the aircraft nearly silent. Such stealth planes could be useful for monitoring environmental conditions or capturing aerial imagery without disturbing natural habitats below.

The aircraft is the first of its kind to be propelled in this way, researchers report in the Nov. 22 Nature. In 10 indoor test flights the small plane, which weighs about as much as a Chihuahua, traveled 40 to 45 meters for almost 10 seconds at a steady height, even gaining about half a meter of altitude over the course of a flight.

Most planes rely on spinning parts to move forward. In some, an engine turns a propeller that pushes the plane forward. Or a turbine sucks in air with a spinning fan, and then shoots out jets of gas that propel the plane forward.

Ionic wind is instead generated by a high-voltage electric field around a positively charged wire, called an emitter. The electricity, often supplied by batteries, makes electrons in the air collide with atoms and molecules, which then release other electrons. That creates a swarm of positively charged air molecules around the emitter, which are drawn to a negatively charged wire. The movement of molecules between the two wires, the ionic wind, can push a plane forward. The current design uses four sets of these wires.

Moving ions have helped other things to fly through the air, such as tiny airborne robots. But conventional wisdom said that using the approach to move something through the air as big as an airplane wasn’t possible, because adding enough battery power to propel a plane this way would make it too heavy to stay aloft. (The ion thrusters that propel spacecraft through the vacuum of space work in a very different way and aren’t functional in air.) Attempts to build ion-propelled aircraft in the 1960s weren’t very successful.

MIT aeronautics researcher Steven Barrett thought differently. With the right aircraft design and light enough batteries, flight might be possible, his initial calculations suggested. So he and his team used mathematical equations to optimize various features of the airplane — its shape, materials, power supply — and to predict how each version would fly. Then the researchers built prototypes of promising designs and tested the planes at the MIT indoor track, launching them via a bungee system.

“The models and the reality of construction don’t always match up perfectly,” Barrett says, so finding the right design took a lot of tries. But in the new study, he and his collaborators report success: 10 flights of the aircraft, which has a 5-meter wingspan and weighs just under 2.5 kilograms.

Barrett’s team isn’t the only one who thought the ionic wind method might take off. Based on calculations done in his lab, “we were confident that this could be done,” says Franck Plouraboue of the Toulouse Fluid Mechanics Institute in France, who wasn’t part of the research. “Here they've done it — which is fantastic!”

It’s an example of distributed electric propulsion, says Plouraboue — spreading out the thrust-generating parts of the plane, instead of having one centralized source. That’s a hot area for aircraft research right now. NASA’s X-57 Maxwell plane, for example, bears 14 battery-operated motors along its wings. Increasing the number of propellers makes the plane go farther on the same amount of energy, says Plouraboue, but also increases the drag. With ionic wind propulsion, increasing the number of wires doesn’t increase drag very much.

The plane still needs some upgrades before it’s ready for the real world: Its longest flight was only 12 seconds. And while the aircraft can maintain steady flight for a short time once launched, it can’t actually get off the ground using ionic wind.

Even with improvements, ion-propelled aircraft won’t find their niche as passenger planes, predicts Daniel Drew, an aerodynamics researcher at the University of California, Berkeley, who was not involved in the work. (Drew has designed miniature flying bots that fly using ionic propulsion.) It’s probably not feasible to scale up to something the size of a 747 — there are efficiency trade-offs as planes get bigger, he says. But down the road, the approach might be useful for small, uncrewed planes or drones.
===============
How Twitter bots get people to spread fake news
To spread misinformation like wildfire, bots will strike a match on social media but then urge people to fan the flames.

Automated Twitter accounts, called bots, helped spread bogus articles during and after the 2016 U.S. presidential election by making the content appear popular enough that human users would trust it and share it more widely, researchers report online November 20 in Nature Communications. Although people have often suggested that bots help drive the spread of misinformation online, this study is one of the first to provide solid evidence for the role that bots play.

The finding suggests that cracking down on devious bots may help fight the fake news epidemic (SN: 3/31/18, p. 14).

Filippo Menczer, an informatics and computer scientist at Indiana University Bloomington, and colleagues analyzed 13.6 million Twitter posts from May 2016 to March 2017. All of these messages linked to articles on sites known to regularly publish false or misleading information. Menczer’s team then used Botometer, a computer program that learned to recognize bots by studying tens of thousands of Twitter accounts, to determine the likelihood that each account in the dataset was a bot.

Unmasking the bots exposed how the automated accounts encourage people to disseminate misinformation. One strategy is to heavily promote a low-credibility article immediately after it’s published, which creates the illusion of popular support and encourages human users to trust and share the post. The researchers found that in the first few seconds after a viral story appeared on Twitter, at least half the accounts sharing that article were likely bots; once a story had been around for at least 10 seconds, most accounts spreading it were maintained by real people.

“What these bots are doing is enabling low-credibility stories to gain enough momentum that they can later go viral. They’re giving that first big push,” says V.S. Subrahmanian, a computer scientist at Dartmouth College not involved in the work.

The bots’ second strategy involves targeting people with many followers, either by mentioning those people specifically or replying to their tweets with posts that include links to low-credibility content. If a single popular account retweets a bot’s story, “it becomes kind of mainstream, and it can get a lot of visibility,” Menczer says.

These findings suggest that shutting down bot accounts could help curb the circulation of low-credibility content. Indeed, in a simulated version of Twitter, Menczer’s team found that weeding out the 10,000 accounts judged most likely to be bots could cut the number of retweets linking to shoddy information by about 70 percent.

Bot and human accounts are sometimes difficult to tell apart, so if social media platforms simply shut down suspicious accounts, “they’re going to get it wrong sometimes,” Subrahmanian says. Instead, Twitter could require accounts to complete a captcha test to prove they are not a robot before posting a message (SN: 3/17/07, p. 170).

Suppressing duplicitous bot accounts may help, but people also play a critical role in making misinformation go viral, says Sinan Aral, an expert on information diffusion in social networks at MIT not involved in the work. “We’re part of this problem, and being more discerning, being able to not retweet false information, that’s our responsibility,” he says.

Bots have used similar methods in an attempt to manipulate online political discussions beyond the 2016 U.S. election, as seen in another analysis of nearly 4 million Twitter messages posted in the weeks surrounding Catalonia’s bid for independence from Spain in October 2017. In that case, bots bombarded influential human users — both for and against independence — with inflammatory content meant to exacerbate the political divide, researchers report online November 20 in the Proceedings of the National Academy of Sciences.

These surveys help highlight the role of bots in spreading certain messages, says computer scientist Emilio Ferrara of the University of Southern California in Los Angeles and a coauthor of the PNAS study. But “more work is needed to understand whether such exposures may have affected individuals’ beliefs and political views, ultimately changing their voting preferences.”
===============
An exploding meteor may have wiped out ancient Dead Sea communities
DENVER — A superheated blast from the skies obliterated cities and farming settlements north of the Dead Sea around 3,700 years ago, preliminary findings suggest.

Radiocarbon dating and unearthed minerals that instantly crystallized at high temperatures indicate that a massive airburst caused by a meteor that exploded in the atmosphere instantaneously destroyed civilization in a 25-kilometer-wide circular plain called Middle Ghor, said archaeologist Phillip Silvia. The event also pushed a bubbling brine of Dead Sea salts over once-fertile farm land, Silvia and his colleagues suspect.

People did not return to the region for 600 to 700 years, said Silvia, of Trinity Southwest University in Albuquerque. He reported these findings at the annual meeting of the American Schools of Oriental Research on November 17.

Excavations at five large Middle Ghor sites, in what’s now Jordan, indicate that all were continuously occupied for at least 2,500 years until a sudden, collective collapse toward the end of the Bronze Age. Ground surveys have located 120 additional, smaller settlements in the region that the researchers suspect were also exposed to extreme, collapse-inducing heat and wind. An estimated 40,000 to 65,000 people inhabited Middle Ghor when the cosmic calamity hit, Silvia said.

The most comprehensive evidence of destruction caused by a low-altitude meteor explosion comes from the Bronze Age city of Tall el-Hammam, where a team that includes Silvia has been excavating for the last 13 years. Radiocarbon dating indicates that the mud-brick walls of nearly all structures suddenly disappeared around 3,700 years ago, leaving only stone foundations.

What’s more, the outer layers of many pieces of pottery from same time period show signs of having melted into glass. Zircon crystals in those glassy coats formed within one second at extremely high temperatures, perhaps as hot as the surface of the sun, Silvia said.

High-force winds created tiny, spherical mineral grains that apparently rained down on Tall el-Hammam, he said. The research team has identified these minuscule bits of rock on pottery fragments at the site.

Examples exist of exploding space rocks that have wreaked havoc on Earth (SN: 5/13/17, p. 12). An apparent meteor blast over a sparsely populated Siberian region in 1908, known as the Tunguska event, killed no one but flattened 2,000 square kilometers of forest. And a meteor explosion over Chelyabinsk, Russia, in 2013 injured more than 1,600 people, mainly due to broken glass from windows that were blown out.
===============
Nuclear ‘knots’ could unravel the mysteries of atoms
Knotlike structures called skyrmions might help scientists untangle the inner workings of atomic nuclei, a new study suggests.

A skyrmion is a tiny disturbance in a substance, a swirling pattern that, like a knot, is difficult to undo. In the 1960s, nuclear physicist Tony Skyrme suggested that these structures — since named after him — could represent protons and neutrons within a nucleus in theoretical calculations. But despite some initial promise, the idea hit snags. In particular, skyrmion calculations produced misshapen nuclei.

But now researchers have improved their calculations of how protons and neutrons should cluster together in the skyrmion picture. Those results agreed with expectations based on experimental data, the team reports in a study in press at Physical Review Letters.

Here’s how the idea works: Inside a nucleus, particles called pions are constantly zinging around, helping to hold the nucleus together. Just as an electron has an electric field that can jostle other particles, those pions are associated with fields too. In Skyrme’s original picture, protons and neutrons can be described as twists in the pion field — or skyrmions — akin to a knot tied in a piece of string.

In reality, protons and neutrons are each made up of smaller subatomic particles called quarks and gluons, and the fundamental theory that describes how those particles interact, called quantum chromodynamics, is impossibly complex. Skyrmions could simplify calculations — if only the technique produced the correct answers.

The right shapes Scientists predict that some atomic nuclei should be shaped into multiple clumps. Previously, the shapes of those nuclei, when calculated with skyrmions, displayed forms that disagreed with that prediction (top row). But new skyrmion calculations found nuclei that consisted of multiple clusters, as expected (bottom row). Colors indicate the type of pion — particles that help hold the nucleus together — that dominates in each region.

Now, physicists from Durham University in England have solved some of skyrmions’ woes, in studies of atomic nuclei as large as carbon-12.

Skyrmion calculations typically neglect heavier particles called rho mesons that are also important for keeping nuclei intact. Including those particles in the calculations changes how the skyrmion “knot” in the field gets tied, and the shapes of the resulting nuclei, says mathematical physicist and study coauthor Paul Sutcliffe. It’s as if the knots were tied in “a boring piece of string before, and now it’s ... a colored string with some sparkles on it.” As a result, “you now get the right shapes,” he says.

The idea of skyrmions caught on in other fields as well. A related skyrmion shows up in spirals of magnetization in certain solid materials (SN: 2/17/18, p. 18), but magnetic skyrmions are much larger and can be manipulated at will.

Researchers have long struggled to use skyrmions to study atomic nuclei, says theoretical physicist Nicholas Manton of the University of Cambridge who was not involved with the study. But the new result “gets closer to being physically reasonable.”

Eventually, such calculations might help scientists study surprising properties of certain nuclei. An example is carbon-14, a radioactive version of carbon that can be used to date ancient artifacts. It decays with a surprisingly long half-life of about 5,700 years. Skyrmions could help scientists better understand that strange decay, Manton says.
===============
NASA’s Mars 2020 rover will look for ancient life in a former river delta
The next NASA Mars rover will hunt for signs of ancient life in what used to be a river delta, the agency announced on November 19.

The rover is expected to launch in July 2020 and to land on Mars around February 18, 2021. It will seek out signs of past life in the sediments and sands of Jezero crater, which was once home to a 250-meter-deep lake and a river delta that flowed into the lake.

“This is a major attraction from our point of view for a habitable environment,” said Mars 2020 project scientist Ken Farley of Caltech in a news conference discussing the site. “A delta is extremely good at preserving biosignatures.” Any evidence of life that may once have existed in the lake water, or even evidence that came from the river’s headwaters and flowed downstream, could be preserved in the rocks that are there today.

The 2020 rover’s design is similar to that of the Curiosity rover, which has been exploring a different ancient crater lake, Gale crater, since 2012 (SN: 5/2/15, p. 24). But where Curiosity has an onboard chemistry lab for studying the rocks and minerals in its crater, Mars 2020 will have a specialized backpack for sample storage. A future mission will pick up the cached samples and return them to Earth for more detailed study, possibly sometime in the 2030s.

“The samples will come back to the best labs — not the best labs we have today, but the best labs we will have then,” said science mission directorate administrator Thomas Zurbuchen of NASA headquarters in Washington, D.C.

Mars 2020 will also use a souped-up version of Curiosity’s landing system called Sky Crane, in which a hovering platform lowers the rover onto the ground with a cable. Mars 2020’s version will include a navigation system that will help it avoid hazards on the ground, like cliff faces and boulders.

Jezero crater is within striking distance of another site on scientists’ wish list. That region, called Midway, is just 28 kilometers away from Jezero and contains some of the most ancient rocks on Mars. At the final landing site selection workshop in October, scientists floated the idea of visiting both sites in one mission, a feat seen as ambitious but achievable. But a decision on that will have to wait until after the rover is safely on Mars, Farley said.
===============
The Society for Science & the Public Gives $100,000 to Middle School Science Teachers
WASHINGTON, D.C. – The Society for Science & the Public is excited to announce that the organization will be giving a total of $100,000 to 24 middle school science teachers across the country to support STEM research activities in their classrooms. Each teacher will receive a grant of up to $5,000 to spend on scientific research equipment, such as Raspberry Pi computers, water and soil testing kits and computer software for data analysis. To date, the Society has provided $120,000 in grants to STEM teachers through this program.

“I’m excited to be providing this financial support to middle school teachers who encounter students at such an important point in their lives,” said Maya Ajmera, President & CEO of the Society for Science & the Public. “During this crucial time, students start setting the stage for their future path in high school while some young people begin drifting away from their interests in STEM. By providing these teachers with the resources to build a meaningful STEM research program, we are helping to engage the next generation of leaders in engineering and science.”

The Society’s STEM Research Grant Program supports educators who are implementing fresh and innovative teaching methods to shed light on discoveries that can be made through original STEM research projects. The one-time grants, in large part, are provided to educators in schools serving in low-income areas or underrepresented students.

In 2017, the program provided $120,000 to nearly 30 high school teachers. In 2018, the Society chose to focus on middle school teachers. The 2018 recipients represent 18 states and the U.S. territory, American Samoa.

The following teachers received STEM Research Grants to fund equipment and other programming:

Devin Berge, La Plata Middle School (Silver City, NM) - $5,000 Elizabeth Clark, Manchester Middle School, (Richmond, VA) - $3,000 Fernando Cleves, Joyce Kilmer Upper School, (West Roxbury, MA) - $3,000 Melanie Corell, Guinyard-Butler Middle School, (Barnwell, SC) - $5,000 Mary Crowley, Murray Middle School, (St. Paul, MN) - $5,000 Anthony Duncan, John S. Gillett Intermediate School, (Kingsville, TX) - $2,000 Kate Elliott, Foothills Elementary School, (Salem, UT) – $3,000 Kimberly Gasaway, STEMM Academy, (Valparaiso, FL) – $5,000 Megan Heitkamp, Salk Middle School, (Elk River, MN) – $3,000 Beth Kenna, Cedarbrook Middle School, (Wyncote, PA) - $5,000 Joseph King, Challenge School, (Denver, CO) - $5,000 Deborah Kletch, Fishers Junior High School, (Fishers, IN) - $5,000 Denise Kratz, Explorers Homeschool Association, (Ann Arbor, MI) - $4,000 Christal Long, From the Heart Christian School, (Suitland, MD) - $5,000 Carole McKee, Northern Lights ABC, (Anchorage, AK) - $5,000 Patricia Mosey, Eagle Ridge Middle School, (Savage, MN) - $1,000 Lucia Perez, Jose Marti MAST 6-12 Academy, (Hialeah, FL) -$5,000 Megan Sabin, Algona Middle School, (Algona, IA) - $3,000 Jonathan Sailer, Schroeder Middle School, (Grand Forks, ND) - $5,000 Ebubekir Sen, Sonoran Science Academy, (Tucson, AZ) - $5,000 Melissa Sleeper, Gifford Middle School, (Vero Beach, FL) - $5,000 Setefano Umaga, Afonotele Elementary School, (Pago Pago, American Samoa) - $4,000 Laura Wilbanks, Southcrest School, (Lubbock, TX) - $4,000 Tammy Will, Morrison Public School, (Morrison, OK) - $5,000

The STEM Research Grant Program is supported, in part, through funding provided by Regeneron.

About Society for Science & the Public

Society for Science & the Public is dedicated to the achievement of young scientists in independent research and to public engagement in science. Established in 1921, the Society is a nonprofit whose vision is to promote the understanding and appreciation of science and the vital role it plays in human advancement. Through its world-class competitions, including the Regeneron Science Talent Search, the Intel International Science and Engineering Fair, and the Broadcom MASTERS, and its award-winning magazines, Science News and Science News for Students, Society for Science & the Public is committed to inform, educate, and inspire. Learn more at www.societyforscience.org and follow us on Facebook, Twitter, Instagram and Snapchat (Society4Science).
===============
A Bronze Age tomb in Israel reveals the earliest known use of vanilla
DENVER — Three jugs placed as offerings in a roughly 3,600-year-old tomb in Israel have revealed a sweet surprise — evidence of the oldest known use of vanilla.

Until now, vanilla was thought to have originated in Mexico, perhaps 1,000 years ago or more. But jugs from the Bronze Age site of Megiddo contain remnants of two major chemical compounds in natural vanilla extract, vanillin and 4-hydroxybenzaldehyde, said archaeologist Vanessa Linares of Tel Aviv University in Israel. Chemical analyses also uncovered residues of plant oils, including a component of olive oil, in the three jugs.

“Bronze Age people at Megiddo may have used vanillin-infused oils as additives for foods and medicines, for ritual purposes or possibly even in the embalming of the dead,” Linares said. She described these findings at the annual meeting of American Schools of Oriental Research on November 16.

Vanillin comes from beans in vanilla orchids. About 110 species of these flowers are found in tropical areas around the world. The chemical profile of the vanillin in the Megiddo jugs best matches present-day orchid species in East Africa, India and Indonesia, Linares said.

Extensive Bronze Age trade routes likely brought vanillin to the Middle East from India and perhaps also from East Africa, she suggested.

“It’s really not surprising that vanillin reached Bronze Age Megiddo given all the trade that occurred between the [Middle East] and South Asia,” says archaeologist Eric Cline of George Washington University in Washington, D.C. But no evidence exists of trade at that time between Middle Eastern societies and East Africa, says Cline, who did not participate in the Megiddo research.

Vanilla orchids or their beans probably reached Megiddo via trade routes that first passed through Mesopotamian society in southwest Asia. However Bronze Age Middle Easterners ended up with those products, discoveries at Megiddo challenge the idea that vanilla use originated only in Mexico and then spread elsewhere, Cline says.

The vanillin-containing jugs at Megiddo came from a tomb of three “highly elite” individuals who were interred with six other people of lesser social rank, said archaeologist Melissa Cradic of the University of California, Berkeley, a member of the current Megiddo research team. Excavations uncovered the tomb in 2016, Cradic also reported at the ASOR meeting.

Primary burials in the tomb consist of an adult female, an adult male and an 8- to 12-year-old boy. Elaborate types of bronze, gold and silver jewelry were found on and around the three skeletons. Exact replicas of several pieces of jewelry appeared on each individual.

The tomb lies in an exclusive part of Megiddo near a palace and a monumental city gate.

“We can’t definitively say that these three people were royals,” Cradic said. “But they were elites in Megiddo and may have belonged to the same family.”
===============
Gut bacteria may guard against diabetes that comes with aging
Losing one variety of gut bacteria may lead to type 2 diabetes as people age.

Old mice have less Akkermansia muciniphila bacteria than young mice do, researchers report November 14 in Science Translational Medicine. That loss triggers inflammation, which eventually leads cells to ignore signals from the hormone insulin. Such disregard for insulin’s message to take in glucose is known as insulin resistance and is a hallmark of type 2 diabetes.

Researchers have suspected that bacteria and other microbes in the gut are involved in aging, but how the microbes influence the process hasn’t been clear. Monica Bodogai of the U.S. National Institute on Aging in Baltimore and colleagues examined what happens to mice’s gut bacteria as the rodents age. The mice lose A. muciniphila, also called Akk, and other friendly microbes that help break down dietary fiber into short-chain fatty acids, such as butyrate and acetate. Those fatty acids signal bacteria and human cells to perform certain functions.

Losing Akk led to less butyrate production, Bodogai’s team found. In turn, loss of butyrate triggered a chain reaction of immune cell dysfunction that ended with mice’s cells ignoring the insulin.

Treating old mice and elderly rhesus macaques with an antibiotic called enrofloxacin increased the abundance of Akk in the animals’ guts and made cells respond to insulin again. Giving old animals butyrate had the same effect, suggesting that there may be multiple ways to head off insulin resistance in older people in the future.
===============
Hemp fields offer a late-season pollen source for stressed bees
VANCOUVER — Fields of hemp might become a late-season pollen bonanza for bees.

Industrial hemp plants, the no-high varieties of cannabis, are becoming a more familiar sight for American bees as states create pilot programs for legal growing. Neither hemp nor the other strains of the Cannabis sativa species grown for recreational or medicinal uses offer insects any nectar, and all rely on wind to spread pollen. Still, a wide variety of bees showed up in two experimental hemp plots during a one-month trapping survey by entomology student Colton O’Brien of Colorado State University in Fort Collins.

Bees in 23 out of the 66 genera known to live in Colorado tumbled into O’Brien’s traps, he reported November 11 at Entomology 18, the annual meeting of the U.S. and two Canadian entomological societies. O’Brien and his adviser, Arathi Seshadri, think this is the first survey of bees in cannabis fields.

“You walk through fields and you hear buzzing everywhere,” O’Brien said. He caught big bumblebees, tiny metallic-green sweat bees and many others clambering around in the abundant greenish-yellow pollen shed by the male flowers.

Bees need pollen to feed their young, and during the trap survey in August 2016, there weren’t a lot of other flowers blooming. Hardly anything is known about the nutritional qualities of hemp pollen for larval bees. Yet, commercial hemp plots may end up as rare food sources for pollinators in stressful times, O’Brien said. Honeybee health has faltered in recent years, and conservationists also worry about the fates of the many, less-studied wild bees. O’Brien urged crop scientists now developing the pest fighting strategies for outdoor hemp to be mindful of bee health.

Pest management techniques for hemp are still a work in progress. There are even questions about which insects are truly hemp pests, said entomologist Whitney Cranshaw, also of Colorado State. New potential menaces have arrived since the early 20th century, when farmers were growing hemp with very low concentrations of the psychoactive compound THC as a crop for fiber and other practical uses. Anti-drug legislation eventually made growing any cannabis forms illegal for decades in the United States.

The 2014 U.S. Farm Bill, however, differentiated between hemp with less than 0.3 percent THC by dry weight, and high-THC cannabis varieties of interest for recreational and medical use. This distinction has allowed states such as Colorado and Kentucky to set up programs for regulated legal growing in a push to revive the potentially valuable crop. But there are a lot of new questions about old plants.
===============
Wombats are the only animals whose poop is a cube. Here’s how they do it.
Of all the poops in the world, only wombats’ are shaped like cubes.

The varied elasticity of the wombat’s intestines helps the marsupials to sculpt their scat into cubelike nuggets, instead of the round pellets, messy piles or tubular coils made by other mammals, researchers reported November 18 at the American Physical Society Division of Fluid Dynamics meeting in Atlanta.

Wombats mark their territories with small piles of scat. Cuboid poops stack better than rounder ones, and don’t roll away as easily.

But cubic shapes in nature are very unusual, says mechanical engineer David Hu of Georgia Institute of Technology in Atlanta. Making and maintaining flat facets and sharp corners takes energy. So it’s surprising that the wombat’s intestines — which look much like those of any other mammal — would create that shape.

When an Australian colleague sent Hu and his colleague Patricia Yang the intestines from two roadkill wombats collecting frost in his freezer, “we opened those intestines up like it was Christmas,” Hu says.

The intestines were packed with poop, Yang says. In humans, a poop-filled bit of intestine stretches out slightly. In wombats, the intestine stretches to two to three times its regular width to accommodate all of the feces.

Yang used skinny balloons — the type that gets sculpted into animals at carnivals — to inflate the intestines and measure their stretchiness in different places. Some regions were more stretchy; some were stiffer. The stiffer regions probably help create the distinct edges on the wombat poops as the waste moves through the gut, Yang proposes.

Sculpting the poop into cuboid nuggets appears to be a finishing touch for the wombat digestive tract. Over a typical 30-meter-long wombat intestine, the poops take on distinct edges only in the last meter or so, Hu says. Up to that point, the waste is gradually solidifying as it moves through the gut.

The finished turds are especially dry and fibrous, which may help them retain their signature shape when they’re squeezed out, Yang suggests. They can be stacked or rolled like dice, standing up on any of their faces. (She knows. She tried it.)

In the wild, wombats deposit their droppings on top of rocks or logs as territory markers, sometimes forming small piles. They seem to prefer to poop in elevated spots, Hu says, but they’re also limited by their stubby legs.

To confirm that the elasticity variation really does form the cubes, Yang and Hu are now trying to model the wombat digestive tract using pantyhose.
===============
Small doses of peanut protein can turn allergies around
Carefully calibrated doses of peanut protein can turn extreme allergies around. At the end of a year of slowly increasing exposure, most children who started off severely allergic could eat the equivalent of two peanuts.

That reversal, reported November 18 in the New England Journal of Medicine, “will be considered life-transforming for many families with a peanut allergy,” says pediatric allergist Michael Perkin of St. George’s, University of London, who wrote an accompanying editorial in the same issue of NEJM. The findings were also presented on the same day at the annual meeting of the American College of Allergy, Asthma and Immunology in Seattle.

Peanut exposure came in the form of a drug called AR101, described in the study as a “peanut-derived investigational biologic oral immunotherapy drug,” or, as Perkin puts it, “peanut flour in a capsule.” Unlike a sack of peanut flour, AR101 is carefully meted out, such that the smallest doses used in the study contained precisely 0.5 milligrams of peanut protein — the equivalent of about one six-hundredth of a large peanut.

In the clinical trial, 372 children ages 4 to 17 years began taking the lowest dose of AR101. The doses increased in peanut protein every two weeks until the kids topped out at 300 milligrams, which is about that of a single peanut.

Tolerance boost Kids who are allergic to peanuts were given either a daily placebo or increasing doses of peanut protein for one year. Only 4 percent of children who received the placebo were able to tolerate 600 milligrams of peanut protein, or the equivalent of two large peanuts without bad reactions. Of those who completed a year of treatment, 67 percent could tolerate the equivalent of two large peanuts. Percentage of allergic children who tolerated 2 nuts’ worth of peanut protein

For the next 24 weeks, participants, located in the United States, Canada and Europe, took that dose daily. When the trial ended, all of the participants were challenged with increasing doses of peanut protein under close supervision. Two-thirds of the 372 children who received the peanut protein regimen, or 250 participants, could tolerate a peanut protein dose of at least 600 milligrams, comparable to about two peanuts. In contrast, only five of the 124 children who received placebos, or 4 percent, could tolerate the same dose. (A smaller number of adults ages 18 to 55 were enrolled in the study, but didn’t show big improvements.)

That improved tolerance “can really change the lives of patients who are peanut allergic,” says study coauthor Daniel Adelman, an allergist and immunologist at Aimmune Therapeutics, a company based in Brisbane, Calif., that makes AR101 and sponsored the trial.

The goal of the therapy is to guard against the potentially dangerous effects of accidental peanut exposures, such as a child mistakenly taking a bite of a friend’s PB&J. “What we’re trying to do is free people up from the fear and anxiety associated with the potential bad things that can happen with minute quantities of peanut exposure,” Adelman says.

During the study, nearly all of the participants who received the drug had allergic reactions to it — reactions that were expected, since “you’re giving people the thing they’re allergic to,” Adelman says. Most of those reactions weren’t severe, such as a rash or slight abdominal pain.

Although the drug is made of peanut protein, parents, or even doctors, shouldn’t attempt a similar treatment by measuring peanut protein themselves, experts say. Without exact measurements, peanut exposure could be dangerous. “This is treating peanut like a medicine, not a food,” says pediatric allergist Scott Sicherer of the Icahn School of Medicine at Mount Sinai in New York City. “Don’t try this at home.”

Schicher also cautions that, while the regimen is promising, it is not a cure. It’s not yet clear how long people would need consistent peanut protein exposure to maintain their tolerance, but regular use is probably needed. “It has to be a routine,” he says.
===============
Tiny satellites will report back to Earth on InSight’s Mars landing
The next spacecraft set to land on Mars is bringing its own communications team. InSight, a lander scheduled to touch down on the Red Planet on November 26, is accompanied by a pair of briefcase-sized spacecraft that will send details of the landing to Earth in almost real time.

The twin craft on this mission are CubeSats — tiny, inexpensive satellites that are easy to build and launch. Called Mars Cube One, or MarCO for short, they will fly past Mars as InSight lands, becoming the smallest spacecraft ever to be entrusted with a task as crucial as relaying landing information for a mission. Now nearing Mars, they are also already the first CubeSats to make it so far from Earth. If all goes well with InSight’s landing, future Mars missions could also be equipped with their own single-use comms team.

“A future where landers and rovers brought their own communications systems for landing, that would be fantastic,” says engineer Joel Krajewski of NASA’s Jet Propulsion Laboratory in Pasadena, Calif., and MarCO’s program manager.

InSight — short for Interior Exploration using Seismic Investigations, Geodesy and Heat Transport — will carry the first seismometer to Mars (SN: 5/26/18, p. 13). After touching down in a wide, flat plain called Elysium Planitia near Mars’ equator, the lander will sit perfectly still to listen to seismic waves and measure how heat flows through the Red Planet’s interior. The results will help scientists understand how Mars, and perhaps other rocky planets like Earth, formed around 4.5 billion years ago.

It will be only 6½ minutes between when InSight enters the Martian atmosphere, at a speed of nearly 1,000 meters per second, to the moment its legs touch the ground. The spacecraft will use a parachute and rockets aimed at the ground to slow to about 2.4 meters per second as it lands. Light-speed signals from the CubeSats or Insight itself will then take about eight minutes to travel between Earth and Mars, so by the time NASA engineers hear that InSight has entered Mars’ atmosphere, the spacecraft will be on the ground.

“Which is terrifying,” says engineer Farah Alibay, also of the Jet Propulsion Laboratory. “Whether it landed softly or pretty hard, we won’t know. But we’ll know when you get that first bit of data, InSight’s already landed.”

We’re listening The MarCO CubeSats will watch InSight’s descent to the Martian surface (red line) and send details of the landing back to Earth, before continuing past the planet.

For most previous Mars landings, one of the large orbiters currently circling the Red Planet had to pause its data-taking to watch the event and send details to Earth. The orbiter that will be in the best position to watch InSight will be NASA’s Mars Reconnaissance Orbiter. While that spacecraft will observe the landing, it won’t be able to relay any details to Earth for at least three hours as its orbit takes the craft behind Mars from Earth’s point of view, blocking communications.

“Three to four hours is not long for most people, but it’s pretty long for us,” Alibay says. “Landing is the scariest part of your mission.” Waiting to hear about the spacecraft’s landing is like waiting for news about a loved one’s health, she says.

To avoid that waiting, the team sent the twin CubeSats. The spacecraft launched with InSight, but have been navigating through deep space on their own since May. The MarCO craft can change their trajectories by expelling compressed cold gas, similar to the way a fire extinguisher works — which earned them the nicknames Wall-E and Eve among the team, after the space-flying Disney robot characters. “We’ve demonstrated that a CubeSat can leave Earth orbit, survive the harsh environment of space and direct itself towards Mars,” Alibay says.

About five minutes before InSight hits the top of the Martian atmosphere, the two MarCO craft will position themselves to track the lander all the way to the ground, and send details back to Earth immediately. Each operates independently, backing each other up.

If all goes well, MarCO could set a precedent for future Mars missions. Existing Mars orbiters will be able to support two Mars missions launching in 2020 — NASA’s Mars 2020 rover and the ExoMars rover run by the European Space Agency and Russia’s space agency. But after that, the future is dim.

“Right now, there’s not an active plan for an orbiter beyond that time frame,” Krajewski says. Plus, existing orbiters have to burn fuel to get into the right position to watch other spacecraft land, which shortens the orbiters’ lives. Sending future spacecraft with their own CubeSat comms team could help scientists monitor landings without compromising the big orbiters’ science missions.

After InSight lands, MarCO’s job will be done. The tiny craft don’t have enough fuel or the right equipment to enter a long-term orbit around Mars. Instead, MarCO will “wave goodbye and continue along,” Krajewski says.

You can watch InSight’s landing online on NASA TV.
===============
A Bronze Age game called 58 holes was found chiseled into stone in Azerbaijan
DENVER — A dotted pattern pecked into stone at a remote Eurasian rock-shelter represents a Bronze Age game that was thought to have existed at that time only in Mesopotamia, Egypt and other Near Eastern regions.

The game is known as 58 holes, or Hounds and Jackals. Archaeologist Walter Crist of the American Museum of Natural History in New York City described his surprising discovery of a roughly 4,000-year-old example of 58 holes in present-day Azerbaijan on November 15 at the annual meeting of the American Schools of Oriental Research.

Azerbaijan sits between the Caucasus Mountains and the Caspian Sea, some 1,000 to 2,000 kilometers from the Near East. “Bronze Age herders in that region must have had contacts with the Near Eastern world,” Crist said. “Ancient games often passed across cultures and acted as a social lubricant.”

While conducting an internet search of publications about 58 holes, Crist saw what looked like an example of the game’s layout in a photograph from a rock-shelter published in an online magazine called Azerbaijan International. He contacted a colleague in the Eurasian nation who helped to arrange a site visit in April 2018.

Once there, Crist found that the site shown in the magazine had been bulldozed for a housing development. But a scientific official in Azerbaijan told him of another rock-shelter with the same dot pattern. Crist, who has studied early Near Eastern versions of 58 holes, recognized the two-person game when he reached that site.

The pattern consists of two central rows of dots and two outer rows of dots that curve in to meet at a slightly larger dot located above the central rows. Dot numbers can vary but usually total 58. Players are thought to have rolled dice to move pebbles or other pieces on each side of the game, each trying to reach the top first. This game might have been a precursor of modern-day backgammon.

Themes and styles of rock art found along with the game in Azerbaijan date to the Bronze Age, Crist says. Techniques to date rock art more precisely have not been used on the finds (SN Online: 11/7/18).
===============
FDA restricts the sale of some flavored e-cigarettes as teen use soars
In an attempt to curtail an alarming rise in teenage vaping, the U.S. Food and Drug Administration announced restrictions on the sale of certain flavored e-cigarettes that appeal to young people on November 15. The agency also said it would seek to ban menthol cigarettes, long a goal of public health advocates, as well as flavored cigars.

The flavor restrictions coincide with the release of new data showing that e-cigarette use by high school students shot up 78 percent from 2017 to 2018. The data, part of the National Youth Tobacco Survey, were reported November 16 in the Centers for Disease Control and Prevention’s Morbidity and Mortality Weekly Report.

“If the policy changes that we have outlined don’t reverse this epidemic, and if the manufacturers don’t do their part to help advance this cause, I’ll explore additional actions,” FDA commissioner Scott Gottlieb said in a statement.

Gottlieb said that only stores that restrict access to the products to customers 18 years or older will be able to sell certain e-cigarette flavors, such as mango or crème brûlée. There will also be limits on online sales, but there are no restrictions on the flavors menthol, mint or tobacco.

Rising vapers Overall use of tobacco products reported among middle and high school students is up, driven largely by popular e-cigarettes. The percentage of high schoolers who reported using e-cigs over the previous month rose 78 percent from 2017 to 2018. Use of tobacco products among U.S. students, 2011–2018

Some experts said the measures didn’t go far enough. “Even if stores are compliant with not selling to people under 18, youth are getting their tobacco products from other people,” says Deborah Ossip, a public health researcher at the University of Rochester Medical Center in New York. “There are work-arounds that may limit the effectiveness of this limited ban.”

Vaping has risen dramatically among teenagers over the last year. Among high school students surveyed, 20.8 percent said they had used e-cigarettes at least once in the last 30 days in 2018, compared with 11.7 percent in 2017 — an increase of 78 percent.

The data also suggest that more high school students are vaping regularly. Of those who had used an e-cigarette in 2018, 27.7 percent had vaped on at least 20 days in the last month, a sign of possible addiction, experts say. That was true for 20 percent of these students in 2017.

Among middle school students, 4.9 percent of survey respondents in 2018 said they had vaped in the last month, compared with 3.3 percent in 2017. Nicotine use is especially dangerous for young people, who are at higher risk of addiction than adults and whose developing brains are especially vulnerable to damage from the chemical.

It will take time to measure whether these regulatory actions have an impact on teen’s use of e-cigarettes.

“It’s really unfortunate that the devil is out of the gate,” says Donna Vallone, the chief research officer of the Truth Initiative, a public health organization in Washington D.C. “Now the question is how best to deal with the fact that you have this cohort of kids who have [had] a serious exposure to nicotine.”

Public health advocates, however, cheered the FDA’s proposal to ban menthol cigarettes. The government had banned cigarettes flavored with fruit or candy in 2009, but excluded menthol. In 2016, menthol cigarettes accounted for 35 percent of all cigarette sales.

Research has shown that young people are especially drawn to menthol cigarettes, and there is evidence that those kids whose first cigarettes are menthol flavored are more likely to become regular smokers.
===============
Astronomers spot another star that flickers like Tabby’s star
There’s another oddly flickering star in the galaxy.

Astronomers using a telescope in Chile have discovered a star whose strange dimming and brightening of light are reminiscent of Tabby’s star, which was once suggested to host an alien megastructure.

The megastructure idea, first posited in 2015, was later quashed by data suggesting that the dips are probably from dust particles obscuring the star’s light (SN Online: 1/3/18). The new star’s behavior is probably not due to aliens, either. But it is baffling, says astronomer Roberto Saito of the Federal University of Santa Catarina in Florianópolis, Brazil. He and his colleagues reported the star’s flickering November 6 on arXiv.org.

“We don’t know what the object is,” he says. “And that’s interesting.” The star could have some sort of orbiting debris that periodically blocks the starlight, but Saito and colleagues say they need more observations to figure out if that’s possible or if the flicker is caused by something else.

The researchers had been searching for supernovas, stars that suddenly brighten as they explode, when the team spotted the object in data taken with the VISTA telescope in the Atacama Desert in northern Chile. The data were part of a larger survey of the galaxy’s center called the VISTA Variables in the Vía Láctea, or VVV.

Instead of brightening, this star suddenly dimmed. The team called it VVV-WIT-07, for “What is this?”

From 2010 to 2018, the star’s brightness waxed and waned with no set pattern. That lack of pattern is similar to Tabby’s star, except VVV-WIT-07’s light dropped by up to 80 percent, while Tabby’s star dimmed by only about 20 percent.

There’s another flickering star, J1407, that might be a closer match. That star periodically dims by up to 95 percent, astronomer Eric Mamajek of the University of Rochester in New York and colleagues reported in 2012. Astronomers think J1407 hosts an orbiting planet with an enormous ring system that periodically eclipses the star (SN: 3/7/15, p. 5).

Finding multiple stars that all dim sporadically could mean that the sources of such flickering, whatever they are, must be relatively routine, says astronomer Tabetha Boyajian of Louisiana State University in Baton Rouge, who is also Tabby’s star’s namesake.

“If this phenomenon is the same as what’s happening with Tabby’s star, then we can’t invoke an elaborate explanation for what’s happening in both systems,” Boyajian says. “If you’re starting to see stars similar to this all over the place, then it’s got to be a really common thing that happens in nature. That’s really cool.”

But she’s not yet convinced that the stars are similar.

Because VVV-WIT-07 is located in the plane of the galaxy, the view from Earth to the star is full of dust, making it hard to make out details such as the star’s distance and even what kind of star it is. If it’s a young variable star, for instance, then its light dips might be internal. Then astronomers wouldn’t need to invoke orbiting rings or other strange things.

“Pretty much everything’s on the table for it right now,” Boyajian says. “We need more data.”

Saito and his colleagues hope to follow up on the star with bigger telescopes, like the 8.1-meter Gemini telescope or the Atacama Large Millimeter Array, both in Chile.
===============
It’s official: We’re redefining the kilogram
Out with the old — kilogram, that is.

Scientists will soon ditch a specialized hunk of metal that defines the mass of a kilogram. Oddly enough, every measurement of mass made anywhere on Earth is tied back to this one cylindrical object. Known as “Le Grand K,” the cylinder, cast in 1879, is kept carefully sequestered in a secure, controlled environment outside Paris.

On November 16, at a session of the 26th General Conference on Weights and Measures in Versailles, France, representatives of countries from around the world voted to kick that convoluted system to the curb, enacting a plan to redefine several units of measurement, in addition to the kilogram (SN: 11/12/16, p. 24).

For a small cadre of scientists called metrologists — those who specialize in the science of measurement — it’s a big day. “It’s about as excited as you're going to see metrologists get,” says David Newell of the National Institute of Standards and Technology in Gaithersburg, Md. He has spent much of his career working toward the change. “I can’t believe we’re finally getting it done.”

On May 20, 2019, Le Grand K will lose its special status, and the mass of a kilogram will be defined by a fundamental constant of nature known as the Planck constant. At the same time, other mainstays of the metric system will also be revamped: the ampere (the unit of electric current), the kelvin (the unit of temperature) and the mole (the unit for amount of substance).

Now, instead of being based on arbitrary quantities or physical artifacts that might change over time, “all the definitions will be based on what we call the fundamental constants of nature,” says metrologist Estefanía de Mirandés of the International Bureau of Weights and Measures in Sèvres, France.

Those unchanging numbers — which include the speed of light and the charge of the electron — are the same everywhere in the universe, making them useful pegs upon which to hang the metric system’s hat. “It's a very big change of paradigm, and now it’s complete,” de Mirandés says.

Most people won’t notice the switcheroo: A kilogram of ground beef will still make the same number of burgers. But metrologists say the change will put precision measurements on a firmer foundation. For example, it will be easier to measure masses that are much smaller than a kilogram, a feature that could be useful for tasks like doling out tiny quantities of pharmaceuticals.

In preparation for the kilogram’s update, several teams of scientists carefully measured the Planck constant, quantifying it to an accuracy of around 10 parts per billion. After May 20, the value of the Planck constant will be fixed at exactly 6.62607015 × 10−34 kilograms times meters squared per second.

As a result, Le Grand K will no longer be a perfect kilogram — its mass will have a fudge factor of plus or minus 10 micrograms. Despite Le Grand K’s loss of stature, metrologists will keep studying the object to understand how stable its mass is over time. Scratches or gunk on the surface of the object may cause its mass to change slightly, for example.

The kilogram’s history can be traced back to 1795, when France adopted a standardized system of units — the metric system. The kilogram was originally designed to be equal to the mass of a liter of water. Soon, the mass came to be represented by a cylinder, and other countries adopted the units.

A key idea behind the development of the metric system — known formally as the International System of Units — was that the units should be accessible to everyone, and should last forever. “When they defined the kilogram, they fell short of this,” says physicist Stephan Schlamminger, also of NIST. Only a select few people have access to Le Grand K, and instead countries rely on imperfect copies of the official kilogram. Soon, however, anyone with the right expertise will be able to use the fixed value of the Planck constant to measure mass, using a device known as a Kibble balance.

In celebration of the new, more accessible kilogram, Schlamminger and Newell had the Planck constant tattooed on their arms, along with the French phrase, “A tous les temps, à tous les peuples” — for all times and for all people — an ideal that the new kilogram will now meet.
===============
High school student generates electricity using biodegradable resources
Macdonald Chirara: Society for Science & the Public Community Innovation Award winner. Photo courtesy of Macdonald Chirara.

Student Innovations

In Macdonald Chirara’s community in Zimbabwe, people often face electricity shortages and they use firewood as a source of energy. This practice can add to increased rates of deforestation and contribute to global climate change. Chirara wants to offer an alternative way to produce electricity for his community.

To solve this problem, he created a biogas digester setup, which converts organic waste into electricity. The technology uses readily available resources such as animal waste and a local invasive plant to produce biogas.

“Biogas has the potential to provide clean renewable energy and to facilitate sustainable development of [an] energy supply for Zimbabwe and Africa at large,” Chirara says.

His device measured a maximum of 1.5 volts. “This electricity can be used especially in rural areas, where most households are not yet connected to the national grid, or in urban areas as a backup power source,” he says.

His work was selected by his local science fair for recognition with a Society for Science & the Public Community Innovation Award. This award honors students participating in science fairs around the world who are making a difference in their communities. In 2018, the Society rewarded 20 young scientists with $500 prizes — and Chirara was one of them.

Previous recipients include Madeleine Yang, from Bloomfield, Mich., who produced a more effective influenza vaccine; Shubh Dholakiya, from Rajkot, India, who built an accessible bike for disabled people; and Claire Wayner, from Baltimore, who studied ways to decrease bacteria in stormwater filtration systems.
===============
Lyme and other tickborne diseases are on the rise in the U.S. Here’s what that means.
There’s no sign that ticks are backing down.

A record high of 59,349 cases of tickborne diseases were reported in 2017 in the United States. That’s a 22 percent increase in cases — or roughly 11,000 more — than were reported in 2016, the Centers for Disease Control and Prevention announced on November 14.

Lyme disease accounted for most of the reported diseases, with nearly 43,000 cases in 2017, up from over 36,000 in 2016. There were increases in all six tick-related illnesses reported, though, including Rocky Mountain spotted fever. Because underreporting is common, experts expect the actual number of cases is higher than what the data show.

“The United States is not fully prepared to control these threats,” the agency said in statement.

The Tick-Borne Disease Working Group, set up by Congress in 2016 to address the threats that ticks pose, also released its first report on November 14, with input from public health officials, scientists, patients and clinicians. Science News discussed the findings with the working group’s chairman, infectious disease physician John Aucott, who is also the director of the Lyme Disease Research Center at Johns Hopkins School of Medicine.

His answers were edited for length and clarity.

SN: What’s the main takeaway of this new report?

Aucott: The key message is these tickborne diseases are serious, even potentially deadly, rapidly growing threats, and they affect hundreds of thousands of people.

So you [have to] combine prevention and accurate diagnoses and treatment.

SN: What are the gaps in prevention, diagnosis or treatment of these illnesses that you identified?

Aucott: A big component of prevention is the potential for future vaccine development [and] better ways to control ticks or control mice that harbor the infections. And there was an almost universal consensus that there needs to be improvement in diagnostic tests. We want to identify, diagnose and treat at the earliest stage, where the prognosis is the best.

We also need to fill in gaps in knowledge about how to better take care of the patients that don’t recover from their tickborne disease. [But] it’s hard to know what the best treatment is for the people that don’t get better until we understand the biology of what’s causing that ongoing illness.

One overriding theme of the report was education. We need to do better at educating physicians because they don’t always know what that Lyme bull’s-eye rash looks like. It’s usually just round and red without the Target department store bull’s-eye appearance to it. So that’s an example of educational things that are pretty simple to do, but at a state and local level and national level aren’t really happening as well as they could.

Steadfast rise The number of reported tickborne illness in the United States has risen overall since 2004. But the actual number of cases is likely even higher since many cases go unreported, the CDC says. Number of tickborne disease cases in the U.S. (2004–2017)

SN: What worries you the most about this increase in tickborne diseases?

Aucott: It’s really dramatic how many people are suffering from tickborne diseases. If you live in an area on the East Coast, everybody knows somebody who has problems with having had Lyme disease.

[The working group] heard from hundreds and hundreds of people that are living in silence in a lot of ways with this chronic illness that doesn’t kill them but really destroys the quality of their life. We don’t understand why this happens to some people. Until we understand that, it’s really hard to know how to help them. And that’s really what keeps me awake at night.

SN: Why are the number of cases of tickborne diseases increasing?

Aucott: It’s because the ticks are spreading. As the infected ticks spread geographically, more people are exposed to infected ticks, and you have more disease.

You have to have ticks in the same place as you have the animals that are reservoirs for the infections. The reservoir that carries the bacteria that cause Lyme disease, for example, are small animals like mice. So you have the mice that carry the bacteria, you have the ticks that spread the bacteria from mouse to mouse and then from mouse to humans. Then you add in the deer, which are the major food source for adult ticks. It’s bringing those all together.

The environmental geography of the United States has changed in a way to make that all very possible. We went from an agricultural economy, where all the land was clear-cut and there were no forests, [to] forests returning. These fragmented forests support that ecological niche for all those things to live together.

SN: What’s the message for those worried about tick exposure outdoors?

Aucott: We do lots of things that are way riskier than hiking in the woods. Way more people die every year driving their car on the Baltimore beltway, but we don’t stop driving our car. We just wear seat belts, we have airbags, we don’t text while we drive.

We want to apply the same precautions to hiking in the woods that we would do to driving our car. I’m a huge outdoorsman, but I always wear permethrin-treated clothing. I always wear long pants, even though it’s 95° F in Baltimore.

SN: Are there indications that cases will continue to rise?

Aucott: The rise of tickborne disease has been upward for the last 20 years. And there’s no indication at all that it’s going to slow down. New territory is being inhabited by infected ticks, and there’s really no barrier to that continuing to happen across large parts of the middle of the country in areas like Ohio and Tennessee and Kentucky [where tickborne diseases haven’t historically been reported].
===============
California's wildfires threaten more homes due to urban expansion
In the past week, the Camp Fire has killed at least 56 people and leveled the Northern California town of Paradise. Another wildfire raging through the Los Angeles suburbs, the Woolsey Fire, has already destroyed more than 500 buildings and forced some 250,000 people to evacuate their homes.

Such disasters are likely to occur more frequently in the coming years, data from recent years suggest. That’s because urban development is creeping further into woodlands, prairies and other natural areas and putting more communities in the path of wildfires. The ongoing California fires have been fueled by drought and high winds, but it’s their proximity to people that has made them especially deadly and destructive — burning through areas where housing abuts grasslands or forests, or where natural vegetation is mixed in with homes.

In California, these “wildland-urban interface areas” expanded almost 20 percent from 1990 to 2010, according to data published in 2017 by the U.S. Forest Service. And the number of homes in that zone increased by almost 34 percent.

Urban expansion into natural areas isn’t unique to California. Nationwide, the wildland-urban interface grew about 33 percent from 1990 to 2010, researchers who worked on the Forest Service dataset reported in March in the Proceedings of the National Academy of Sciences. And other Western states that face frequent wildfires have seen even larger leaps: Colorado’s wildland-urban interface areas expanded by 65 percent, Montana by 67 percent and Idaho by 72 percent, over the same period, the Forest Service found.

Meanwhile, climate change is contributing to more intense droughts and a longer fire season in California and other Western states. The types of wildfires that once occurred every few years are now happening multiple times a year, says Kurt Henke, retired chief of the Sacramento Metropolitan Fire District.

But other Western states “just don't have the population centers” that California has, Henke says. That new fire regime in a place like California, with lots of people living close to ample fire fuel, is a “recipe for disaster.”

Wildland-urban interface fires are especially hard to manage because there aren’t yet good computer simulations for predicting how they’ll behave, says Volker Radeloff, a landscape ecologist at the University of Wisconsin–Madison who worked on the recent analyses. And dealing with such fires doesn’t fall squarely under the expertise of either wildland firefighters or municipal firefighters. “The wildland-urban interface sort of falls through the cracks. It's a messy middle — a little wild, but not truly wild.”

As of November 15, the Camp Fire was 35 percent contained, while firefighters had 52 percent of the Woolsey Fire under control. Once the fires are tamed, rebuilding scorched communities can take years.

The problem is likely to get worse as people continue to build homes closer to natural areas, Radeloff says. He and his team calculated urban-interface expansion in each U.S. state using 2010 U.S. census data, and will update their findings once the 2020 census results are released.

California has some of the strictest fire codes in the country, including regulations on new construction in the wildland-urban interface. Buildings must be made of fire-resistant materials, for example, and residents are required to clear brush away from near their houses.

But even housing developments built five years ago may not be able to withstand the severity and frequency of fires in California today, Henke says. “We're going to have to go back and take another look.”
===============
Mini ‘solar panels’ help yeast shine at churning out drug ingredients
Bionic microbes outfitted with tiny semiconductor components can generate useful chemicals more efficiently than normal cells.

Microorganisms like fungi are commonly used in biomanufacturing to convert simple carbon-based molecules, such as sugar, into a wide range of chemical ingredients for pharmaceuticals and other products. But much of a microbe’s carbon intake typically gets used to power the creature itself, cutting the amount available to form desired chemicals.

In the new setup, described in the Nov. 16 Science, microbial cells are coated in semiconductor nanoparticles that absorb and transfer energy from sunlight to the cell, similar to the way rooftop solar panels supply energy to a house. That process allows the cell to funnel carbon it would normally use as a fuel toward its chemical output instead.

Chemical and biological engineer Neel Joshi of Harvard University and colleagues tested this scheme using baker’s yeast cells covered in nanoparticles made of the semiconductor indium phosphide. Baker’s yeast consumes the sugar glucose to produce shikimic acid, which is used to make the flu medication Tamiflu. In lab experiments, cyborg microbes equipped with nanoparticles produced about three times as much shikimic acid as normal baker’s yeast fed the same amount of glucose.

Light-harvesting nanoparticles could boost the chemical output of other microbes, as well, such as yeast cells that generate benzylisoquinoline alkaloids, the family of chemicals that includes morphine (SN Online: 6/17/15). Microbe-semiconductor hybrids could also increase the production of ingredients for multivitamins, fragrances, renewable fuels and other materials.
===============
Coffee or tea? Your preference may be written in your DNA
Whether people prefer coffee or tea may boil down to a matter of taste genetics.

People with a version of a gene that increases sensitivity to the bitter flavor of caffeine tend to be coffee drinkers, researchers report online November 15 in Scientific Reports. Tea drinkers tended to be less sensitive to caffeine’s bitter taste, but have versions of genes that increase sensitivity to the bitterness of other chemicals, the researchers found.

It’s long been thought that people avoid eating bitter foods because bitterness is an indicator of poison, says John Hayes, a taste researcher at Penn State who was not involved in the study. The coffee and tea findings help challenge that “overly simplistic ‘bitter is always bad, let’s avoid it’” view, he says.

In the new study, researchers examined DNA variants of genes involved in detecting the bitter taste of the chemicals caffeine, quinine — that bitter taste in tonic water — and propylthiouracil, or PROP, a synthetic chemical not naturally found in food or drink. Other bitter components naturally in coffee and tea may trigger the same taste responses as quinine and PROP do, Hayes says.

Researchers in Australia, the United States and England examined DNA from more than 400,000 participants in the UK Biobank, a repository of genetic data for medical research. Participants also reported other information about their health and lifestyle, including how much tea or coffee they drink each day.

The team added up each person’s variants in the taste genes, creating a genetic score for how intensely the person tastes each of the bitter chemicals. The researchers then compared those scores to the people’s reported beverage choices.

People who had the highest genetic score for detecting caffeine’s bitterness were 20 percent more likely to be heavy coffee drinkers, downing four or more cups a day, than those without the increased sensitivity, the researchers calculate.

Researchers had thought that people who are genetically inclined to taste bitter more intensely might avoid bitter beverages. “In this case, it’s strange how we’re seeking caffeine,” says study coauthor Marilyn Cornelis.

Coffee drinkers may have learned to enjoy caffeine’s bitterness because it’s a sign of the buzz the chemical provides. But tea drinkers may not actually like the bitterness of PROP and quinine, says Cornelis, a nutritional and genetic epidemiologist at Northwestern University Feinberg School of Medicine in Chicago. Rather, people tend to stick with either coffee or tea, so the tea result may just be a rejection of coffee.

It’s unclear how big of a role bitter taste genes play in determining whether someone chooses coffee or tea. In previous studies that sought genetic variants linked to coffee consumption, “taste genes did not come up,” Cornelis says. Instead, genes involved in breaking down caffeine may play bigger role in determining how much coffee or tea people drink (SN: 10/1/16, p. 14).
===============
A massive crater hides beneath Greenland’s ice
There’s something big lurking beneath Greenland’s ice. Using airborne ice-penetrating radar, scientists have discovered a 31-kilometer-wide crater — larger than the city of Paris — buried under as much as 930 meters of ice in northwest Greenland.

The meteorite that slammed into Earth and formed the pit would have been about 1.5 kilometers across, researchers say. That’s large enough to have caused significant environmental damage across the Northern Hemisphere, a team led by glaciologist Kurt Kjær of the University of Copenhagen reports November 14 in Science Advances.

Although the crater has not been dated, data from glacial debris as well as ice-flow simulations suggest that the impact may have happened during the Pleistocene Epoch, between 2.6 million and 11,700 years ago. The discovery could breathe new life into a controversial hypothesis that suggests that an impact about 13,000 years ago triggered a mysterious 1,000-year cold snap known as the Younger Dryas (SN: 7/7/18, p. 18).

Hidden bowl A crater was discovered at the edge of Hiawatha Glacier in northwest Greenland. Airborne radar data revealed a round depression (bottom image) buried beneath almost a kilometer of ice. Researchers also found deformed quartz minerals and other signatures of an ancient impact within sediments collected just outside the edge of the ice (black circle).

Members of the research team first spotted a curiously rounded shape at the edge of Hiawatha Glacier in northwest Greenland in 2015, during a scan of the region by NASA’s Operation IceBridge. The mission uses airborne radar to map the thickness of ice at Earth’s poles. The researchers immediately suspected that the rounded shape represented the edge of a crater, Kjær says.

For a more detailed look, the team hired an aircraft from Germany’s Alfred Wegener Institute that was equipped with ultra-wideband radar, which can send pulses of energy toward the ice at a large number of frequencies. Using data collected from 1997 to 2014 from Operation Icebridge and NASA’s Program for Arctic Regional Climate Assessment, as well as 1,600 kilometers’ worth of data collected in 2016 using the ultra-wideband radar, the team mapped out the inner and outer contours of their target.

The object is almost certainly an impact crater, the researchers say. “It became clear that our idea had been right from the beginning,” Kjær says. What’s more, it is not only the first crater found in Greenland, but also one of the 25 or so largest craters yet spotted on Earth. And it has held its shape beautifully, from its elevated rim to its bowl-shaped depression.

“It’s so conspicuous in the satellite imagery now,” says John Paden, an electrical engineer at the University of Kansas in Lawrence and a member of the team. “There’s not another good explanation.”

On the ground, the team hunted for geochemical and geologic signatures of an asteroid impact within nearby sediments. Sampling from within the crater itself was impossible, as it remains covered by ice. But just beyond the edge of the ice, meltwater from the base of the glacier had, over the years, deposited sediment. The scientists collected a sediment sample from within that glacial outwash and several from just outside of it.

The outwash sample contained several telltale signs of an impact: “shocked” quartz grains with deformed crystal lattices and glassy grains that may represent flash-melted rock. The sample also contained elevated concentrations of certain elements, including nickel, cobalt, platinum and gold, relative to what’s normally found in Earth’s crust. That elemental profile points not only to an asteroid impact, the researchers say, but also suggests that the impactor was a relatively rare iron meteorite.

Determining when that iron meteorite slammed into Earth is trickier.

The ice-penetrating radar data revealed that the crater bowl itself contains several distinct layers of ice. The topmost layer shows a clear, continuous sequence of smaller layers of ice, representing the gradual deposits of snow and ice through the most recent 11,700 years of Earth’s history, known as the Holocene. At the base of that “well-behaved” layer is a distinct, debris-rich layer that has been seen elsewhere in Greenland ice cores, and is thought to represent the Younger Dryas cold period, which spanned from about 12,800 to 11,700 years ago. Beneath that Younger Dryas layer is another large layer — but unlike the Holocene layer, this one is jumbled and rough, with undulating rather than smooth, nearly flat smaller layers.

“You see folding and strong disturbances,” says study coauthor Joseph MacGregor, a glaciologist with Operation IceBridge. “And below that, we see yet deeper, complex basal ice.” Radar images of that bottommost ice layer within the crater show several curious peaks, which MacGregor says could represent material from the ground that got incorporated into the ice. “Putting that all together, what you have is a snapshot of an ice sheet that looked fairly normal during the Holocene, but was quite disturbed before that.”

Those data clearly suggest that the impact is at least 11,700 years old, Kjær says. And the rim of the crater appears to cut through a preexisting ancient river channel that must have flowed across the land before Greenland became covered with ice about 2.6 million years ago.

That time span — essentially, the entire Pleistocene Epoch — is a large range. The team is working on further narrowing the possible date range, with more sediment samples, simulations of the rate of ice flow and possibly cores collected from within the crater.

The date range does include the possibility that the impact occurred near the onset of the Younger Dryas. “It’s the woolly mammoth in the room,” MacGregor says.

HIDDEN TREASURE A newly discovered 31-kilometer-wide crater, long buried underneath Greenland’s ice sheet, may be the remnants of an iron asteroid that struck Earth sometime in the distant past.

Planetary scientist Clark Chapman of the Southwest Research Institute in Boulder, Colo., notes that “there are plenty of roughly circular landforms on Earth of many different sizes, most of which are not impact craters.” Still, he says, the paper presents several lines of evidence that strongly support the conclusion that the object is a crater, including the shocked quartz and the topography.

As for the idea that a crater may have formed within the last couple of million years, Chapman says, it’s “quite unlikely.” Such strikes are rare in general, he adds, and asteroids barreling into Earth are far more likely to land somewhere in an ocean. “[And] it would be at least a hundred times less likely that it could have happened so recently as to have affected the Younger Dryas.”

Regardless of when the crater formed, it is “a straight-up exciting discovery,” MacGregor says. “And we’re just happy not to have to keep it a secret anymore.”
===============
Belly bacteria can shape mood and behavior
Science News for Students

Science News for Students (www.sciencenewsforstudents.org) is an award-winning, free online magazine that reports daily on research and new developments across scientific disciplines for inquiring minds of every age — from middle school on up.

Belly bacteria can shape mood and behavior

When Margaret Morris goes to the grocery store, she fills her cart with french fries, cheesecakes, meat pies and other tasty treats. People often ask if she’s throwing a party. And she is, but her guests are lab rats. The animals are helping her study how a junk food diet affects the nonstop chemical “chatter” between the brain, the gut and all of the many microbes living in the gut. By eavesdropping on what those microbial freeloaders “tell” the brain, Morris and other scientists are revealing the extent to which foods can influence our feelings and behavior. — Bethany Brookshire

Read more: www.sciencenewsforstudents.org/belly.

Surprise! Fire can help some forests keep more of their water

In forests, abundant trees are good and fire is bad, right? Actually, the reverse can be true — especially in some dry parts of California, a new study concludes. Trees release some of the water they absorb into the air through pores in their leaves. Periodic tree loss due to wildfires clears out many of the young trees, leaving fewer plants to pull water from the soil. With less competition from other plants, the larger trees can grow and remain healthy. Not suppressing wildfires in the 5,310-square-kilometer American River basin could save an estimated 773 billion liters of water per year that’s not being lost to the air, the study finds. — Michelle Donahue

Read more: www.sciencenewsforstudents.org/forest-fire.

This robot can wash a skyscraper’s windows

Washing windows on a high-rise can be a dangerous job, notes Oliver Nicholls. So the Australian teen created a robot to do it. About the size of a medium-sized picnic cooler, the robot withstands winds of up to 45 kilometers per hour. The economically competitive, computer-controlled device sprays a window, then scrubs away dirt and leftover water. Next, propellers push the device off the glass so cables can bring the bot to the next window. This nifty invention earned Nicholls, age 19, $75,000 and the top prize at the Intel International Science and Engineering Fair in May. — Sid Perkins

Read more: www.sciencenewsforstudents.org/window-washer.
===============
Skull damage suggests Neandertals led no more violent lives than humans
Neandertals are shaking off their reputation as head bangers.

Our close evolutionary cousins experienced plenty of head injuries, but no more so than late Stone Age humans did, a study suggests. Rates of fractures and other bone damage in a large sample of Neandertal and ancient Homo sapiens skulls roughly match rates previously reported for human foragers and farmers who have lived within the past 10,000 years, concludes a team led by paleoanthropologist Katerina Harvati of the University of Tübingen in Germany.

Males suffered the bulk of harmful head knocks, whether they were Neandertals or ancient humans, the scientists report online November 14 in Nature.

“Our results suggest that Neandertal lifestyles were not more dangerous than those of early modern Europeans,” Harvati says.

Until recently, researchers depicted Neandertals, who inhabited Europe and Asia between around 400,000 and 40,000 years ago, as especially prone to head injuries. Serious damage to small numbers of Neandertal skulls fueled a view that these hominids led dangerous lives. Proposed causes of Neandertal noggin wounds have included fighting, attacks by cave bears and other carnivores and close-range hunting of large prey animals.

Paleoanthropologist Erik Trinkaus of Washington University in St. Louis coauthored an influential 1995 paper arguing that Neandertals incurred an unusually large number of head and upper-body injuries. Trinkaus recanted that conclusion in 2012, though. All sorts of causes, including accidents and fossilization, could have resulted in Neandertal skull damage observed in relatively small fossil samples, he contended (SN: 5/27/17, p. 13).

Harvati’s study further undercuts the argument that Neandertals engaged in a lot of violent behavior, Trinkaus says.

Still, the idea that Neandertals frequently got their heads bonked during crude, close-up attacks on prey has persisted, says paleoanthropologist David Frayer of the University of Kansas in Lawrence. The new report highlights the harsh reality that, for Neandertals and ancient humans alike, “head trauma, no matter the level of technological or social complexity, or population density, was common.”

Harvati’s group analyzed data for 114 Neandertal skulls and 90 H. sapiens skulls. All of these fossils were found in Eurasia and date to between around 80,000 and 20,000 years ago. One or more head injuries appeared in nine Neandertals and 12 ancient humans. After statistically accounting for individuals’ sex, age at death, geographic locations and state of bone preservation, the investigators estimated comparable levels of skull damage in the two species. Statistical models run by the team indicate that skull injuries affected an average of 4 percent to 33 percent of Neandertals, and 2 percent to 34 percent of ancient humans.

Estimated prevalence ranges that large likely reflect factors that varied from one locality to another, such as resource availability and hunting conditions, the researchers say.

Neandertals with head wounds included more individuals under age 30 than observed among their human counterparts. Neandertals may have suffered more head injuries early in life, the researchers say. It’s also possible that Neandertals died more often from head injuries than Stone Age humans did.

Researchers have yet to establish whether Neandertals experienced especially high levels of damage to body parts other than the head, writes paleoanthropologist Marta Mirazón Lahr of the University of Cambridge in a commentary in Nature accompanying the new study.
===============
Sound-absorbent wings and fur help some moths evade bats
Some moths aren’t so easy for bats to detect.

The cabbage tree emperor moth has wings with tiny scales that absorb sound waves sent out by bats searching for food. That absorption reduces the echoes that bounce back to bats, allowing Bunaea alcinoe to avoid being so noticeable to the nocturnal predators, researchers report online November 12 in the Proceedings of the National Academy of Sciences.

“They have this stealth coating on their body surfaces which absorbs the sound,” says study coauthor Marc Holderied, a bioacoustician at the University of Bristol in England. “We now understand the mechanism behind it.”

Bats sense their surroundings using echolocation, sending out sound waves that bounce off objects and return as echoes picked up by the bats’ supersensitive ears (SN: 9/30/17, p. 22). These moths, without ears that might alert them to an approaching predator, have instead developed scales of a size, shape and thickness suited to absorbing ultrasonic sound frequencies used by bats, the researchers found.

The team shot ultrasonic sound waves at a single, microscopic scale and observed it transferring sound wave energy into movement. The scientists then simulated the process with a 3-D computer model that showed the scale absorbing up to 50 percent of the energy from sound waves.

What’s more, it isn’t just wings that help such earless moths evade bats. Other moths in the same family as B. alcinoe also have sound-absorbing fur, the same researchers report online October 18 in the Journal of the Acoustical Society of America.

Holderied and his colleagues studied the fluffy thoraxes of the Madagascan bullseye moth and the promethea silk moth, and found that the fur also absorbs sound waves through a different process called porous absorption. In lab tests, the furry-bellied moths absorbed as much as 85 percent of the sound waves encountered. Researchers suspect that the equally fluffy cabbage tree emperor moth also has this ability.

Other moths that have ears can hear bats coming, and can quickly swerve out of the way of their predators, dipping and diving in dizzying directions (SN: 5/26/18, p. 11). Some moths also have long tails on their wings that researchers suspect can be twirled to disrupt bats’ sound waves (SN: 3/21/15, p. 17). Still other moths produce toxins to fend off foes.

Having sound-absorbent fur and scales “might require a lot less energy in terms of protection from the moth's side,” says Akito Kawahara, an evolutionary biologist at the Florida Museum of Natural History in Gainesville who was not involved with the study. “It's a very different kind of passive defense system.”

Holderied and his colleagues hope next to study how multiple scales, locked together, respond to ultrasonic sound waves. The findings could one day help in developing better soundproofing technology for sound engineers and acousticians.
===============
U.S. cases of a polio-like illness rise, but there are few clues to its cause
The cause of a rare polio-like disease continues to elude public health officials even as the number of U.S. cases grows.

Confirmed cases of acute flaccid myelitis cases have risen to 90 in 27 states, out of a possible 252 under investigation, the U.S. Centers for Disease Control and Prevention announced November 13. That’s up from 62 confirmed cases out of 127 suspected just a month ago (SN Online: 10/16/18). There were a record 149 cases in 2016.

“I understand parents want answers,” Nancy Messonnier, director of the CDC’s National Center for Immunization and Respiratory Diseases in Atlanta, said at a news conference. The agency continues to investigate the disease, which causes weakness in one or more limbs and primarily affects children. But “right now the science doesn’t give us an answer,” she said.

A deep dive into 80 of the confirmed cases offered some details about the course of AFM. In most, fever or respiratory symptoms like coughing and congestion, or both, preceded limb weakness by three to 10 days. Most cases involved weakness in an upper limb, researchers report online November 13 in the Morbidity and Mortality Weekly Report.

Only two samples of cerebrospinal fluid — the clear fluid that bathes the brain and spinal cord — tested positive for a pathogen, each for a different enterovirus. Since 2014, when the first big outbreak of AFM occurred, most AFM spinal fluid samples haven’t produced a culprit, Messonnier said. The body may clear the pathogen or it hides in tissues, she said, or the body’s own immune response to a pathogen may lead to spinal cord damage.

“This time of year, many children have fever and respiratory symptoms [and] most of them do not go on to develop AFM,” Messonnier said. “We’re trying to figure out what the triggers are that would cause someone to develop AFM later.”
===============
How mammoths competed with other animals and lost
The Gray Fossil Site, a sinkhole in northeastern Tennessee, is full of prehistoric treasures. Between 7 million and 4.5 million years ago, rhinoceroses, saber-toothed cats and other creatures, even red pandas, perished here by the edge of a pond. But that bounty of fossils pales next to the site’s biggest find: a mastodon’s skeleton, nearly 5 million years old, preserved in exquisite detail all the way down to its ankle bones. “It is just fantastic,” says Chris Widga, a paleontologist at East Tennessee State University in nearby Johnson City.

The ancient elephant relative became known as Ernie because it was enormous, calculated soon after its 2015 discovery to have weighed 16 tons in life. The name came from musician Tennessee Ernie Ford, known for the coal-mining song “Sixteen Tons.” Since then the researchers have revised the mastodon’s weight down to 10.5 tons, says Widga, but the name stuck.

Ernie is still the biggest mastodon ever found in North America. He would have dwarfed today’s large African elephants, which average up to six tons. Excavators are working to dig up the rest of Ernie’s bones before this winter, with an eye to reassemble the ancient beast, the researchers reported in October in Albuquerque at a meeting of the Society of Vertebrate Paleontology.

Ernie is a jaw-dropping example of the ancient elephants that once roamed Earth. Scientists have found the remains of mastodons and their relatives, the mammoths, throughout the Northern Hemisphere — from huge tusks buried in the Alaskan permafrost to mummified baby mammoths in Siberia (SN Online: 7/14/14).

Now, researchers are knitting together these scattered discoveries into a more coherent picture of the lives and deaths of mammoths and mastodons. Scientists are exploring what plants these megaherbivores ate as they rambled across the landscape, and how they competed with other animals — including humans — as climate changed and the last ice age ended some 11,700 years ago.

Clues to these mysteries lie in ancient teeth and bones. Tiny scratches on the teeth of mastodons from North America suggest that they ate a surprisingly varied diet of grasses, twigs and other plants, depending on their environment. A recent analysis of the chemistry of European mammoth bones reveals that those animals probably struggled with dwindling food sources as the climate warmed, which probably hastened the animals’ demise.

Excavating some of the last known sites where mammoths and humans coexisted points to how early Americans gathered around a kill, making the most of the giant carcass to feed themselves.

Scientists hope to better understand the extinct elephants’ role in ancient ecosystems. “How did these big herbivores respond to climatic shifts, both before and after humans arrived?” asks Hendrik Poinar, a geneticist and anthropologist at McMaster University in Hamilton, Canada. “How resilient were these populations — or not?”

The answers may even help biologists eke out lessons about how modern elephants might cope as habitats shrink and hunting pressures rise.

Regional diets

Roughly a dozen species of mammoths and mastodons ranged across the globe at different times in the last 25 million years. The last of them died out for the most part at the end of the Pleistocene Epoch, which marked the end of the last ice age. The most famous is the woolly mammoth (Mammuthus primigenius), which appeared on the scene relatively late, around 350,000 years ago, and survived long enough to coexist with early humans in North America, Europe and Asia. Its shaggy coat and upturned tusks have made it an ice age icon, famous for roaming northern grasslands alongside saber-toothed cats, cave bears and other extinct beasts.

North America also had the Columbian mammoth (Mammuthus columbi), which arose about 1 million years ago and was bigger and less hairy than the woolly mammoth. It wandered as far south as Central America and left its heavy footprints in places like White Sands National Monument in New Mexico. Park rangers there have studied vast “trample grounds,” where herds of Columbian mammoths once thundered across the landscape.

A third extinct relative of elephants is the mastodon, including the American version (Mammut americanum). Mastodons were typically smaller and longer-bodied than mammoths, and quite a bit heftier. “We often think of mammoths as the supermodels of the Pleistocene, long, slender, very tall animals for their weight,” Widga says. In contrast, “mastodons are stocky.”

To tell a mammoth from a mastodon, start at the teeth. Mastodon teeth have cone-shaped tips, unlike the broad, flat teeth of mammoths. That suggests that mastodons gnawed on more branches, twigs and leafy things as opposed to the grasses that mammoths ground between their teeth.

With new detailed dental studies, researchers are getting a closer look at the animals’ diets. Paleoecologists Gregory Smith and Larisa DeSantis of Vanderbilt University in Nashville recently teamed up with Jeremy Green, a paleontologist at Kent State University in Ohio. They looked at patterns of wear, like the small pits left by nuts or acorns and the elongated scratches left by blades of grass. The team’s study of 65 mastodons from across North America, dating from 51,000 to 11,000 years ago, showed one group of mastodons ate very different plants than another, depending on where the animals lived. In Florida, the teeth indicated that the mastodons had been chewing on relatively soft material, perhaps the delicate tips of cypress trees. In Missouri, mastodons ate harder materials, such as seeds and bark. In New York, they chewed on conifer needles and twigs.

This rare effort to look at mastodon diets across a big geographic area, reported last year in Palaeogeography, Palaeoclimatology, Palaeoecology, shows that mastodons were adaptable. They chomped whatever trees and shrubs were common in their habitat. “It really hadn’t been proven until we started looking at it,” Smith says.

Those mastodons, at least, were flexible enough to change food sources as they migrated across the landscape. Another big ice age herbivore was not as adaptable, Smith reported in October at the Albuquerque meeting.

Living across both North and South America were the gomphotheres (including the genus Cuvieronius). These elephant relatives were smaller than mammoths and mastodons and had a body shape and size more like a modern elephant. Gomphotheres were hunted by early Americans (SN: 8/9/14, p. 7), but the creatures had also begun to dwindle well before people arrived on the scene.

They came and went Among many species of ancient elephant relatives, American mastodons arose about 5 million years ago, followed by gomphotheres about a million years later. In North America, gomphotheres were eventually outcompeted by mastodons and mammoths. These giant creatures had all died out by 11,000 years ago, except for a few isolated populations of woolly mammoths that lingered another 7,000 years. A look at some ancient elephant relatives American mastodon (Mammut americanum)

Lived: 5 million years ago to 11,000 years ago

Size: Up to 3 meters at the shoulder, weight up to six tons

Diet: Primarily trees and other woody material : 5 million years ago to 11,000 years ago: Up to 3 meters at the shoulder, weight up to six tons: Primarily trees and other woody material Gomphothere (Cuvieronius)

Lived: 4 million years ago to 11,000 years ago

Size: Up to 2.3 meters at the shoulder, weight up to 3.5 tons

Diet: Grasses, trees and a wide variety of other plants : 4 million years ago to 11,000 years ago: Up to 2.3 meters at the shoulder, weight up to 3.5 tons: Grasses, trees and a wide variety of other plants Columbian mammoth (Mammuthus columbi)

Lived: 1 million years ago to 11,000 years ago

Size: More than 4 meters at the shoulder, weight up to 10 tons

Diet: Mainly grasses and other plants Woolly mammoth (Mammuthus primigenius)

Lived: 350,000 years ago to 11,000 years ago *(isolated populations lingered another 7,000 years)

Size: Up to 3.5 meters at the shoulder, weight up to six tons

Diet: Grasses and other plants : 350,000 years ago to 11,000 years ago *(isolated populations lingered another 7,000 years): Up to 3.5 meters at the shoulder, weight up to six tons: Grasses and other plants

The decline of the gomphotheres is surprising because they could eat just about any plant, from woody material to grasses. In theory, the animals should have been able to adapt to any food source. And yet they were apparently unable to cope as mammoths and mastodons moved into their chomping grounds, and as climate change squeezed the available resources.

To find out why, Smith compared patterns of tooth wear and other evidence from mammoths, mastodons and gomphotheres that once lived along the Gulf coastal plains of Texas and Florida. Starting around 1.8 million years ago, gomphotheres switched from grazing to eating a wider range of foods, Smith found. But the mammoths were already well specialized for eating grasses, and mastodons for eating the woodier plants. Gomphotheres couldn’t compete with the other elephants, Smith reported.

Ultimately, gomphotheres began to disappear from the scene. Only a few lingered until their final extinction, by at least 11,000 years ago.

Competing interests

On the other side of the Atlantic Ocean, a similar battle for resources in the face of climate change unfolded. This time, though, mammoths were competing with horses.

Chemical clues in an animal’s teeth and bones show variations of elements, or isotopes, specific to the types of plants or meat eaten.

Certain plants contain extra neutrons in the atomic nuclei of some of their elements. That distinction is reflected in the isotopic makeup of the skeletons of animals that ate those plants. Meat eaters retain a record of the plant eaters that they ate.

Compared with other herbivores, mammoths have unusual isotopes. Their bones are typically higher in the isotope nitrogen-15, even when compared with horses and other grazing animals in the same region. It may be that mammoths preferred to eat mature and dry grasses, which are higher in nitrogen-15 than younger, greener grasses preferred by other grazers.

But there’s one place where mammoths did not show the high nitrogen-15 levels: a site called Mezhyrich in Ukraine, which is famous for its prehistoric huts made of mammoth bones. Mezhyrich bones contain far less nitrogen-15 than is typical for mammoths. “For me, it was something absolutely new and unusual,” says biogeochemist Dorothée Drucker of the University of Tübingen in Germany.

To see what was going on, she and colleagues recently studied mammoth bones from other sites near Mezhyrich. All date to around 18,000 to 17,000 years ago, a time when the landscape was gradually warming. These other bones, too, contained surprisingly low levels of nitrogen-15. In fact, they were as low as the nitrogen-15 levels found in horse bones from nearby and dating to the same time period, the researchers reported in a paper published online in June in Quaternary Research.

That suggests that mammoths weren’t grazing on their usual grasses rich in nitrogen-15. Instead, something had apparently forced them to shift to a new menu. Perhaps the changing climate altered the types of vegetation growing in the mammoth landscape, shifting from rich and diverse grasslands to a less productive shrubland. Having to compete with other grazers, such as horses, for this less-preferred diet some 17,000 to 13,800 years ago may have been one of the last straws.

Dinner camp

Of course, climate change wasn’t the only thing stressing mammoths and mastodons as the last ice age wound to a close.

People hunted mammoths across Europe and northern Asia for thousands of years, possibly contributing to the animals’ gradual decline (SN: 7/27/13, p. 10). In North America, the downfall was more abrupt. Mammoths and mastodons roamed without major predators for hundreds of thousands of years or more. Then humans crossed a land bridge from Siberia to Alaska probably some time after 16,000 years ago (SN Online: 8/8/18), bringing with them the knowledge of how to use spears to take down the huge hairy beasts.

Scientists have argued for decades about how much human hunting versus climate and other environmental changes contributed to the death of North America’s mammoths and mastodons. Todd Surovell, an archaeologist at the University of Wyoming in Laramie, says the evidence points mostly to people. “Humans arrived to a continent full of large naïve animals,” he says. The mammoths were “easy pickings,” he says, “practically like a herd of cattle.”

Heavy eaters Three humans who lived in Crimea 38,000 to 33,000 years ago ate mammoth and other meat, as determined by unique nitrogen values in their bones (each color bar represents a person’s protein intake). Early humans in Crimea ate mammoths

Surovell studies archaeological sites where humans butchered mammoths, mastodons or gomphotheres. He and colleagues have spent the last few years excavating a 12,900-year-old site known as La Prele, in eastern Wyoming. It is one of about 15 butchery sites known of in North America.

When first discovered in 1986, the site yielded part of a mammoth and a few stone tools. In 2014, Surovell and his team stumbled on archaeological gold. While widening a path 12 meters away from the 1986 find, a team member’s shovel struck a large stone artifact, a tool probably used for chopping. “All of a sudden the site expanded hugely,” Surovell says.

Since then, the researchers have unearthed a dramatic story of how early hunters gathered triumphantly around their kill. The mammoth bones mark where the animal lay; nearby is a string of fire pits, presumably where people camped as they butchered the meat. Near the campfires lie domestic artifacts such as bone needles and bone beads, which suggest that several families or a small village temporarily settled around the mammoth kill.

“We always expected to find campsites associated with animals, but this is only the second time we’ve found this kind of thing archaeologically” in North America, Surovell says. For a smaller animal, hunters might have cut up the carcass and carried the meat back to their camp. That wasn’t possible for a mammoth, which could have weighed up to 10 tons. Instead, the people “moved their camp to the mammoth,” he says. Surovell has presented findings from the La Prele site at various small archaeology meetings.

Based on the number of artifacts at La Prele, Surovell thinks the people may have stayed around the carcass for perhaps a week, feasting and drying meat to take with them. There’s no question that humans ate mammoth; Drucker, among others, has found high nitrogen-15 levels in the bones of early European humans, which suggest the people derived a large fraction of their protein from mammoth meat.

Ultimately, many experts say, mammoths and mastodons probably went extinct because of some combination of human hunting and climate change, with those factors varying around the globe. Different species winked out at different times in different locations; most vanished by around 11,000 years ago as the great northern ice sheets receded and temperatures rose. A few isolated herds hung on for another few thousand years.

One group of woolly mammoths made it until about 5,600 years ago on St. Paul Island, north of Alaska’s Aleutian Islands. The animals probably died out when the island’s lakes dried up (SN Online: 8/1/16). Another group survived all the way until 4,000 years ago on Wrangel Island off Siberia, where genetic studies suggest that the creatures eventually succumbed to too much inbreeding.

Northern exposure One dominant species of mammoth, the woolly, ranged across large swaths of the Northern Hemisphere (brown) near the end of the last ice age. The species crossed the land bridge between Siberia and Alaska but could not conquer mountain chains such as the Himalayas.

Once those last animals vanished, it was the end of the mammoth lineage. But understanding their fate may help researchers help modern elephants. Across Asia and Africa, elephants are facing some of the same stresses that mammoths and mastodons did long ago. Climate change is reshaping the landscape. Humans are hunting elephants and destroying their habitat.

The lessons of the past might help conservationists come up with new ways to help elephants survive, Smith says. “The fossil record can tell us what happened in the past in similar circumstances,” he says. “My hope is that a better understanding of ancient ecology can give us some insight into the future.”

Poinar agrees. He and grad student Emil Karpinski are working on the biggest analysis of mastodon DNA. They have more than 100 samples gathered from around the Northern Hemisphere. (Sadly, Tennessee Ernie is too old for good DNA preservation.) The researchers hope to show how mastodon populations grew and shrank over time, and how those changes were linked to shifting climate and to human hunting.

“It’s no shock to say that humans have played a drastic role in extinctions in the past and are doing so as we speak,” Poinar says. “But if we were to leave species on their own, how would climate change affect their ability to be resilient?”

The answers, when they come, may just show what today’s elephants need to survive.

This article appears in the November 24, 2018 issue of Science News with the headline, "A Mammoth World: How ice age beasts competed with other animals and lost."
===============
Climate change may be making the Arctic deadlier for baby birds
Climate change may be flipping good Arctic neighborhoods into killing fields for baby birds.

Every year, shorebirds migrate thousands of kilometers from their southern winter refuges to reach Arctic breeding grounds. But what was once a safer region for birds that nest on the ground now has higher risks from predators than nesting in the tropics, says Vojtěch Kubelka, an evolutionary ecologist and ornithologist at Charles University in Prague. With many shorebird populations dwindling, nest success matters more every year.

A longtime fan of shorebirds, Kubelka had heard about regional tests of how predator risk changes by latitude for bird nests. He, however, wanted to go global. Shorebirds make a great group for such a large-scale comparison, he says, because there’s not a lot of variation in how nests look to predators. A feral dog in the United States and a fox in Russia are both creeping up on some variation of a slight depression in the ground.

So Kubelka and his colleagues crunched data from decades of records of predator attack rates on about 38,000 nests of various sandpipers, plovers and other shorebirds. After a massive literature search, the study zeroed in on the experiences of 237 populations of a total of 111 shorebird species at 149 places on six continents. It’s the first attempt at a global comparison by latitude of predator attack rates on shorebird nests over time, he says.

Historical data of predator attack rates worldwide averaged about 43 percent before 1999, but has since reached 57 percent, the team reports in the Nov. 9 Science. The most dramatic upward swoop came from the Arctic nest reports. There, the rate of predator attacks averaged around 40 percent in the last century, jumping to about 65 or 70 percent since 1999. Meanwhile, tropical perils in the Northern Hemisphere changed “only modestly” the researchers say, from around 50 percent to about 55 percent.

Researchers also looked at how much, and how erratically, temperatures had changed at each site. Overall, the growing dangers to nests fit with climate change trends.

Danger zone In the past few decades, the average number of nests attacked in 86 Arctic shorebird populations (pink) swerved up, surpassing even the average predator danger at 17 breeding grounds in the Northern Hemisphere’s tropical zone (tan) and 96 populations in the northern temperate zone (green). Annual shorebird nest attacks, 1944 – 2016

Biologists have discussed the idea that nest predation generally lessens when birds move out of the tropics. One advantage of migrating toward the pole to breed was, in theory, to escape from tropical abundance of snakes, rodents and other egg-lovers.

But rapid warming in the Arctic might have discombobulated some of the old predator-prey relationships, says coauthor Tamás Székely, a conservation biologist at the University of Bath in England. For instance, Arctic foxes used to get much of their nourishment from lemmings, voles and other small rodents. Skimpy snow cover in warmer winters, however, doesn’t insulate little rodents as well as it used to. Boom-and-bust cycles of lemming populations are in many places now “mostly bust,” he says. Foxes and other predators may be shifting more to bird eggs and nestlings.

That scenario of rodent-loving predators hunting more birds sounds “highly probable,” but may be just part of what’s going on, says Dominique Fauteux, an ecologist at the Canadian Museum of Nature in Ottawa who studies small mammals. Lemming collapses haven’t been reported across the whole Canadian Arctic, he says.

Instead, some researchers have proposed that shorebird nest failures come from a boom in geese that attract more bird predators overall. Also, a 2010 study suggests that nest predation in the Canadian Arctic was still lower than in temperate areas. There may be some global pattern, but on the ground, Fauteux says, “there clearly are nuances.”
===============
One of Earth’s shimmering dust clouds has been spotted at last
Meet the Kordylewski dust clouds, shimmering pseudo-satellites that orbit Earth near the moon. A team of Hungarian astronomers say they have spotted light scattered from one of these clouds, providing evidence that the clouds really exist after nearly 60 years of controversy.

The twin dust clouds gather at two of the points in space where the gravity of Earth and the moon cancel each other out. That gravitational stability makes these spots, called Lagrange points, good places to park spacecraft. They also could trap interplanetary debris.

No one had seen any dust clouds since 1961, when Polish astronomer Kazimierz Kordylewski reported the first sighting at two gravity holes, L4 and L5. Some astronomers thought that the sun’s stronger gravity would periodically sweep dust out of L4 and L5, making it hard for the areas to support clouds.

Astronomers Judit Slíz-Balogh, András Barta and Gábor Horváth, all of Eötvös Loránd University in Budapest, looked for the clouds using specially designed filters. These filters detect light that’s been polarized, or had its electromagnetic waves aligned, by bouncing around the dust grains.

The team spent several months making observations in Slíz-Balogh’s private observatory in the western Hungarian village of Badacsonytördemic. “It is hard to find moonless and cloudless good nights in Hungary,” the astronomers write in a paper set to be published in January in the Monthly Notices of the Royal Astronomical Society. But the team finally spotted a telltale shimmer at L5. The physics of the Lagrange points suggests that, if one cloud exists, the other does, too. The trio still wants to search for the L4 cloud directly.

Computer simulations suggest L4 and L5 are only partially stable, the team reports in a paper in the Nov. 11 MNRAS. The clouds may hang around for years or decades, but the sun’s gravity will eventually scatter them into space. That could explain the “now you see it, now you don’t” results of past searches for the clouds, the team says.
===============
Physicists wrangled electrons into a quantum fractal
Physicists have created an oddity known as a quantum fractal, a structure that could reveal new and strange types of electron behaviors.

Fractals are patterns that repeat themselves on different length scales: Zoom in and the structure looks the same as it does from afar. They’re common in the natural world. For instance, a cauliflower stalk looks like a miniature version of the full head. A lightning stroke splits into many branches, each of which has the same forked structure as the whole bolt.

But in the tiny quantum realm, fractals aren’t so easy to come by. Now scientists have artificially created a quantum fractal by placing carbon monoxide molecules on a copper surface. Confined between the molecules, electrons in the copper form a fractal shape of triangles within triangles called a Sierpinski triangle (SN Online: 12/30/02), the researchers report November 12 in Nature Physics. A full-fledged Sierpinski triangle would contain an infinite number of triangles, so the researchers created an approximation to that shape, with enough triangles for its repeating structure to be evident.

Electrons inhabiting a fractal don’t live in 3-D like the rest of us. Nor do they exist in a flat 2-D world or a one-dimensional line. Instead they occupy an in-between, fractional number of dimensions. In this case, the scientists found that the electrons lived in approximately the number of dimensions expected for a Sierpinski triangle, 1.58.

Quantum particles tend to act in unusual ways when confined to one or two dimensions (SN: 10/20/16, p.6). Scientists don’t yet know how electrons will behave in fractional dimensions, says physicist Cristiane Morais Smith of Utrecht University in the Netherlands. “What can come out of our work is completely uncharted territory.”
===============
Car tires and brake pads produce harmful microplastics
There’s a big problem where the rubber meets the road: microplastics.

Scientists analyzed more than 500 small particles pulled from the air around three busy German highways, and found that the vast majority — 89 percent — came from vehicle tires, brake systems and roads themselves. All together, these particles are classified by the researchers as microplastics, though they include materials other than plastic.

Those particles get blown by wind and washed by rain into waterways that lead to the ocean, where the debris can harm aquatic animals and fragile ecosystems, says environmental scientist Reto Gieré of the University of Pennsylvania. He presented the findings on November 6 at the annual meeting of the Geological Society of America in Indianapolis. Previous research has estimated that about 30 percent of the volume of microplastics polluting oceans, lakes and rivers come from tire wear.

“We all want to reduce CO 2 emissions” from vehicle exhaust, Gieré says. “But you can’t stop tire abrasion.” Traffic congestion makes the problem worse. Vehicles traveling at constant speeds, without so much brake use, produced fewer particles, the researchers found.

Because some materials, including synthetic rubber, become coated in dust and other tinier bits of debris, they’re not always easy to identify. The researchers figured out what each particle was by examining each of them under a scanning electron microscope and running chemical analyses.

“These [tire] particles are stealthy,” says John Weinstein, an environmental toxicologist at the Citadel in Charleston, S.C., who was not involved in the study.
===============
China is about to visit uncharted territory on the moon
China is about to make space history. In December, the country will launch the first spacecraft ever to land on the farside of the moon. Another craft, slated for takeoff in 2019, will be the first to bring lunar rocks back to Earth since 1976.

These two missions — the latest in China’s lunar exploration series named after the Chinese moon goddess, Chang’e — are at the forefront of renewed interest in exploring our nearest celestial body. India’s space agency as well as private companies based in Israel and Germany are also hoping for robotic lunar missions in 2019. And the United States aims to have astronauts orbiting the moon starting in 2023 and to land astronauts on the lunar surface in the late 2020s.

The time is ripe for new lunar exploration. Despite decades of study, Earth’s only natural satellite still contains mysteries about its formation as well as clues to the history of the solar system (SN: 4/15/17, p. 18). “There are too many things we don’t know,” says planetary scientist Long Xiao of China University of Geosciences in Wuhan. He is a coauthor of two studies published in June and July in the Journal of Geophysical Research: Planets describing the landing sites of the new Chinese missions, Chang’e-4 and -5.

To figure out what secrets the moon may still be hiding, scientists are excited to get their hands on new rock samples. The Chang’e-5 sample return mission “no doubt will have additional rock types that we haven’t sampled yet,” says planetary scientist David Blewett of Johns Hopkins University Applied Physics Laboratory in Laurel, Md. “If you came to the Earth and landed in Great Britain and made all your conclusions about the Earth from what you saw … you really wouldn’t have the whole picture.”

Journey to the dark side

The Chang’e-4 spacecraft includes a lander and a rover that were originally built as backups for the 2013 Chang’e-3 mission, which marked China’s first moon landing — and the first moon landing at all since the 1970s (SN Online: 12/16/13). The uncrewed Chang’e-3 lander-rover duo touched down in a vast lava plain in the north known as Mare Imbrium, where the craft measured the composition and thickness of the lunar soil and discovered what might be a new type of basalt, or lava-based rock.

This time, China has its sights set on lunar regions never before explored. Chang’e-4 is aiming for the moon’s largest, deepest and possibly oldest known feature created by an impact, the South Pole–Aitken basin, on the lunar farside, which always faces away from Earth. The whole basin, which is 2,500 kilometers wide and up to 8.2 kilometers deep, is too big for the rover to explore. So Chang’e-4 is shooting for the 186-kilometer-wide Von Kármán crater within the larger basin for a cosmic hole in one.

The enormous impact that formed the South Pole–Aitken basin is thought to have excavated parts of the lunar mantle, the once-molten layer of denser rock that sits below the crust. Exploring the crater could offer a window into the moon’s interior.

“There’s a big argument about the composition of the lunar mantle,” Xiao says. For instance, is the mantle “wet” and full of hydrated minerals, or dry? If it is wet, how did water survive the colossal impact thought to have formed the moon? Chang’e-4 won’t solve those mysteries, but its measurements can help calibrate future remote observations.

Three cameras, an infrared spectrometer and two ground penetrating radars, like those used in the Chang’e-3 mission, will help the spacecraft conduct its investigation of Von Kármán crater. Chang’e-4 also carries some newer tech: a Swedish instrument to study how charged particles from the sun interact with the lunar surface; a German instrument to gauge radiation levels, which could be important for future astronauts; and a container with seeds and insect eggs to test whether plants and insects, if they hatch, can grow together on the moon.

Because the moon always shows the same face to Earth, astronomers on the ground won’t be able to communicate directly with Chang’e-4. So in May, the Chinese space agency launched a transmission relay satellite to a point beyond the moon to bounce data and communication signals back and forth between the lunar surface and Earth (SN Online: 5/20/18). That satellite, called Queqiao, is named after the mythical bridge of magpies that spans the Milky Way once a year to enable a tryst between two lovers.

Delving into geologic history

Sometime in 2019, the Chang’e-5 craft will visit a region on the near side of the moon that no spacecraft or astronaut has been to before. And that mission will give scientists something they haven’t had in more than four decades — new lunar rock samples.

So far, scientists have studied rocks from lava fields formed early in the moon’s history, about 3.5 billion years ago. Those were brought to Earth by the U.S. Apollo missions, which ended in 1972, and the Soviet Luna missions, ending in 1976. Together, those missions brought back more than 380 kilograms of moon material.

Chang’e-5’s lander will scoop surface rocks and dig two meters deep in a 58,000-square-kilometer area called the Rümker region that’s strewn with minerals dating to a variety of periods of volcanic activity. The craft will then bundle up to two kilograms of material into a rocket, which will launch to meet Chang’e-5’s orbiter and return to Earth.

Marking the spot Moon rocks brought back by the Soviet Union’s Luna missions (yellow) and NASA’s Apollo missions (blue) in the 1960s and ’70s all came from ancient lava flows mostly clustered around the moon’s equator. In 2013, China’s Chang’e-3 (red square) landed in a different zone of old flows farther north, but brought back no samples. The Chang’e-5 mission will return volcanic rocks from an area that has never been sampled before (outlined in red).

Studying samples from this region could reveal if the moon has been geologically active more recently than previously thought. “According to the study of Apollo samples, people think the moon was dead” for the last 3 billion years, Xiao says. But observations from previous orbiters suggest that Rümker includes basalt from lava flows that are less than 1.4 billion years old. “If the young mare basalt were confirmed, we would rewrite the heat history of the moon” — in other words, when the moon’s hot liquid rock cooled and hardened (SN: 8/5/17, p. 7).

Understanding the moon’s volcanic history could shed light on competing ideas about how the moon came to be. For instance, scientists still don’t agree on whether our neighbor formed from one giant impact with Earth in the early days of the solar system, around 4.5 billion years ago, or from about 20 small ones, or something else. Finding evidence for more recent geologic activity could be a ding for the single impact hypothesis.

What’s more, the returned samples would also be stored and preserved “so that future scientists who aren’t born yet can answer future questions we haven’t asked yet, with tools we haven’t invented yet,” says astrochemist Jamie Elsila of NASA’s Goddard Space Flight Center in Greenbelt, Md. She would know: Born nearly two years after the last Apollo mission, Elsila published a study in 2016 that used modern techniques to show that Apollo soil samples contain amino acids mostly derived from Earth.

Tricky access to new moon rocks

The prospect of studying those new rocks has excited NASA researchers and other scientists. Sample return is “the gift that keeps on giving,” says former Apollo astronaut Harrison “Jack” Schmitt, the only geologist to walk on the moon. “All of my colleagues who work directly with the samples certainly would like to get their hands on [those new rocks].”

But U.S. scientists face roadblocks to studying the new samples, thanks to the Wolf Amendment, a 2011 federal budget clause that requires congressional approval before U.S. scientists can collaborate with China or any Chinese-owned company.

“In terms of space science, I think with the Wolf Amendment, the United States took very careful aim and shot ourselves in the foot,” says space policy analyst Joan Johnson-Freese of the U.S. Naval War College in Newport, R.I. “We’ve made it very difficult for American scientists to work with otherwise unobtainable data.”

Difficult, but not impossible. American scientists could join scientists from other countries who can work directly with China, using their colleagues as a sort of go-between. The United States could also trade Apollo samples for Chang’e-5 samples, says space policy analyst Scott Pace, the executive secretary of the U.S. National Space Council.

“I think the U.S. and Russia would certainly be open to being part of a sample exchange process,” Pace says. “From a purely science standpoint, we’d love to have that. Whether the politics allows it, we’ll have to see.”

From China, Xiao agrees that collaboration is essential to understanding the moon’s history. “We don’t want this kind of thing to badly impact the science.”
===============
Vitamin D supplements don’t prevent heart disease or cancer
CHICAGO — Taking a vitamin D supplement does not reduce the risk of having a potentially fatal heart attack or stroke or for getting an invasive cancer, according to highly anticipated results of a large clinical trial.

The VITAL trial found no significant difference in cancer or heart health risk between people taking 2,000 international units, or IU, of vitamin D a day and those who took a placebo, researchers reported November 10 at the American Heart Association’s annual scientific sessions. The results dim the luster of a vitamin once hailed as a drug that could strengthen bones and prevent conditions from obesity and diabetes to heart and autoimmune diseases.

“What this does show is that the general population does not need to be taking vitamin D for cardiovascular health or cancer health,” says Erin Michos, a preventive cardiologist at Johns Hopkins School of Medicine who was not involved in the study. “This is the most definitive trial to date on this issue.”

Researchers have known for a long time that people with low levels of vitamin D in their blood are at higher risk for heart attacks, strokes, heart failure and an irregular heartbeat known as atrial fibrillation. But VITAL, a Phase III clinical trial, is the largest randomized trial to specifically test whether boosting levels of the vitamin can prevent cardiovascular disease.

JoAnn Manson, an epidemiologist at Brigham and Women’s Hospital and Harvard Medical School in Boston, and her colleagues followed 25,871 U.S. participants — men age 50 and older and women age 55 and older — up to six years. Participants were relatively healthy, having no history of cardiovascular disease or cancer, except non-melanoma skin cancer, at the start. This trial also included 5,106 black participants, important because pigmentation reduces vitamin D production in the skin so people with darker skin have lower levels of the vitamin.

Among participants taking the daily dose of vitamin D, 396 suffered from a heart attack or stroke or died from cardiovascular disease, compared with 409 taking a placebo. The differences were similarly insignificant when the researchers looked at cancer: 793 of those taking vitamin D were diagnosed with invasive cancers — including breast, prostate and colorectal cancers — compared with 824 people taking a placebo. The results were published online November 10 in the New England Journal of Medicine.

In a Nov. 10 news conference on the results, Jane Armitage, an epidemiologist at University of Oxford not involved in the research, noted that the trial was large, ethnically diverse and balanced between men and women, and that adherence to the treatment was good. Because of that, “I think we need to accept that it’s a good test of the hypothesis that universal supplementation with a decent dose of vitamin D is not worthwhile,” she said.

Previous trials have hinted that vitamin D may fall short in preventing heart disease and cancer. But those trials have been smaller, used lower doses of vitamin D and most were designed to test the effects of the supplement on other health issues, such as bone strength.

Michos notes that vitamin D doesn’t appear to help bone health either. A recent study in the Nov. 1 Lancet Diabetes & Endocrinology found no evidence that vitamin D reduced fractures or falls and found little benefit for bone density.

“Low vitamin D in the blood might just be a marker of someone in a poorer health state in general,” Michos says.

Manson’s team also looked at whether taking a fish oil supplement in a preparation similar to what consumers can get over the counter affected heart health and cancer risk. As with vitamin D, the researchers found no significant difference between people taking a daily one-gram capsule containing omega-3 fatty acids and those taking a placebo. Those results also were presented at the heart meeting on November 10 and published online in NEJM the same day.
===============
A potent fish oil drug may protect high-risk patients against heart attacks
Cholesterol-lowering drugs may one day gain a sidekick in the battle against heart disease. Taking a potent drug derived from fish oil along with a statin lowers the risk of heart attack and stroke in some high-risk people, researchers report.

A clinical trial called REDUCE-IT tested the approach in more than 8,000 participants who either had cardiovascular disease or were at high risk for it. These people were already on statins to lower their cholesterol, and also had high levels of fats called triglycerides in their blood. Elevated triglycerides can increase one’s risk of heart attack and stroke.

People took either a two-gram pill of a highly purified omega-3 fatty acid — the oil found in fatty fish — twice daily or a placebo, and were followed up to six years. Of the omega-3 group, 17.2 percent had a fatal or nonfatal heart attack or stroke, compared with 22 percent in the placebo group.

Overall, the omega-3 drug, called Vascepa, reduced the risk of heart attack or stroke by 25 percent, researchers announced November 10 at the American Heart Association’s annual scientific sessions in Chicago and in a study published online the same day in the New England Journal of Medicine.

The results are “strikingly positive,” says cardiologist Carl Orringer of the University of Miami Miller School of Medicine who was not involved in the study. For people taking statins and working to combat high levels of triglycerides with healthy diet and exercise, the new drug appears to provide additional benefit, he says.

But it’s possible that benefit may not be as large as it seems. That’s because the placebo that the researchers used was made of mineral oil, which can interfere with the absorption of statins, thereby lowering the statin dose, says cardiologist Steven Nissen at the Cleveland Clinic who was not involved in the study. He wonders if the drug’s positive effect may be partly due to people on the placebo being somewhat worse off. Nissen is leading a clinical trial of a different omega-3 drug that is using corn oil for the placebo.

People take statins primarily to reduce the amount of “bad” cholesterol, called LDL cholesterol, in the blood. Both groups in the trial did have a small increase in LDL cholesterol, with the placebo group’s increase higher than the omega-3 group’s. But, the researchers say, this small difference isn’t likely to account for the 25 percent reduction in risk between the groups.

Orringer agrees. “Even if there was a slight effect of mineral oil, it would be so minimal that I don’t believe there is any way that this could account for the striking difference seen” between those on the omega-3 drug and those not, he says.

Cardiovascular disease is the leading cause of death for both men and women in the United States and accounts for approximately 800,000 deaths each year. High LDL cholesterol is one risk factor for the disease. Excess LDL cholesterol contributes to the buildup of plaques in artery walls, and it’s considered high when it’s at or above 160 milligrams of cholesterol per deciliter of blood.

For some patients taking statins, doctors also monitor triglycerides in the blood, because high levels of the fats can increase the risk of heart attack and stroke. Conditions like obesity and diabetes can lead to high triglyceride levels — those at or above 200 milligrams per deciliter of blood.

Vascepa has already been approved by the U.S. Food and Drug Administration to lower triglycerides in people with very high levels (500 milligrams per deciliter of blood or higher). In the new Phase III clinical trial — conducted to gain approval for the drug’s use in a different group of people — preventive cardiologist Christie Ballantyne and colleagues tested whether the drug could help prevent heart attacks and strokes in heart disease patients and in those with risk factors like diabetes. Study participants’ triglyceride levels were mostly borderline high or high, from 150 to 499 milligrams per deciliter; this is lower than the level for which Vascepa is already approved.

The resulting drop in heart-related health risk is “very encouraging,” says Ballantyne, of Baylor College of Medicine in Houston. Diet and exercise are important for reducing triglycerides, Ballantyne says. But if a patient with heart disease on statins is making lifestyle changes and his or her triglyceride levels are still high, “this treatment could be of benefit,” he says.

Surprisingly, while the drug did lower triglyceride levels, the change probably wasn’t enough to fully explain the reduced heart attack and stroke risk, Ballantyne says, so Vascepa may have additional effects.

Still, the finding that a particular omega-3 drug helped some high-risk patients doesn’t mean that popular, but less potent, supplements containing omega-3 fatty acids have a similar effect, says Orringer. “We don’t want people to go running out to the drugstore to buy a fish oil pill. It won’t help them.”

A number of studies of less potent fish oil pills and other triglyceride-lowering drugs haven’t shown similar cardiovascular benefits. For example, a clinical trial called VITAL found that taking an omega-3 supplement didn’t help lower heart disease risk in a broader and more generally healthy population than that studied in the REDUCE-IT trial. That finding was presented November 10 at the American Heart Association’s meeting.
===============
These tiny, crackly bubbles are a new type of volcanic ash
Bread-crust bubble

\Bred krəst ˈbəb(ə)l\ n.

Tiny, gas-filled beads of volcanic ash with a scaly surface.

Scientists have identified a new type of volcanic ash that erupted from a volcano in central Oregon roughly 7 million years ago. The particles are similar to larger bread-crust bombs, which form as gases trapped inside globs of lava expand, cracking the bombs’ tough exterior. Bread-crust bubbles, each no more than a millimeter wide, have a distinctly crackled surface that can reveal secrets about how volcanoes erupt, researchers reported November 4 at the Geological Society of America annual meeting in Indianapolis.

The researchers had been sifting through other types of volcanic ash in the lab when they spotted the strange ash formations. Viewing the bits of ash through a scanning electron microscope revealed their crusty texture, indicating the gas bubbles expanded rapidly on their way up to Earth’s surface, but did not pop. Analysis of the texture also indicated the bubbles’ depth when they first exploded in the foamy magma. In the case of the Oregon sample, says volcanologist Ben Andrews of the Smithsonian Institution in Washington D.C., the frothy ash formed roughly 500 to 2,000 meters deep — a short distance, geologically — and erupted from the volcano at a rate of about 30 to 80 meters per second.

Loafing around Newfound volcanic ash is like a mini-me version of bread-crust bombs, which get their name from having a cracked surface that can resemble a loaf of bread. Bread-crust bombs form as gases trapped within a glob of hard-shelled lava expand. This one, about 15 centimeters in diameter, erupted from Mount St. Helens in Washington.

“Where magmas are sitting before they erupt and how fast they erupt — that gives us an idea how to forecast future eruptions,” says Andrews, who discovered the spherical ash particles along with volcanologist Steve Quane of Quest University in British Columbia. The team is now collaborating with another group of researchers to study bread-crust bubbles collected from Laguna del Maule volcano in Chile.
===============
Ancient DNA suggests people settled South America in at least 3 waves
DNA from a 9,000-year-old baby tooth from Alaska, the oldest natural mummy in North America and remains of ancient Brazilians is helping researchers trace the steps of ancient people as they settled the Americas. Two new studies give a more detailed and complicated picture of the peopling of the Americas than ever before presented.

People from North America moved into South America in at least three migration waves, researchers report online November 8 in Cell. The first migrants, who reached South America by at least 11,000 years ago, were genetically related to a 12,600-year-old toddler from Montana known as Anzick-1 (SN: 3/22/14, p. 6). The child’s skeleton was found with artifacts from the Clovis people, who researchers used to think were the first people in the Americas, although that idea has fallen out of favor. Scientists also previously thought these were the only ancient migrants to South America.

But DNA analysis of samples from 49 ancient people suggests a second wave of settlers replaced the Clovis group in South America about 9,000 years ago. And a third group related to ancient people from California’s Channel Islands spread over the Central Andes about 4,200 years ago, geneticist Nathan Nakatsuka of Harvard University and colleagues found.

Roads taken Early Americans moved into prehistoric South America in at least three migratory waves, a study proposes. Ancestral people who crossed from Siberia into Alaska first gave rise to groups that settled North America (gray arrows). The first wave of North Americans (blue) were related to Clovis people, represented by a 12,600-year-old toddler from Montana called Anzick-1. They moved into South America at least 11,000 years ago, followed by a second wave (green) whose descendants contributed most of the indigenous ancestry among South Americans today. A third migration wave (yellow) from a group that lived near California’s Channel Island moved into the Central Andes about 4,200 years ago. Dotted areas indicate that people there today still have that genetic ancestry.

People who settled the Americas were also much more genetically diverse than previously thought. At least one group of ancient Brazilians shared DNA with modern indigenous Australians, a different group of researchers reports online November 8 in Science.

Genetically related, but distinct groups of people came into the Americas and spread quickly and unevenly across the continents, says Eske Willerslev, a geneticist at the Natural History Museum of Denmark in Copenhagen and a coauthor of the Science study. “People were spreading like a fire across the landscape and very quickly adapted to the different environments they were encountering.”

Both studies offer details that help fill out an oversimplified narrative of the prehistoric Americas, says Jennifer Raff, an anthropological geneticist at the University of Kansas in Lawrence who was not involved in the work. “We’re learning some interesting, surprising things,” she says.

For instance, Willerslev’s group did detailed DNA analysis of 15 ancient Americans different from those analyzed by Nakatsuka and colleagues. A tooth from Trail Creek in Alaska was from a baby related to a group called the ancient Beringians, who occupied the temporary land mass between Alaska and Siberia called Beringia. Sometimes called the Bering land bridge, the land mass was above water before the glaciers receded at the end of the last ice age. The ancient Beringians stayed on the land bridge and were genetically distinct from the people who later gave rise to Native Americans, Willerslev and colleagues found.

The link between Australia and ancient Amazonians also hints that several genetically distinct groups may have come across Beringia into the Americas.

The Australian signature was first found in modern-day indigenous South Americans by Pontus Skoglund and colleagues (SN: 8/22/15, p. 6). No one was sure why indigenous Australians and South Americans shared DNA since the groups didn’t have any recent contact. One possibility, says Skoglund, a geneticist at the Francis Crick Institute in London and a coauthor of the Cell paper, was that the signature was very old and inherited from long-lost ancestors of both groups.

So Skoglund, Nakatsuka and colleagues tested DNA from a group of ancient Brazilians, but didn’t find the signature. Willerslev’s group, however, examined DNA from 10,400-year-old remains from Lagoa Santa, Brazil, and found the signature, supporting the idea that modern people could have inherited it from much older groups. And Skoglund is thrilled. “It’s amazing to see it confirmed,” he says.

How that genetic signature got to Brazil in the first place is still a mystery, though. Researchers don’t think early Australians paddled across the Pacific Ocean to South America. “None of us really think there was some sort of Pacific migration going on here,” Skoglund says.

That leaves an overland route through Beringia. There’s only one problem: Researchers didn’t find the Australian signature in any of the ancient remains tested from North or Central America. And no modern-day indigenous North or Central Americans tested have the signature either.

Still, Raff thinks it likely that an ancestral group of people from Asia split off into two groups, with one heading to Australia and the other crossing the land bridge into the Americas. The group that entered the Americas didn’t leave living descendants in the north. Or, because not many ancient remains have been studied, it’s possible that scientists have just missed finding evidence of this particular migration.

If Raff is right, that could mean that multiple groups of genetically distinct people made the Berigian crossing, or that one group crossed but was far more genetically diverse than researchers have realized.

The studies may also finally help lay to rest a persistent idea that some ancient remains in the Americas are not related to Native Americans today.

The Lagoa Santans from Brazil and a 10,700-year-old mummy from a place called Spirit Cave in Nevada had been grouped as “Paleoamericans” because they both had narrow skulls with low faces and protruding jaw lines, different from other Native American skull shapes. Some researchers have suggested that Paleoamericans — including the so-called Kennewick Man, whose 8,500-year-old remains were found in the state of Washington (SN: 12/26/15, p. 30) — weren’t Native Americans, but a separate group that didn’t have modern descendants.

But previous studies of Paleoamericans and Willerslev’s analysis of the Spirit Cave mummy’s DNA provide evidence that, despite their skull shapes, the Paleoamericans were not different from other Native Americans of their time. And the ancient people are more closely related to present-day Native Americans than any other group.

Willerslev presented the results about the Spirit Cave mummy to the Fallon Paiute-Shoshone tribe when the data became available. Based on the genetic results, the tribe was able to claim the mummy as an ancestor and rebury the remains.
===============
Hints of Oort clouds around other stars may lurk in the universe’s first light
A thick sphere of icy debris known as the Oort cloud shrouds the solar system. Other star systems may harbor similar icy reservoirs, and those clouds may be visible in the universe’s oldest light, researchers report.

Astronomer Eric Baxter of the University of Pennsylvania and colleagues looked for evidence of such exo-Oort clouds in maps of the cosmic microwave background, the cool cosmic glow of the first light released after the Big Bang, roughly 13.8 billion years ago. No exo-Oort clouds have been spotted yet, but the technique looks promising, the team reports November 2 in the Astronomical Journal. Finding exo-Oort clouds could help shed light on how other solar systems — and perhaps even our own — formed and evolved.

The Oort cloud is thought to be a planetary graveyard stretching between about 1,000 and 100,000 times as far from the sun as Earth. Scientist think that this reservoir of trillions of icy objects formed early in the solar system’s history, when violent movements of the giant planets as they took shape tossed smaller objects outward. Every so often, one of those frozen planetary fossils dives back in toward the sun and is visible as a comet (SN: 11/16/13, p. 14).

But it’s difficult to observe the Oort cloud directly from within it. Despite a lot of circumstantial evidence for the Oort cloud’s existence, no one has ever seen it.

Ironically, exo-Oort clouds might be easier to spot, Baxter and colleagues thought. The objects in an exo-Oort cloud wouldn’t reflect enough starlight to be seen directly, but they would absorb starlight and radiate it back out into space as heat. For the sun’s Oort cloud, that heat signal would be smeared evenly across the entire sky from Earth’s perspective. But an exo-Oort cloud’s warmth would be limited to a tiny region around its star.

Baxter and colleagues calculated that the expected temperature of an exo-Oort cloud should be about –265° Celsius, or 10 kelvins. That’s right in range for experiments that detect the cosmic microwave background, or CMB, which is about 3 kelvins.

The team used data from the CMB-mapping Planck satellite to search for areas across the sky with the right temperature (SN Online: 7/24/18). Then, the researchers compared the results with the Gaia space telescope’s ultraprecise stellar map to see if those regions surrounded stars (SN: 5/26/18, p. 5).

Although the astronomers found some intriguing signals around several bright, nearby stars, it wasn’t enough to declare victory. “That’s pretty interesting, but we can’t definitively say that it’s from an Oort cloud or not,” Baxter says.

Other ongoing CMB experiments with higher resolution, like those with the South Pole Telescope and the Atacama Cosmology Telescope in the Chilean Andes, could confirm if those hints of exo-Oort clouds are real.

“It’s a super clever observational idea,” says astronomer Nicolas Cowan of McGill University in Montreal who was not involved in the new work. “Looking for exo-Oort clouds is looking for a signature of these violent histories in other solar systems.”

Cowan has suggested that the cosmic microwave background could also be used to search for a hypothetical Planet Nine in the sun’s Oort cloud (SN: 7/23/16, p. 7). “The very coolest thing would be if we could get measurements of the exo-Oort clouds and find planets in those systems,” he says.
===============
How a life-threatening allergic reaction can happen so fast
Within minutes of biting into peanut-tainted food, people with a peanut allergy may find their pulse quickening, blood pressure plummeting and throat closing up. They’re experiencing a rapid and sometimes fatal allergic reaction called anaphylaxis.

New research in mice explains how even a small amount of an allergen can quickly trigger such a strong, full-body reaction. The culprit is a type of cell that probes the bloodstream for allergens and then broadcasts the invaders’ presence to anaphylaxis-inducing immune cells, researchers report in the Nov. 9 Science.

When these immune cells, called mast cells, detect an allergen that they’re sensitized to, they flood the body with inflammatory proteins that set off an allergic reaction. But how mast cells, which line the space surrounding blood vessels, are so efficient at detecting allergens floating along in the blood has been a long-standing question, says Stephen Galli, an immunologist at Stanford University who wasn’t involved in the research. In the case of a snakebite, fangs can pierce blood vessels and make it easy for venom, which also activates mast cells, to reach the cells. But with a food allergy, the vessels are usually intact.

In the study, researchers systematically lowered the levels of different types of immune cells in mice to see how the animals’ response to egg allergens changed.

“We found that the mast cells didn't really pick up the allergens,” says study coauthor Soman Abraham, a pathologist at Duke University School of Medicine. “Instead, there was an intermediary cell.”

When the number of intermediary cells was reduced, the mice didn’t seem to experience anaphylactic symptoms, Abraham and his colleagues noticed. Those cells were a type of dendritic cell, which like mast cells are located outside of the bloodstream.

Usually, a dendritic cell detects foreign molecules, takes them in and processes them, and then displays proteins on its surface to advertise the invaders’ presence to other immune cells. Using a technique called two-photon microscopy, which visualizes cells in action in live animals, Abraham and his colleagues showed that this group of dendritic cells has a different, quicker way of alerting mast cells to allergens.

These dendritic cells extend protrusions into blood vessels to periodically sample the blood. Then, the cells bud off tiny packets called microvesicles that carry potential allergens that are found. Those packets get distributed to mast cells and other immune cells, which may then trigger an allergic response.

“When the dendritic cells capture the [allergen] from the blood, they don't internalize it,” Abraham says. Instead, the microvesicles quickly distribute allergen advertisements in all directions — like posting flyers around a neighborhood, rather than displaying a yard sign. By unleashing the microvesicles, the dendritic cells can reach a larger audience than they would by showing a warning protein on their surface.

A 2013 study showed that mast cells can also extend protrusions into the bloodstream, and suggested that these cells might directly detect allergens. But “the fact that one cell can do something doesn't necessarily prove that it's the main responsible cell type,” Galli says. The new research builds a “very thorough” case for dendritic cells being the main messengers between allergens in the blood and mast cells, he says.

These dendritic cells could someday be a target for treating and preventing allergic reactions, Abraham says, though that’s a long way away for humans.
===============
We’re probably undervaluing healthy lakes and rivers
For sale: Pristine lake. Price negotiable.

Most U.S. government attempts to quantify the costs and benefits of protecting the country’s bodies of water are likely undervaluing healthy lakes and rivers, researchers argue in a new study. That’s because some clean water benefits get left out of the analyses, sometimes because these benefits are difficult to pin numbers on. As a result, the apparent value of many environmental regulations is probably discounted.

The study, published online October 8 in the Proceedings of the National Academy of Sciences, surveyed 20 government reports analyzing the economic impacts of U.S. water pollution laws. Most of these laws have been enacted since 2000, when cost-benefit analyses became a requirement. Analysis of a measure for restricting river pollution, for example, might find that it increases costs for factories using that river for wastewater disposal, but boosts tourism revenues by drawing more kayakers and swimmers.

Only two studies out of 20 showed the economic benefits of these laws exceeding the costs. That’s uncommon among analyses of environmental regulations, says study coauthor David Keiser, an environmental economist at Iowa State University in Ames. Usually, the benefits exceed the costs.

So why does water pollution regulation seem, on paper at least, like such a losing proposition?

Keiser has an explanation: Summing up the monetary benefits of environmental policies is really hard. Many of these benefits are intangible and don’t have clear market values. So deciding which benefits to count, and how to count them, can make a big difference in the results.

Many analyses assume water will be filtered for drinking, Keiser says, so they don’t count the human health benefits of clean lakes and rivers (SN: 8/18/18, p. 14). That’s different from air pollution cost-benefit studies, which generally do include the health benefits of cleaner air by factoring in data tracking things like doctor’s visits or drug prescriptions. That could explain why Clean Air Act rules tend to get more favorable reviews, Keiser says — human health accounts for about 95 percent of the measured benefits of air quality regulations.

“You can avoid a lake with heavy, thick, toxic algal blooms,” Keiser says. “If you walk outside and have very polluted air, it's harder to avoid.”

But even if people can avoid an algae-choked lake, they still pay a price for that pollution, says environmental scientist Thomas Bridgeman, director of the Lake Erie Center at the University of Toledo in Ohio.

Communities that pull drinking water from a lake filled with toxic blooms of algae or cyanobacteria spend more to make the water safe to drink. Bridgeman’s seen it firsthand: In 2014, Lake Erie’s cyanobacteria blooms from phosphorus runoff shut down Toledo’s water supply for two days and forced the city to spend $500 million on water treatment upgrades.

Most of the studies surveyed by Keiser and his team were missing other kinds of benefits, too. The reports usually left out the value of eliminating certain toxic and nonconventional pollutants — molecules such as bisphenol A, or BPA, and perfluorooctanoic acid, or PFOA (SN: 10/3/15, p. 12). In high quantities, these compounds, which are used to make some plastics and nonstick coatings, can cause harm to humans and wildlife. Many studies also didn’t include discussion of how the quality of surface waters can affect groundwater, which is a major source of drinking water for many people.

A lack of data on water quality may also limit studies, Keiser’s team suggests. While there’s a national database tracking daily local air pollution levels, the data from various water quality monitoring programs aren’t centralized. That makes gathering and evaluating trends in water quality harder.

Plus, there are the intangibles — the value of aquatic species that are essential to the food chain, for example.

“Some things are just inherently difficult to put a dollar [value] on,” says Robin Craig, an environmental law professor at the University of Utah in Salt Lake City. “What is it worth to have a healthy native ecosystem?... That's where it can get very subjective very fast.”

That subjectivity can allow agencies to analyze policies in ways that suit their own political agendas, says Matthew Kotchen, an environmental economist at Yale University. An example: the wildly different assessments by the Obama and Trump administrations of the value gained from the 2015 Clean Water Rule, also known as the Waters of the United States rule.

The rule, passed under President Barack Obama, clarified the definition of waters protected under the 1972 Clean Water Act to include tributaries and wetlands connected to larger bodies of water. The Environmental Protection Agency estimated in 2015 that the rule would result in yearly economic benefits ranging from $300 million to $600 million, edging out the predicted annual costs of $200 million to $500 million. But in 2017, Trump’s EPA reanalyzed the rule and proposed rolling it back, saying that the agency had now calculated just $30 million to $70 million in annual benefits.

The difference in the conclusions came down to the consideration of wetlands: The 2015 analysis found that protecting wetlands, such as marshes and bogs that purify water, tallied up to $500 million in annual benefits. The Trump administration’s EPA, however, left wetlands out of the calculation entirely, says Kotchen, who analyzed the policy swing in Science in 2017.

Currently, the rule has gone into effect in 26 states, but is still tied up in legal challenges.

It’s an example of how methodology — and what counts as a benefit — can have a huge impact on the apparent value of environmental policies and laws.

The squishiness in analyzing environmental benefits underlies many of the Trump administration’s proposed rollbacks of Obama-era environmental legislation, not just ones about water pollution, Kotchen says. There are guidelines for how such cost-benefit analyses should be carried out, he says, but there’s still room for researchers or government agencies to choose what to include or exclude.

In June, the EPA, then under the leadership of Scott Pruitt, proposed revising the way the agency does cost-benefit analyses to no longer include so-called indirect benefits. For example, in evaluating policies to reduce carbon dioxide emissions, the agency would ignore the fact that those measures also reduce other harmful air pollutants. The move would, overall, make environmental policies look less beneficial.

These sharp contrasts in how presidential administrations approach environmental impact studies are not unprecedented, says Craig, the environmental law professor. “Pretty much every time we change presidents, the priorities for how to weigh those different elements change.”
===============
Science News
New oddball exoplanets flout the known rules of planetary cooking. They don’t follow the traditional recipes observed in our solar system. So astronomers are investigating what the exoplanet cookbook really looks like.Read more: https://www.sciencenews.org... ...CreditsStoryLisa GrossmanScript, production & narrationHelen ThompsonVideoHelen ThompsonKate TravisBaking & blowtorch servicesKate TravisProduction assistantsKyle PlantzStella TravisSound effectsSnapper 4298/freesound(CC BY 3.0)Mr. Auralization/freesound(CC BY 3.0)Music“An Opus in Ab” by Blue Dot Sessions(CC BY 4.0) http://freemusicarchive.org... ... https://creativecommons.org... ... Show less
===============
2017 Top 10
In science, progress rarely comes in one big shebang. Well, it has now, two years running. The first-ever direct detection of gravitational waves, our top story in 2016, launched a long-dreamed-of kind of astronomy capable of “unlocking otherwise unknowable secrets of the cosmos,” as physics writer Emily Conover puts it. 2017’s key event: a never-before-seen neutron star collision that immediately validated some theories in physics and killed others. And so a new way to probe cosmic mysteries wins our top spot again this year.

Another turning point is coming, and maybe soon, via CRISPR/Cas9, a biotechnology that holds the promise of curing genetic diseases (and the peril of making permanent, heritable tweaks). Nearly five years after the gene-editing tool debuted, researchers for the first time have used it to alter genes in viable human embryos. That’s a big advance, and worthy of the No. 2 spot.

Don’t be fooled, though. Even eureka moments like these are the fruits of the slow build of progress: Fossil by fossil, paleoanthropologists draw a picture of Homo sapiens’ earliest days. Brain by brain, the extent of damage caused by chronic traumatic encephalopathy — a disease linked to hard head knocks — becomes clear.

And crack by crack, one of the biggest icebergs ever recorded calves. That story, No. 3 on our list, is not exactly progress, but it’s surely an opportunity to make scientific headway. Teams racing to Antarctica’s Larsen C ice shelf will have an unprecedented chance to collect real-time data on how the remaining ice reacts and to reveal secrets of a long-hidden ecosystem. Building on those advances, as well as others described in our Top 10 picks, will fuel “aha!” moments — both revolutionary and incremental — well into the future. — Macon Morehouse, News Director

The Top 10 Stories of 2017
===============
The economics of climate change and tech innovation win U.S. pair a Nobel
Two U.S. economists, William Nordhaus and Paul Romer, have received the 2018 Nobel Memorial Prize in Economic Sciences for their efforts to untangle the economics of climate change and technological innovations.

Nordhaus and Romer “significantly broadened the scope of economic analysis by constructing models that explain how the market economy interacts with nature and knowledge,” the Royal Swedish Academy of Sciences said in a statement announcing the awards on October 8.

Nordhaus, of Yale University, developed two computer simulations that weigh the costs and benefits of taking various steps to slow global warming. He has argued for taxes on the carbon content of fuels as an effective way to get businesses to reduce greenhouse gas emissions. The Environmental Protection Agency has used Nordhaus’ work, among others, to estimate the economic impacts of climate change.

The announcement of Nordhaus’ award came just hours after a United Nations panel on climate change released a report predicting grim future effects of climate change and calling for more vigorous action by world governments to limit warming to 1.5 degrees Celsius over preindustrial times (SN Online: 10/7/18). The new report cites Nordhaus’ work.

Romer, of New York University, expanded economic theory by arguing that government policies, such as funding for research and development, can stimulate technological advances. The presence or absence of such policies helps to explain national differences in wealth and economic growth, in Romer’s view.

Romer’s ideas about policy making and technological innovation, first published in 1990, inspired a school of research that examines how business regulations and policies lead to new ideas and economic growth.

The pair will split the 9-million-Swedish-kronor (about $1 million) award.
===============
Speeding up evolution to create useful proteins wins the chemistry Nobel
Techniques that put natural evolution on fast-forward to build new proteins in the lab have earned three scientists this year’s Nobel Prize in chemistry.

Frances Arnold of Caltech won for her method of creating customized enzymes for biofuels, environmentally friendly detergents and other products. She becomes the fifth woman to win the Nobel Prize in chemistry since it was first awarded in 1901. Gregory Winter of the University of Cambridge and George Smith of the University of Missouri in Columbia were recognized for their development and use of a technique called phage display. This molecule-manufacturing process can generate biomolecules for new drugs.

The trio will share the 9-million-Swedish-kronor prize (about $1 million), with Arnold getting half and Winter and Smith splitting the other half.

“Wow, well-deserved!” says Paul Dalby, a biochemical engineer at University College London. “Protein engineering as a field is absolutely founded upon their work.”

In the 1990s, Arnold wanted to make an enzyme that would break down a milk protein called casein in an organic liquid, rather than in water. Instead of trying to manually sculpt the chemical building blocks of that enzyme, subtilisin E, to give it the right properties, she opted for a more hands-off approach.

Arnold’s insight “was to recognize that the most amazing molecules in the world weren’t created by chemists, but rather by the biological world,” says Jesse Bloom, a microbiologist at the Fred Hutchinson Cancer Research Center in Seattle. “Biology didn’t make these chemicals using the methods we might learn in an organic chemistry class — rather, it worked by evolution.”

Arnold first made many copies of the original enzyme, each with a different set of genetic mutations. She then inserted the genes for those enzymes into bacteria. These bacteria served as living factories, churning out many copies of each enzyme variant. Arnold picked out the version that did the best job breaking down casein in an organic solvent and repeated the mutation process, starting with that enzyme.

Catalytic action Frances Arnold’s lab put the enzyme subtilisin E through directed evolution to make it more active in an organic liquid. The changes to create the first variant are shown here in yellow; the molecule was then further evolved to include the additional mutations shown in blue. Molecules and ions bound to the enzyme are pictured in grey.

After several rounds of mutating genes and choosing only the best enzyme to advance to the next round, Arnold was left with an extremely efficient custom-made enzyme. This enzyme-tailoring technique, known as directed evolution, allows scientists to fine-tune proteins the same way nature does, but thousands of times faster. Researchers have used directed evolution to create enzymes that jump-start chemical reactions for creating new drugs and eco-friendly biofuels. Arnold’s lab has also used directed evolution to create enzymes that help forge chemical connections not found in nature, such as bonds between carbon and silicon atoms (SN: 12/24/16, p. 11).

The other half of the Nobel Prize honors work on a molecule-making procedure known as phage display. The primary tool for this process is a type of virus known as a bacteriophage — a simple microbe made of genetic material enclosed in a protein package.

In the 1980s, Smith got the idea to insert the genes responsible for producing various specific proteins into bacteriophages’ genetic code, creating phages that bore these proteins on their surface. Smith’s original motivation was identifying which genes created which proteins. By fishing around a bacteriophage soup with a molecule known to bind to a certain protein, Smith surmised, a researcher could pick out only the phage armed with that particular protein and discover which gene allowed the phage to create it.

In 1985, Smith manipulated bacteriophage DNA to create a phage carrying a piece of protein, called a peptide, that wouldn’t naturally occur on its surface. He then used a peptide-binding molecule to pick this bacteriophage out of the crowd. Smith’s method of meddling with bacteriophage DNA is the foundation of phage display. Since his seminal work producing a peptide-bearing phage, other researchers have adopted his method to create phages that harbor other biomolecules, like antibodies.

Our bodies naturally produce hundreds of thousands of different antibodies that are designed to latch on to viruses and bacteria — essentially putting a hit on these invaders for the immune system to destroy. But researchers had long wanted to create in the lab antibodies that work as medications to curb various diseases. In 1990, Winter used the phage display method to create a phage armed with part of an antibody that binds to the molecule phOx. Winter then used phOx to collect the phage with the antibody from a collection of 4 million other phages.

To ensure that he was using phages to farm the best antibodies possible, Winter then adopted a similar method of directed evolution as Arnold. Winter first created a pool of bacteriophages genetically programmed to produce billions of different antibodies. From that group, he could use a target molecule, like phOx, to collect only the antibody-carrying phages that bound to it the best. From those phages, Winter created a new generation of antibody-toting viruses, and once again used the target molecule to pick out only the best of the bunch.

In the 1990s, Winter and his colleagues used this survival-of-the-fittest-phage technique to produce the antibody adalimumab, creating a drug that neutralizes a chemical that incites inflammation in patients with autoimmune diseases. The drug, known as Humira, was cleared to treat rheumatoid arthritis in 2002 and is now also used to treat psoriasis and inflammatory bowel disease (SN: 8/1/09, p. 8).

Packing peptides George Smith developed a method called phage display that farms for biomolecules. First, the researcher inserts a gene for making a particular molecule (here, a piece of protein called a peptide) into a phage’s genetic code (1). That creates a phage programmed to produce that peptide on its surface (2). Using an antibody that binds to that peptide, the researcher can harvest the peptide-carrying phage from a soup of many phages (3).

Phage display “is an extremely versatile technology,” says Jonathan Lai, a biochemist at the Albert Einstein College of Medicine in New York City, whose lab uses phage display to develop vaccines. Other researchers have used phage display to produce antibodies that help treat the autoimmune disease lupus and fight cancer.

Directed evolution and phage display provide "a great demonstration of how studying fundamental biological questions like the natural process of evolution can lead to great breakthroughs in technology and medicine,” says Jon Lorsch, director of the National Institute of General Medical Sciences in Bethesda. Md.

These techniques could be used to create molecules that we haven’t yet discovered or have never even considered, says Peter Dorhout, president of the American Chemical Society. “It’s an open frontier.”
===============
The SN 10: These scientists defy limits to tackle big problems
Scientific disciplines, as we know them, are a fairly recent invention. As late as the 18th century, both amateur and professional scientists let their intellect range unfettered. The great Renaissance painter Leonardo da Vinci explored architecture, engineering, geology, botany and more. He is credited with inventing the helicopter, a diving suit and painting the Mona Lisa.

Only later did scientific disciplines emerge as a powerful way to speed learning as scientific knowledge accumulated rapidly. Today's scientists, including this year’s SN 10 researchers, are stepping over these boundaries to borrow tools and inspiration from other fields to solve knotty questions facing science and society.

Members of the SN 10 class of 2018 are skilled at moving between scientific worlds. One uses physics to learn how cell movement in the lungs encourages asthma. Another sees architecture in how volcanoes build planets. Several venture into other fields to help answer difficult questions in their own fields: Maybe the proteins of biology can teach a materials scientist how to make self-repairing batteries.

This is the fourth year that Science News is spotlighting a group of early- and mid-career scientists who are breaking ground. It’s a confident, tough group. Try to set limits or box these people in and they bristle. Some had childhood experiences that opened their minds to the possibilities of scientific research. Others dug in their heels to do something that an adult said would be too difficult.

From a pool of standout researchers nominated by Nobel laureates and recently elected members of the National Academy of Sciences, Science News staff chose 10 to introduce on these pages. The scientists, all under 40, come from different backgrounds and fields of study. But their colleagues and mentors describe many of them in the same way: fearless, with a thirst for knowledge and a drive to grasp the unknown, boundaries be damned. — Cori Vanchieri

Meet the SN 10
===============
Shahzeen Attari explores the psychology of saving the planet
Shahzeen Attari, 37

Environmental decision making

Indiana University Bloomington

When Shahzeen Attari was growing up in Dubai, her father ran a machine shop. Her mother, a gregarious people person, worked at a bank.

“My curiosity about how things work came from my father,” Attari says. “I learned to love getting to know people from my mother.”

That yin-yang background may help explain why Attari, now at Indiana University Bloomington, found a way to merge the practical and the personal in her scientific pursuits, by blending civil and environmental engineering with public policy and psychology.

At age 37, she has become a leader in the study of how people think about conservation, energy use and climate change. At its heart, Attari’s research explores people’s difficulties in grasping complex physical systems. She has studied the ways in which people underestimate their own water and energy use.

“We live in a world that must dramatically reduce its use of fossil fuels and water, but efforts to encourage people to change their behavior have proven notoriously difficult,” says communications researcher Edward Maibach of George Mason University in Fairfax, Va. “Shahzeen’s research has taught us much about why that is, and what can be done to improve our efforts,” says Maibach, who studies public understanding of climate change.

Her graduate school adviser at Carnegie Mellon University, environmental engineer and air quality researcher Cliff Davidson, quickly noticed her interdisciplinary bent when she arrived with an undergraduate degree in engineering physics. In graduate school, Attari decided on a joint degree in engineering and public policy. “Students who pursue degrees in engineering and public policy are almost always holistic thinkers as opposed to narrow, focused thinkers that delve only into one topic,” says Davidson, now at Syracuse University in New York.

Attari’s 2009 dissertation on how people might decrease energy consumption in the face of global climate change marked her as a rare physical scientist interested in behavioral and social perspectives. Soon, psychologists David Krantz and Elke Weber recruited her to the Center for Research on Environmental Decisions, part of Columbia University’s Earth Institute.

There, Attari led a study that suggested people know surprisingly little about their daily energy use and how best to save energy. Participants in a national online survey were asked to recommend ways to conserve energy. Volunteers cited less effective behaviors, such as turning off lights, over more effective approaches, such as installing high-efficiency light bulbs.

Those findings, published in 2010 in the Proceedings of the National Academy of Sciences, highlighted a need for product labels and energy conservation campaigns that do a much better job of informing people how to most effectively reduce energy consumption, Attari says.

Public understanding of the water system also needs an upgrade, Attari found. In the wake of a furor over high lead levels in drinking water in Flint, Mich. (SN: 3/19,16, p. 8), she and colleagues asked 457 college students to draw diagrams of how water reaches home taps.

H 2 UH-OH Troubling knowledge gaps appeared when a team led by Shahzeen Attari asked volunteers to diagram how water reaches the tap in an average U.S. home. A drawing including a sewage system and treatment plant indicated partial understanding. Another drawing (below) depicts natural water sources connected to a home via “magic” rather than through a water treatment plant.

Nearly one-third of participants failed to draw a drinking water treatment plant that filters and disinfects water from natural sources before delivering it to homes. And 1 in 5 incorrectly drew wastewater returning directly to the natural environment from home pipes, rather than first going through a sewage treatment plant, Attari’s team reported in 2017 in Judgment and Decision Making. Attari hopes to learn whether educating people about how their local water systems work will change their attitudes on policy. It’s an open question whether better-informed citizens would want and demand funding to remedy crumbling sewage pipes and other infrastructure concerns.

On climate change, Attari’s research suggests that scientists can spread a more effective conservation message by shrinking their own carbon footprints. In online experiments she conducted with Krantz and Weber, nearly 3,000 volunteers read different versions of stories about hearing a leading climate researcher advocate for cuts in energy use.

A researcher described as an energy miser at home received much higher credibility ratings than one described as an energy guzzler. Participants reported stronger intentions to reduce their own energy use after reading about energy-frugal climate scientists. The finding held up even among those who regarded climate change as unimportant.

In the future, Attari wants to look at how personal experiences and feelings influence opinions about climate science.

For an interdisciplinary environmental engineer like Attari, people are complex systems, too.
===============
Genealogy databases could reveal the identity of most Americans
Protecting the anonymity of publicly available genetic data, including DNA donated to research projects, may be impossible.

About 60 percent of people of European descent who search genetic genealogy databases will find a match with a relative who is a third cousin or closer, a new study finds. The result suggests that with a database of about 3 million people, police or anyone else with access to DNA data can figure out the identity of virtually any American of European descent, Yaniv Erlich and colleagues report online October 11 in Science.

Erlich, the chief science officer of the consumer genetic testing company MyHeritage, and colleagues examined his company’s database and that of the public genealogy site GEDMatch, each containing data from about 1.2 million people. Using DNA matches to relatives, along with family tree information and some basic demographic data, scientists estimate that they could narrow the identity of an anonymous DNA owner to just one or two people.

Recent cases identifying suspects in violent crimes through DNA searches of GEDMatch, such as the Golden State Killer case (SN Online: 4/29/18), have raised privacy concerns (SN Online: 6/7/18). And the same process used to find rape and murder suspects can also identify people who have donated anonymous DNA for genetic and medical research studies, the scientists say.

Genetic data used in research is stripped of information like names, ages and addresses, and can’t be used to identify individuals, government officials have said. But “that’s clearly untrue,” as Erlich and colleagues have demonstrated, says Rori Rohlfs, a statistical geneticist at San Francisco State University, who was not involved in the study.

Using genetic genealogy techniques that mirror searches for the Golden State Killer and suspects in at least 15 other criminal cases, Erlich’s team identified a woman who participated anonymously in the 1000 Genomes project. That project cataloged genetic variants in about 2,500 people from around the world.

Erlich’s team pulled the woman’s anonymous data from the publicly available 1000 Genomes database. The researchers then created a DNA profile similar to the ones generated by consumer genetic testing companies such as 23andMe and AncestryDNA (SN: 6/23/18, p.14) and uploaded that profile to GEDMatch.

A search turned up matches with two distant cousins, one from North Dakota and one from Wyoming. The cousins also shared DNA indicating that they had a common set of ancestors four to six generations ago. Building on some family tree information already collected by those cousins, researchers identified the ancestral couple and filled in hundreds of their descendants, looking for a woman who matched the age and other publicly available demographic data of the 1000 Genomes participant.

It took a day to find the right person.

That example suggests scientists that need to reconsider whether they can guarantee research participants anonymity if genetic data are publicly shared, Rohlfs says.

In reality, though, identifying a person from a DNA match with a distant relative is much harder than it appears, and requires a lot of expertise and gumshoe work, Ellen Greytak says. She is the director of bioinformatics at Parabon NanoLabs, a company in Reston, Va., that has helped close at least a dozen criminal cases since May using genetic genealogy searches. “The gulf between a match and identification is absolutely massive,” she says.

The company has also found that people of European descent often have DNA matches to relatives in GEDMatch. But tracking down a single suspect from those matches is often confounded by intermarriages, adoptions, aliases, cases of misidentified or unknown parentage and other factors, says CeCe Moore, a genealogist who spearheads Parabon’s genetic genealogy service.

“The study demonstrates the power of genetic genealogy in a theoretical way,” Moore says, “but doesn’t fully capture the challenges of the work in practice.” For instance, Erlich and colleagues already had some family tree information from the 1000 Genome woman’s relatives, “so they had a significant head start.”

Erlich’s example might be an oversimplification, Rohlfs says. The researchers made rough estimates and assumptions that are not perfect, but the conclusion is solid, she says. “Their work is approximate, but totally reasonable.” And that conclusion that almost anyone can be identified from DNA should spark public discussion about how DNA data should be used for law enforcement and research, she says.
===============
What I actually learned about my family after trying 5 DNA ancestry tests
Commercials abound for DNA testing services that will help you learn where your ancestors came from or connect you with relatives. I’ve been interested in my family history for a long time. I knew basically where our roots were: the British Isles, Germany and Hungary. But the ads tempted me to dive deeper.

Previous experience taught me that different genetic testing companies can yield different results (SN: 5/26/18, p. 28). And I knew that a company can match people only to relatives in its customer base, so if I wanted to find as many relatives as possible, I would need to use multiple companies. I sent my DNA to Living DNA, Family Tree DNA, 23andMe and AncestryDNA. I also bought the National Geographic Geno 2.0 app through the company Helix. Helix read, or sequenced, my DNA, then sent the data to National Geographic to analyze.

These companies analyze hundreds of thousands of natural DNA spelling variations called single nucleotide polymorphisms, or SNPs. To estimate ethnic makeup, a company compares your overall SNP pattern with those of people from around the world. SNP matches also help companies see who in their database you’re related to.

Some of the companies also analyze a person’s Y chromosome or mitochondrial DNA. Y chromosome DNA traces a man’s paternal line. In contrast, mitochondrial DNA traces maternal heritage, since people inherit mitochondria, which generate energy for cells, only from their mothers. Neither type of DNA changes that much over time, so those tests usually can’t tell you much about recent ancestors.

Once I sent in DNA samples, my Web-based results arrived in just a few weeks. But my user experience, and results, were quite different for each company.

National Geographic Geno 2.0

At $199.95, National Geographic’s test is the most expensive, yet the least useful. The results are generic, and the ethnicity categories are overly broad. My results say that 45 percent of my heritage came from people living in southwestern Europe 500 to 10,000 years ago. That doesn’t tell me much and doesn’t reflect what I know of my family history.

Pros Specialized for looking into the deep past Cons Provides no ancestry information within the last 500 years

There’s no relative matching, though Geno 2.0 shows which historical “geniuses” may have shared your mitochondrial or Y chromosome DNA. I don’t know how National Geographic knows about the mitochondria of Petrarch, Copernicus or Abraham Lincoln. So I’m skeptical that I am actually related to those famous figures, even from the distance of 65,000 years, the last time we supposedly had an ancestor in common. The service also calculated the percentage of Neandertal ancestry that I carry. I take geeky pride that 1.5 percent of my DNA comes from Neandertals, topping the 1.3 percent average for Geno 2.0 customers.

Overall, Geno 2.0 has a nice presentation, but I learned more about my family history elsewhere. Since I bought the Geno 2.0 kit as an app through Helix, I don’t know if the kit purchased directly from National Geographic, which is processed by Family Tree DNA, would yield different results.

Living DNA

Another expensive test ($159) came from Living DNA. When I saw the company’s ad claiming to pinpoint exactly where in the British Isles a person’s genetic roots stem from, I decided to give it a go. The company highlights ethnicity on a world map, then lets you zoom in from the continent level. I found that 22.5 percent of my heritage came from Lincolnshire in east-central England. I haven’t yet traced any ancestors to Lincolnshire, but I did find through much genealogical sleuthing that one of my sixth-great-grandfathers came from Aberdeen, Scotland. Living DNA says that 3.1 percent of my DNA is from Aberdeenshire. Written narratives on the website provide a history of each reported region.

Pros Offers detailed ethnicity estimates for people of British or Irish descent Cons Can’t link relatives to a family tree

Using mitochondrial DNA and, if applicable, Y chromosome DNA, the company can trace your maternal and paternal lines back to human origins in Africa and show where and when your particular line probably branched off the original. My “motherline” probably arose in the Near East 19,000 to 26,000 years ago, Living DNA claims, and my ancestors were some of the first people to enter Europe. In February, the company announced that it would soon launch a relative-matching service for its customers.

I’m not sure the service would be worth the price tag for people whose ancestry doesn’t contain a strong British or Irish tilt, though Living DNA says it is working to improve ethnicity estimates in Germany and elsewhere.

Family Tree DNA

The most no-frills of the bunch is Family Tree DNA. For $79, “autosomal” testing looks for genetic variants on all of the chromosomes except the X and Y sex chromosomes. Y chromosome and mitochondrial DNA analysis costs extra.

Pros Incorporates DNA results into family trees Cons Doesn’t explain results well

Website is hard to navigate

Family Tree DNA allows a user to build a family tree, incorporating personal DNA tests and matches from the site’s relative-matching section. I found more than 2,400 potential relatives. A chromosome viewer lets me see exactly which bit of DNA I have in common with any particular relative, or with up to five relatives at a time. That feature also allows users to trace how they inherited DNA from a shared ancestor. But I found this tool difficult to use.

The website offers little explanation of results. For instance, I was excited to see that my DNA was compared with that of ancient Europeans, including Ötzi the Iceman, who lived 5,300 years ago (SN: 9/17/16, p. 9). Family Tree DNA is the only company I tried that incorporates ancient DNA into its results and that feature was what convinced me try this company. I did get a breakdown of how different groups — Stone Age hunter-gatherers, early farmers and “Metal Age Invaders” from the Eurasian steppes — contributed to my DNA. But when I saw Ötzi’s dot on my ancestry map, it wasn’t clear if that meant we share DNA or if the map was merely showing where he lived.

23andMe

23andMe ($99) offers one of the more complete packages of information. Most companies show a map of ethnic heritage. 23andMe does, too, but also presents an interactive diagram of all of a person’s chromosomes, indicating which portions carry a particular ethnic ancestry. Because my parents also did 23andMe, I learned that my dad handed me a tiny bit of chromosome 15 that carries western Asian and northern African heritage. My mom gave me the 0.3 percent of my DNA that comes from the Balkans, in a single chunk on chromosome 7, which makes sense since her grandparents came from Hungary. Playing with the chromosomes is fun. But I question the accuracy of these results (see my related article for more on why ancestry tests may miss the mark).

Pros Explains results well Cons Can’t link relatives to a family tree

23andMe presents Neandertal heritage in terms of the number of genetic variants you carry. A family-and-friends scoreboard shows where you stack up. (I top my leaderboard with 296 Neandertal variants, more than what 80 percent of 23andMe customers have.) The report also explains what some of those Neandertal variants do, including ones linked to back hair, straight hair, height and whether you’re likely to sneeze after eating dark chocolate. The company doesn’t test for all possible Neandertal variants, including ones that have been linked to health (SN Online: 10/10/17; SN: 3/5/16, p. 18).

Like Geno 2.0, 23andMe uses mitochondrial and Y chromosome DNA to trace the migration patterns of a person’s ancestors, from Africa to the present day.

Relative matching is both interesting and frustrating. I could see the people I match, how we might be related and compare our chromosomes. But 23andMe doesn’t provide a way to build family trees to further explore these relationships.

AncestryDNA

AncestryDNA ($99) doesn’t give the variety of information other companies do. But it has useful genealogical tools, provided you link your results to a family tree that you can build with help from historical records via a paid subscription to Ancestry.com.

Pros Allows DNA results to be combined with traditional genealogical records Cons Provides no information about ancient ancestry

One interesting feature of my heritage report was that it went beyond spots on the map in Europe to also show a region of the United States called “Northeastern States Settlers.” A match to that category tells me that my ancestors who came from Europe probably initially settled in New England or around the Great Lakes. They did. One branch of my family tree set roots in Massachusetts in the 1640s. Using birth, death and immigrant records from Ancestry.com, I could build a timeline to show when and from where individual ancestors immigrated to the United States.

AncestryDNA also matches you with relatives, but you can only see how you’re related to those people if they have also chosen to make family trees.

A feature unique to AncestryDNA is called DNA circles. It shows connections between individuals and family groups who share DNA with you. These circles also contain descendants of your ancestors who you don’t directly share DNA with. Therefore, this feature allows you to extend relative matches beyond what traditional DNA matching can do.

For instance, I am in a family group with my uncle and a cousin. We all share DNA with 24 other descendants of Samuel Pickerill, a drummer during the Revolutionary War. Pickerill has 42 other descendants with whom my family group doesn’t share DNA. Those 42 Pickerill descendants happened to inherit different bits of DNA from Pickerill than my uncle, his cousin and I did. That sometimes happens because of the random nature of the rules of biology and genetics (for more on those rules, check out this video).

Genealogy junkie

Although I’ve always been interested in family history, DNA testing has gotten me hooked on genealogy research.

23andMe and AncestryDNA were the most fun to use. 23andMe can tell me whether a relative is on my mother’s or father’s side of the family. But then I have to go back to AncestryDNA and comb through my family tree to learn how we’re really connected. DNA can kick-start a genealogy hunt, but combing through marriage certificates, military rolls, census records, immigration documents, old photographs and other records — which Ancestry.com can provide — is what really tells me who my ancestors were.
===============
A neutron star crash may have spawned a black hole
The first observed smashup of two stellar remnants known as neutron stars probably forged the least massive black hole yet discovered, researchers report in the June 1 Astrophysical Journal Letters.

This cosmic collision, observed in August 2017, took the astronomical community by storm and offered insights into the origins of precious metals and the mysterious dark energy that fuels the expansion of the universe (SN: 11/11/17, p. 6). Ever since, astronomers have wondered what became of the two neutron stars once they’d merged. This enigmatic amalgamation of dead stars — weighing about 2.7 times the mass of the sun — was suspected to be either a neutron star heftier than any yet discovered or the most lightweight black hole. The previous record-holder for lightest black hole was about four solar masses, says study coauthor David Pooley, a physicist at Trinity University in San Antonio.

Pooley and colleagues analyzed data collected by NASA’s Chandra X-ray space telescope several months after gravitational wave detectors first identified the neutron star collision. If the pair of neutron stars united to form an even more massive dead star, then researchers would expect that mega-neutron star to be surrounded by a bright shell of high-energy particles — similar to the Crab Nebula, but much brighter (SN Online: 6/13/08). X-rays coming from the site of the crash were far too faint to match this explanation, leading the team to conclude that the collision birthed a black hole instead.

Future observations of the collision remnant’s X-ray brightness could help confirm or deny the black hole’s existence, says study coauthor Pawan Kumar, an astrophysicist at the University of Texas at Austin.
===============
This year’s neutron star collision unlocks cosmic mysteries
Thousands of astronomers and physicists. Hundreds of hours of telescope observations. Dozens of scientific papers. Two dead stars uniting into one.

In 2017, scientists went all in on a never-before-seen astronomical event of astounding proportions: a head-on collision between two neutron stars, the ultradense remnants of exploded stars.

The smashup sent shivers of gravitational waves through Earth, and the light show that followed sent shivers of excitement down astronomers’ spines. A real-life scientific drama quickly unfolded as night after night, astronomers around the world raced the sunrise, capturing every possible bit of data before day broke at their observatories.

Scientists had long fantasized about using light together with gravitational waves to forge a new kind of astronomy capable of unlocking otherwise unknowable secrets of the cosmos. “Now we’re finally here,” says Vicky Kalogera, an astrophysicist at Northwestern University in Evanston, Ill.

Almost overnight, the discovery vanquished some theories and vindicated others. It has had implications for the origins of the universe’s heaviest elements, the mysterious dark energy that makes up about 70 percent of the cosmos and the source of brilliant-but-brief flashes of high-energy light known as short gamma-ray bursts. In the last two decades of astronomy, the detection “is more significant than any other event,” says theoretical astrophysicist Avi Loeb of Harvard University.

So frenzied was the excitement over the find that researchers had trouble keeping their discovery under wraps as they raced to analyze data. Social media buzzed with rumors. When a Twitter account that regularly announces targets of the Hubble Space Telescope reported one named “BNS-Merger” (presumably for “binary neutron star”), corners of the internet exploded with speculation (SN Online: 8/25/17). Astronomy enthusiasts mined the publicly available logs of astronomical observatories for hints of what telescopes were zeroing in on.

The two neutron stars converged in the galaxy NGC 4993, 130 million light-years from Earth, emitting gravitational waves in the process (SN: 11/11/17, p. 6). Those waves, predicted by Einstein’s general theory of relativity, stretched and compressed spacetime, traveling outward like ripples on a pond. As the waves began their outbound journey 130 million years ago, Earth was in the midst of the Cretaceous Period: Dinosaurs were large and in charge. Life capable of building the complex detectors necessary for spotting the gravitational rumbles wouldn’t arise for many millions of years.

But these spacetime ripples arrived at Earth at an opportune moment, when detectors were finally prepared to spot them. Decades in the making, these highly-sensitive devices can register spacetime shifts a fraction of the size of a proton. On August 17, 2017, the instruments — the two detectors of the Advanced Laser Interferometer Gravitational-Wave Observatory, LIGO, in Livingston, La., and Hanford, Wash., and Advanced Virgo near Pisa, Italy — spotted the waves and joined forces to triangulate their source. Gamma-ray space telescopes recorded a high-energy light burst less than two seconds later.

A worldwide network of telescopes sprang into action to look for light in the region of the sky where LIGO and Virgo predicted the waves came from. Less than 11 hours after the gravitational waves appeared, astronomers spotted a new point of visible light in the sky. That finding kicked off an astronomical free-for-all: “Basically every telescope pointed at this thing,” says LIGO member Daniel Holz of the University of Chicago. As a result, he says, “there’s just this wealth of information.”

Telescopes captured visible, infrared and ultraviolet light, followed by X-rays and radio waves days later. Each observation was precious: The light faded rapidly and changed colors over time, says Josh Simon, an astronomer with Carnegie Observatories in Pasadena, Calif. “This is a fairly rare occurrence in astronomy,” he says. “Most things we look at don’t change over time at all.”

Researchers announced October 16 that Advanced LIGO (the Laser Interferometer Gravitational-Wave Observatory) and its sister experiment, Advanced Virgo, had detected gravitational waves from colliding neutron stars — a cosmic crash also observed by more than 70 observatories around the world.

Observations revealed a previously theorized process dubbed a “kilonova” — thought to be a source of heavy elements like gold, silver, platinum and uranium — which could form as neutron-rich material is ejected from the stars. Light from the collision directly confirmed this hunch for the first time. “My wedding band emerged from a neutron star merger,” Loeb marvels.

The intensity of observations outstripped all previous astronomical finds, said astronomer Edo Berger of Harvard University at an October 16 news conference in Washington, D.C. “I don’t think there has been anything like this before.” Roughly 15 percent of all astronomers were involved in the discovery, estimates astronomer Bryan Gaensler of the University of Toronto. One of the many papers announcing results, published in Astrophysical Journal Letters, boasted over 3,000 authors. Additional papers appeared in that journal, as well as in Science, Nature and Physical Review Letters, among others.

Already, this event has ruled out hundreds of theories that provide alternatives to dark energy, a perplexing facet of our universe that is the most common explanation for why the cosmos is expanding faster and faster. Some of these theories aren’t consistent with the near-simultaneous detection of light and gravitational waves, several teams reported online October 17 and 18 at arXiv.org (SN: 11/25/17, p. 10).

Researchers also made a new measurement of how fast the universe is expanding, a number that could help solve a lingering puzzle. Observations of supernovas suggest that the universe is expanding at 73 kilometers per second for each megaparsec (about 3.3 million light-years). That’s significantly faster than measurements made using the cosmic microwave background, ancient light from the early years of the universe (SN: 8/6/16, p. 10), which peg the expansion rate at 67 km/s per megaparsec. The new measurement is right in between the previous two, at 70 km/s per megaparsec, researchers reported online in Nature. Resolving the impasse will require catching additional neutron star mergers.

Astrophysicists are confident they’ll get that chance. “This event is just the first of many that will be discovered in the future,” Loeb says. Additional mergers could also tell scientists more about the properties of neutron stars, like how “squishy” their extremely dense matter is, a property known as the equation of state (SN: 12/23/17, p. 7). And pinning down how often such collisions occur will help determine whether neutron stars can explain the abundances of heavy elements observed in the cosmos.

Gravitational waves also topped Science News’ list of discoveries in 2016. That honor marked the first direct detection of gravitational waves (SN: 12/24/16, p. 17), which the LIGO team captured in the aftermath of a merger of two black holes. This year, three pioneers of LIGO, Rainer Weiss of MIT and Kip Thorne and Barry Barish, both of Caltech, received the Nobel Prize in physics for that discovery (SN: 10/28/17, p. 6).

With the neutron star collision, gravitational wave hunters continue their streak of successes. The sighting came just days after a detection of two merging black holes — LIGO’s fourth and the first made in conjunction with Virgo (SN: 10/28/17, p. 8). A fifth black hole merger was reported in November (SN Online: 11/16/17). LIGO and Virgo are now shut off for upgrades until the fall of 2018. Additional gravitational wave detectors, like KAGRA in Japan and LIGO-India, are planned for the future, filling out a global network for monitoring the heavens’ temblors.

Both the fourth black hole collision and the neutron star crash appeared during a short window of less than a month when all three existing gravitational wave detectors were simultaneously operational. That was a stroke of amazing luck: Three detectors are needed to narrow down the location of a neutron star collision and pinpoint its light.

“It’s a little unreasonable how lucky we are,” Holz says. “It really was this gift — just a gift.”
===============
Five things we learned from last year’s Great American Eclipse
It’s been a year since the total solar eclipse of August 21, 2017, captured millions of imaginations as the moon briefly blotted out the sun and cast a shadow that crisscrossed the United States from Oregon to South Carolina.

“It was an epic event by all measures,” NASA astrophysicist Madhulika Guhathakurta told a meeting of the American Geophysical Union in New Orleans in December. One survey reports that 88 percent of adults in the United States — some 216 million people — viewed the eclipse either directly or electronically.

Among those were scientists and citizen scientists who turned their telescopes skyward to tackle some big scientific mysteries, solar and otherwise. Last year, Science News dove deep into the questions scientists hoped to answer using the eclipse. One year out, what have we learned?

1. The eclipse sent ripples through Earth’s atmosphere.

Normally, the sun’s radiation splits electrons from atoms in the atmosphere, forming a charged layer called the ionosphere, which stretches from 75 to 1,000 kilometers up. But when sunlight briefly disappears during an eclipse, the electrons rejoin their atoms, creating a disturbance in the ionosphere that is detectable with receivers on the ground (SN Online: 8/13/17).

The moon’s supersonic shadow produced a bow wave of atoms piling up in the ionosphere, similar to the wave at the prow of a boat, Shun-Rong Zhang of MIT’s Haystack Observatory in Westford, Mass., reported in December. Although such bow waves were predicted in the 1960s, this was the first time they were definitively observed.

The eclipse also sent a wave traveling through the thermosphere, an uncharged layer of the atmosphere about 250 kilometers high, that was observed from as far away as Brazil nearly an hour after the eclipse ended (SN: 5/26/18, p. 14). And measurements of temperature, wind speed and sunlight intensity showed that the eclipse briefly changed the weather along the path of darkness.

2. Showing Einstein was right is not so simple.

Physicists chased the moon’s shadow to redo the iconic experiment that showed Einstein’s theory of general relativity was correct (SN Online: 8/15/17). In Einstein’s view, the sun’s mass should warp spacetime enough that the positions of stars should appear to be slightly different during an eclipse. During the May 1919 solar eclipse, British astronomer Arthur Stanley Eddington took photographs that proved Einstein right.

During the 2017 eclipse, almost a century later, amateur astronomer Donald Bruns of San Diego made similar measurements with modern equipment and came to the same conclusion as Eddington: Stars visible during the eclipse were all askew. Bruns published his results in Classical and Quantum Gravity in March.

But astrophysicist Bradley Schaefer of Louisiana State University in Baton Rouge and others had far more difficulty reproducing the measurement with enough precision to show that Einstein was right. “‘Bummer’ is an understatement,” Schaefer says. “This all may have been for naught.”

Schaefer had enough trouble that he thinks it may have been impossible for Eddington to get the precision he claimed. The earlier astronomer may have hit upon the right answer by luck, not because he actually measured it.

3. Infrared light will help measure the corona’s magnetic field.

Some eclipse experiments didn’t revolutionize our understanding of the sun on their own, but will enable future ones to pull back the veil. One of these was the first infrared observations of the sun’s corona, the shimmering halo of hot, diffuse plasma that is only visible in its entirety during a total solar eclipse. The shape and motion of all that plasma are guided by magnetic fields, but the corona’s magnetic field is so weak that it has never been measured directly (SN Online: 8/16/17).

Previous studies suggested that infrared wavelengths of light might be particularly sensitive to the corona’s magnetic field. So two groups chased the August 2017 eclipse in airplanes to get some infrared observations. Amir Caspi of the Southwest Research Institute in Boulder, Colo., and his colleagues took the first infrared image of the entire corona.

Flying in another aircraft, Jenna Samra of Harvard University measured the corona in five specific wavelengths, one of which had never been seen before. Comparing those results with observations taken from the ground in Casper, Wyo., (where I watched the eclipse) showed that those wavelengths are bright enough that a telescope now under construction in Hawaii will be able to help map the corona’s magnetism (SN Online: 5/29/18).

4. Figuring out what heats the corona will take more work.

Almost every experiment aimed at the eclipsed sun last August had some bearing on the biggest solar mystery of all: Why is the corona so hot? The solar surface simmers at around 5500° Celsius, but the corona — despite being farther away from the solar furnace and made of much more diffuse material — rages at millions of degrees.

One year after the Great American Eclipse, scientists are still scratching their heads. Caspi’s team searched for waves rippling through the corona, which could distribute energy far from the solar surface. Those waves could also help comb out magnetic tangles in the corona and explain its well-ordered look (SN Online: 8/17/17).

In a complementary measurement, the group in Wyoming saw signs of neutral helium atoms in the corona, says solar physicist Philip Judge of the National Center for Atmospheric Research in Boulder. Those uncharged atoms probably represent cool material embedded in the corona (SN Online: 6/16/17).

Similar cool spots have been seen during earlier eclipses, although it’s hard to imagine how the cool atoms survive in the searing heat, like ice cubes remaining solid in hot soup. But collisions between charged ions and neutral atoms could help convert ordered motions, like Caspi’s waves, into coronal heat.

The results so far are interesting, but inconclusive, Caspi says. “It’s certainly possible we will get some very interesting results from this set of observations alone,” he says. But for such a big problem as coronal heating, eclipse observations may play a supporting role to more direct measurements, such as those that the recently launched Parker Solar Probe will make (SN Online: 8/12/18).

5. People are already looking to the next eclipse.

A survey done by researchers at the University of Michigan found that eclipse watchers sought more information about eclipses and the scientific questions involved an average of 16 times in the three months following the event.

Several research groups are planning observations for the next total eclipses, visible in South America in July 2019 and December 2020 (SN: 8/5/17, p. 32). Caspi and Samra’s teams both hope to fly through those eclipses in aircraft again.

And amateurs and pros alike are preparing for the Great American Eclipse version 2.0, which will cross from Texas to Maine in 2024.

“Everybody’s eyes are on 2024,” Caspi says.
===============
On a mountain in Wyoming, the eclipse brings wonder — and, hopefully, answers
View the video

CASPER MOUNTAIN, Wyo. — It’s nothing like a sunset. It’s cold and dark, but it’s not like nighttime, or even twilight. The moon just snaps into place over the last slivers of the sun, turning the sun into a dark hole. The only illumination — a flat, ghostly, metallic sort of light — is from peaked gossamer streamers stretching out toward the edges of the sky.

I’ve been writing about eclipse science and interviewing researchers who study that eerie halo for the better part of a month. I thought I knew what to expect from my first total solar eclipse.

I had no idea.

I’m at a Baptist summer camp called Camp Wyoba about a half hour’s drive up a mountain from Casper, Wyo., with a group of engineers and solar physicists. Most come from the National Center for Atmospheric Research, or NCAR, in Boulder, Colo.

Our presence here is a stroke of luck: Retired NCAR researcher William Mankin’s wife Mary Beth is a Baptist pastor. When they realized the camp would be in the path of the total eclipse, the Mankins suggested holding an event, complete with a scientific lecture the night before and a church service in the morning. They also invited Mankin’s former NCAR colleagues to bring their experiments — and their families.

The day before the eclipse, scientists tested their equipment in a field at the top of Casper Mountain near camp, while a group of kids played dodgeball nearby. But by afternoon, the team’s luck seemed to be flagging. One of their telescopes started malfunctioning in a way they hadn’t seen before. They had less than 24 hours to fix it. “It’s a very bad thing if we can’t get it going,” said instrument leader Steven Tomczyk.

Tomczyk and his colleagues schlepped three telescopes and a spectrometer the size of a coffee table up here to try to solve one of the greatest mysteries of the sun’s corona: Why this ethereal solar atmosphere is so much hotter than the sun’s surface?

INTO THE DARK This time-lapse video shows how a group of solar physicists and engineers studying the sun’s wispy atmosphere kept busy during totality, but also got to take a look at the corona with their own eyes. In the foreground, Paul Bryans and Ben Berkey uncover and cover the telescopes’ lenses, while Steven Tomczyk, Alyssa Boll and Keon Gibson record data and Philip Judge calls out the time. L. Grossman

The visible surface of the sun is about 5,500° Celsius. Higher up in the sun’s atmosphere, though, the temperature jumps to 10,000° C and then makes a sudden leap to millions of degrees. It’s a real puzzle why. Most materials transfer heat via atoms smacking into each other or through swirling, churning currents. In the corona, which is made of a diffuse charged gas called plasma, particles are so far apart that neither scenario seems likely.

Solar physicists are pretty sure that the corona’s magnetic field is somehow to blame for the heat up (SN Online: 8/16/17), but it’s so weak that it has never been measured directly. So the team in Wyoming hopes to chip away at understanding that magnetic field. Their experiments will take steps toward measuring its strength and shape so that a future telescope can make a more complete measurement.

The spectrometer will measure the corona in infrared wavelengths between 1 and 6 micrometers — the first time it has been measured fully in this range. Infrared light is a good probe of the magnetic field because stronger magnetic fields change the way light is emitted in that range. Atoms in the corona are so hot that they give up many of their electrons — iron atoms have been known to lose up to half of their original count. The remaining electrons are often excited to higher energy levels, and when they drop back into their original state, they emit a particle of light in a particular wavelength. That photon shows up as a peak in the spectrum.

Magnetic fields make the higher energy levels split into two new levels, so electrons dive from two different platforms and emit different particles of light. That makes the peak split in two as well. The stronger the magnetic field, the farther the distance between the peaks.

The spectrometer won’t directly view the sun — it’s inside a trailer. A hole in the trailer wall leads to an angled mirror, which will track the eclipsed sun as it moves across the sky and direct the light into the instrument.

There, a beam splitter will split the light in two and direct it through a series of gold-plated mirrors. Ultimately, the light beams will be recombined. If all goes well, the shape of the light wave at the end will allow the team to calculate the sun’s infrared spectrum. They’re looking for already known peaks in the spectrum — one from silicon that has lost eight electrons, for instance, was observed in 2003 when the sun wasn’t eclipsed — and ones theorized in the 1990s but never observed.

“We’re at the ragged edge of our signal to noise,” says James Hannigan, who’s in charge of the spectrometer. “I’m really not sure what we’re gonna see.”

This eclipse is this instrument’s maiden voyage; it was designed in the 1990s but completed only a few months ago. It has had some last-minute headaches, too, Hannigan says. The beam splitter, a sort of half-transparent mirror, had to be polished until its height varied no more than 80 nanometers — or 80 billionths of a meter. It was so difficult to do that the piece of equipment arrived at Hannigan’s house only nine days before the eclipse. “It’s a little more harried than I would have liked,” Hannigan says. “I would have loved to have been testing this thing for the last month and a half, but so it goes.”

Outside, Tomczyk and the rest of the crew are testing the three telescopes. One will take a picture of the entire corona in infrared wavelengths out to 10 solar radii away from the sun’s surface. That will provide context for the other measurements, letting the team figure out the strength of the field in different parts of the corona.

Another is actually two telescopes linked together: one infrared and one that measures visible wavelengths. Both send data to a spectrograph, which splits light into all its component wavelengths. The visible light telescope’s job is to take a quick spectrum of the layers of the sun’s atmosphere between the photosphere and the corona, an area called the chromosphere.

The chromosphere is only visible for a few seconds at the beginning and end of an eclipse. For those few seconds, the visible telescope will take a picture once every 1/125th of a second. “It will help us understand how the atmosphere is changing with height, which helps connect the corona to the surface,” says Philip Judge, one of the experiment’s principal investigators.

The third telescope — a polarization camera that will measure the magnetic field’s shape — is the one that’s acting up.

“We’ve been rehearsing this dance over the last couple of days,” says Ben Berkey, who works for NCAR in Hawaii. They’ve practiced every motion they’ll make during the eclipse: Check that the sun is in each telescope’s field of view; remove the lens caps at just the right moment, to get as much time watching the corona as possible without frying the delicate instruments; and so on.

“If things are boring, that’s not a bad thing necessarily,” Tomczyk says during one run-through.

“But you won’t be bored,” says Paul Bryans, one of the science leads. “You’ll be watching the partial eclipse.”

By 4 p.m., the problem with the polarimeter is solved: The computer storing the data needed its hard drive reconfigured. The team is so nervous about losing the data that they plan to make four copies of the hard drives before leaving Casper Mountain, and send them back to NCAR in four different cars, just in case. “They’re precious,” Tomczyk says.

The morning of the eclipse dawns cool and clear.

It’s already getting chilly when Judge bellows, “Two-minute warning!” The team jumps into action, taking peeks at the last tiny slices of sunlight through eclipse glasses.

The moment of totality is sudden and absolute. The corona pops into view all at once, pointing its silvery arms at the treetops and the sky. People cheer; some children scream. Someone lends me a pair of binoculars, and through them I can see the chromosphere, glowing red and purple. I can see Mercury, nestled right up next to the corona.

And just as suddenly, it’s over. Judge counts down the seconds to the end of totality, and right on schedule, the sun returns. It’s incredible how much light that tiny dot of sunlight provides. I had been told that a 99 percent eclipse is nothing at all like a total eclipse. I get it now.

Tomczyk and the crew, meanwhile, are already backing up their data and taking the telescopes off their tripods. All the instruments worked, although they’ll have to take the data back to Boulder and process it to know if they got all they’d hoped for.

“Who knows what we’ll see,” Tomczyk says. “I feel exhausted. And relieved.”

Editor’s note: This story was updated August 24, 2017, to correct the photo caption. The bright spot in the lower left corner is the star system Regulus, not Mercury.
===============
Colliding neutron stars, gene editing, human origins and more top stories of 2017
In science, progress rarely comes in one big shebang. Well, it has now, two years running. The first-ever direct detection of gravitational waves, our top story in 2016, launched a long-dreamed-of kind of astronomy capable of “unlocking otherwise unknowable secrets of the cosmos,” as physics writer Emily Conover puts it. 2017’s key event: a never-before-seen neutron star collision that immediately validated some theories in physics and killed others. And so a new way to probe cosmic mysteries wins our top spot again this year.

Another turning point is coming, and maybe soon, via CRISPR/Cas9, a biotechnology that holds the promise of curing genetic diseases (and the peril of making permanent, heritable tweaks). Nearly five years after the gene-editing tool debuted, researchers for the first time have used it to alter genes in viable human embryos. That’s a big advance, and worthy of the No. 2 spot.

Don’t be fooled, though. Even eureka moments like these are the fruits of the slow build of progress: Fossil by fossil, paleoanthropologists draw a picture of Homo sapiens’ earliest days. Brain by brain, the extent of damage caused by chronic traumatic encephalopathy — a disease linked to hard head knocks — becomes clear.

And crack by crack, one of the biggest icebergs ever recorded calves. That story, No. 3 on our list, is not exactly progress, but it’s surely an opportunity to make scientific headway. Teams racing to Antarctica’s Larsen C ice shelf will have an unprecedented chance to collect real-time data on how the remaining ice reacts and to reveal secrets of a long-hidden ecosystem. Building on those advances, as well as others described in our Top 10 picks, will fuel “aha!” moments — both revolutionary and incremental — well into the future. — Macon Morehouse, News Director

The Top 10 Stories of 2017
===============
Here are our favorite science books of 2017
Have you fallen behind on your reading this year? Or maybe you’ve plowed through your must-reads and are ready for more. Science News has got you covered. Here are the staff’s picks for some of the best science books of 2017. Find detailed reviews from previous issues in the links below or in our Editors pick: Favorite books of 2017.

Against the Grain

James C. Scott Armed with the latest archaeological research, a political anthropologist argues that the rise of civilization came at a big cost. The initial switch from hunting and gathering to agricultural states brought poor diets, labor-intensive work, outbreaks of infectious diseases and other hardships (SN: 10/14/17, p. 28). Yale Univ., $26

The Great Quake

Henry Fountain Historical records and interviews with survivors flesh out this tale of how a massive earthquake in Alaska in 1964 provided geologists with key evidence needed to verify the theory of plate tectonics (SN: 9/16/17, p. 32). Crown, $28

Eclipse

Frank Close More than just a primer on the science of solar eclipses, this memoir chronicles a physicist’s lifetime fascination with the celestial phenomenon and introduces readers to the quirky world of eclipse chasers (SN: 5/13/17, p. 28). Oxford Univ., $21.95

Rise of the Necrofauna

Britt Wray Resurrecting woolly mammoths, passenger pigeons and other extinct creatures isn’t just a technological problem, as this book explains. “De-extinction” is also rife with ethical dilemmas (SN: 10/28/17, p. 28). Greystone Books, $26.95

Big Chicken

Maryn McKenna Antibiotics transformed chicken farming, to the detriment of the birds and of human health, a journalist contends. Widespread use of the drugs fueled the industrialization of poultry production and the rise of antibiotic-resistant bacteria (SN: 9/30/17, p. 30). National Geographic, $27 Inferior

Angela Saini A science writer makes a persuasive case that centuries of biased thinking and flawed scientific research have reinforced sexist stereotypes about women (SN: 9/2/17, p. 27). Beacon Press, $25.95

Caesar’s Last Breath

Sam Kean Through fun historical anecdotes and lesser-known backstories of scientific greats, this entertaining book profiles the chemical elements that make up the air we breathe and traces the history of Earth’s atmosphere (SN: 7/8/17 & 7/22/17, p. 38). Little, Brown and Co., $28

Cannibalism

Bill Schutt The grisly practice of eating your own kind turns out to be widespread in the animal kingdom, a zoologist explains in this captivating look at cannibalism (SN: 2/18/17, p. 29). Algonquin Books, $26.95

The Lost City of the Monkey God

Douglas Preston A journalist tags along on an archaeological expedition to search for the real-life remains of a mythological city in this rainforest adventure tale that morphs into a medical mystery (SN: 2/4/17, p. 28). Grand Central Publishing, $28

The Death and Life of the Great Lakes

Dan Egan Invasive species, urbanization and other threats have wreaked havoc on the Great Lakes, but this book still finds some glimmers of hope in the scientists who are making headway in resuscitating the ecosystem (SN: 3/18/17, p. 30). W.W. Norton & Co., $27.95

How to Tame a Fox

Lee Alan Dugatkin and Lyudmila Trut An experiment to replay animal domestication by selectively breeding wild silver foxes is lovingly retold, including by the researcher who has kept the project alive for nearly 60 years (SN: 5/13/17, p. 29). Univ. of Chicago, $26

Making Contact

Sarah Scoles In the face of numerous obstacles, Jill Tarter still managed to spearhead the search for extraterrestrial intelligence for decades, as this biography recounts (SN: 8/5/17, p. 26). Pegasus Books, $27.95

A Crack in Creation

Jennifer A. Doudna and Samuel H. Sternberg Two experts, including one of the pioneers of CRISPR/Cas9, discuss the science and ethics of gene editing. Houghton Mifflin Harcourt, $28

These book reviews contain links to Amazon.com. Science News is a participant in the Amazon Services LLC Associates Program. Please see our FAQ for more details.
===============
Resurrecting extinct species raises ethical questions
Rise of the Necrofauna

Britt Wray

Greystone Books, $26.95

A theme park populated with re-created dinosaurs is fiction. But if a handful of dedicated scientists have their way, a park with woolly mammoths, passenger pigeons and other “de-extincted” animals could become reality.

In Rise of the Necrofauna, writer and radio broadcaster Britt Wray presents a comprehensive look at the unprecedented technical difficulties of raising the dead, plus the deep philosophical questions surrounding de-extinction.

The aim of current de-extinction efforts is to use gene-editing tools to engineer living species to re-create extinct cousins, such as engineering a woolly mammoth from an elephant. This &ldquo
===============
Saturn’s ‘ring rain’ is a surprising cocktail of chemicals
The “ring rain” of material falling from Saturn’s rings into the planet’s atmosphere is a much more intense, contaminated downpour than scientists thought.

For decades, astronomers have suspected that Saturn’s rings pelt the planet with grains of water ice, but some of the final observations from NASA’s Cassini spacecraft provide the first detailed views of these celestial showers (SN: 4/14/18, p. 6). Ring rain is highly contaminated with organic matter and other molecules, and hammers Saturn at thousands of kilograms per second, researchers report in the Oct. 5 Science. Understanding the rain’s surprising quantity and quality could help clarify the origins and evolution of Saturn’s rings.

Researchers analyzed data collected by Cassini’s Ion Neutral Mass Spectrometer during the spacecraft’s final few orbits in 2017, as it sailed through the gap between Saturn and its innermost ring, known as the D ring (SN Online: 9/15/17). Water constituted only about 24 percent of the material tumbling from Saturn’s ring system into its atmosphere; the rest was methane, carbon monoxide, dinitrogen, ammonia, carbon dioxide and fragments of organic nanoparticles.

The ring rain’s diverse chemical composition “was a big surprise,” because remote observations show that Saturn’s ring system, on the whole, is almost entirely water ice, says Cassini project scientist Linda Spilker of NASA’s Jet Propulsion Laboratory in Pasadena, Calif., who wasn’t involved in the study. Researchers aren’t sure why ring rain is so deprived of water.

Cocktail conundrum The ring rain that Cassini’s Ion Neutral Mass Spectrometer caught falling from Saturn’s rings into its atmosphere is only about 24 percent water — a major surprise, given that Saturn’s ring system is almost entirely water ice. The rest of the ring rain is composed of organic material and other molecules. The chemical breakdown of Saturn’s ring rain

A belt of radiation tucked inside Saturn’s rings could be responsible, says study coauthor Jeff Cuzzi, a planetary scientist at NASA’s Ames Research Center in Moffett Field, Calif. Cassini’s end-of-life observations also provided the first closeup views of this band of energetic protons, described in another study in the Oct. 5 Science. The belt’s high-energy radiation may have blasted away much of the water in the D ring very close to Saturn, leaving behind heavier materials like organic compounds, Cuzzi speculates.

Whatever its cause, the ring rain’s peculiar composition may reshape our understanding of the carbon and nitrogen content of Saturn’s atmosphere, says Hunter Waite, a planetary scientist at the Southwest Research Institute in San Antonio and a coauthor of the ring rain study. Saturn’s presumed chemical makeup may be “just a veneer” created by falling material masking the gas giant’s true composition, he says.

In addition to the ring rain’s chemistry, the Cassini observations also revealed how quickly this material is falling into Saturn’s atmosphere: between 4,800 and 45,000 kilograms per second.

I spy Observations from Cassini’s final few months revealed a radiation belt composed of high-energy protons tucked inside the gas giant’s ring system. This belt (bright colors below) passes through Saturn’s thin, innermost D ring, and might be stripping water out of the ring material closest to Saturn.

“It’s just an enormous amount of mass flowing into the planet,” Cuzzi says.

Saturn’s rings probably haven’t sustained this rate of rainfall over the course of the planet’s history, he says. If that were the case, the spindly little D ring would have long since eroded away. But a comet or other rogue object may have hit Saturn’s ring system recently, jostling grains out of their usual orbit and making them more vulnerable to Saturn’s gravitational pull.

But if the current deluge is, in fact, Saturn’s normal rate of ring rain, then perhaps the D ring continually siphons material from Saturn’s outer rings, says Kelly Miller, a cosmochemist at the Southwest Research Institute and a coauthor on the ring rain study.

The exact nature of Saturn’s relationship to its rings “is still a puzzle,” Spilker says. But devising more detailed theoretical simulations of what’s going on between the planet’s atmosphere and rings, and testing these theories against the trove of data collected by Cassini could help fill in the picture.

Such work may help scientists better understand not only Saturn, but also any ring-bearing planets that might circle other stars in the universe, she says.
===============
5 things we’ve learned about Saturn since Cassini died
THE WOODLANDS, Texas — It’s been six months since NASA’s Cassini spacecraft plunged to its doom in the atmosphere of Saturn, but scientists didn’t spend much time mourning. They got busy, analyzing the spacecraft’s final data.

The Cassini mission ended September 15, 2017, after more than 13 years orbiting Saturn (SN Online: 9/15/17). The spacecraft’s final 22 orbits, dubbed the Grand Finale, sent Cassini into the potentially dangerous region between the gas giant and its rings, and its final orbit sent it directly into Saturn’s atmosphere.

That perspective helped solve mysteries about the planet and its moons that could not be tackled any other way, scientists said March 19 at the Lunar and Planetary Science Conference in The Woodlands, Texas.

“In so many ways, the Grand Finale orbits provided information that was totally unexpected,” said Cassini project scientist Linda Spilker of NASA’s Jet Propulsion Laboratory in Pasadena, Calif. “So many of our models were not correct.”

Here are five things we now know and a few outstanding mysteries.

1. Saturn’s clouds go deep

Those final daredevil orbits allowed Cassini to measure the gravity of Saturn and its rings independent of one another. Looking at the planet’s gravity field alone revealed that the swirling bands of clouds penetrate much deeper into the planet than expected.

Astronomers this month announced a similar discovery for an even larger gas giant, reporting that the Juno spacecraft, which is orbiting Jupiter, had found that the planet’s rotating cloud belts reach roughly 3,000 kilometers below the top of the atmosphere.

Saturn’s clouds reach a few times deeper than that. “This was an astonishing result,” Spilker said.

“People used to think that maybe Saturn was just a slightly smaller version of Jupiter, but it’s evident that that’s not the case,” says planetary scientist Paul Schenk of the Lunar and Planetary Institute in Houston, who was not involved in the gravity measurements. The difference speaks to how diverse planets are, he says. “Every place you look, everywhere we’ve been to, it’s just been so dramatically different and unique.”

2. Ring rain is eroding the innermost ring

Grains of ice from the rings are raining down into Saturn’s atmosphere, Cassini’s final orbits confirmed. This “ring rain” idea has been suggested since the 1980s, but only by tasting the atmosphere and directly sampling the space between Saturn and the rings could Cassini confirm the rains are real.

In its last five full orbits, Cassini found a zoo of organic molecules in and just above Saturn’s atmosphere, said planetary scientist Kelly Miller of the Southwest Research Institute in San Antonio. The spacecraft found a lot of water, which wasn’t surprising — water makes up about 90 percent of the rings. But there were also a lot of hydrocarbons similar to propane, plus some methane and sulfur-bearing molecules.

The types of molecules became less well-mixed as the spacecraft looked deeper into Saturn’s atmosphere, which is what would happen if the particles came from the rings and sank at different speeds. The researchers think this material is especially raining from Saturn’s D ring, the thin innermost ring. Other Cassini data suggest this ring is losing mass.

“The D ring is slowly being eroded away and going into the planet,” Spilker said.

3. Organics could explain mysterious ring hues

The organics in the ring rain could solve a debate about why Saturn’s rings appear reddish in some spots.

“We’ve had this debate going on for a couple of years now — are they red because of good old-fashioned rust like Mars, or because of the same kinds of organic materials … that make carrots and tomatoes and watermelon red?” said planetary scientist Jeff Cuzzi of NASA's Ames Research Center in Moffett Field, Calif. “To me, this answers the question of what makes the rings red: It’s organics.”

It’s still not clear where the organics come from, though. They could be created within the rings, or they could come from cosmic dust from the tails of comets. Miller and her colleagues are comparing the ring rain molecules with data on comet 67P, which the Rosetta spacecraft observed, to see how well they match up (SN: 11/11/17, p. 32).

4. Titan’s “magic islands” aren’t islands, or bubbles

Mysterious disappearing features in the lakes of Saturn’s moon Titan are caused by sunlight reflecting off giant waves, said planetary scientist Alexander Hayes of Cornell University.

These features were named “magic islands” when they were first spotted in 2014. As recently as April 2017, planetary scientists thought they had the islands solved: They seemed to be the result of champagnelike bubbles of nitrogen burbling through the moon’s methane and ethane seas (SN Online: 4/18/17).

But Hayes presented newly analyzed data from August 2014, when Cassini looked at Kraken Mare, the moon’s largest northern sea, in radar and infrared wavelengths within two hours of each other. The radar images showed a magic island, and the infrared ones showed a peak in brightness at the same spot.

Because the observations were taken two hours apart, the island probably couldn’t have been due to bubbles, Hayes said — bubbles would pop or disperse too quickly. Instead, he thinks the brightening could be the glint of sunlight reflecting directly off of giant waves on the lake, like how the ocean ripples with gold at sunset. Simulations of Titan’s atmosphere suggest these waves could be raised by winds as slow as 0.5 meters per second, which would barely move a wind vane on Earth.

5. Enceladus’ plumes may brighten by the pull of another moon

Saturn’s tiny moon Enceladus has plumes that may be driven by nudges from another moon.

The spurts of liquid water were discovered in 2006. Over the next six years, scientists noticed that the plumes varied in brightness (a proxy for how much material is gushing from the moon) on a daily cycle, probably driven by Saturn’s different positions in Enceladus’ sky.

Then, in 2015 some researchers noted that the plumes’ overall brightness had been decreasing since the beginning of the Cassini mission.

One possible explanation was that the plumes changed with Saturn’s seasons. Another was that ice built up in the vents, clogging them and decreasing the flow. But looking at the full 13-year dataset, planetary scientist Francis Nimmo found that the plumes grow brighter in a regular cycle every four and 11 years. The pattern is too coherent to be explained by clogged vents, said Nimmo, of the University of California, Santa Cruz. Oddly, the plume grew brighter in 2017, so the seasonal explanation doesn’t fit either.

The variations could be explained by a neighboring moon, Dione. Every time Dione and Enceladus line up, their gravitational stress on each other could force Enceladus’ vents open a bit more, causing the plumes to grow brighter.

Unsolved enigmas

So far, analyzing data from Cassini hasn’t answered all of scientists’ questions. Is Enceladus the only moon with plumes? Dione showed signs of activity, too, but Cassini wasn’t able to confirm it. How thick is Enceladus’ ice sheet? Why are Titan’s smaller lakes full of clear, pure methane, when scientists expected the lakes to be clogged with hydrocarbon silt?

Even though the spacecraft is gone, it left decades’ worth of data to sift through in search of answers. “Cassini is going to keep on giving as long as we keep looking,” Hayes said.

Editors’ note: This story was updated on March 21, 2018, to include the affiliations of Jeff Cuzzi and Francis Nimmo.
===============
Observers caught these stars going supernova
In A.D. 185, Chinese records note the appearance of a “guest star” that then faded away over the span of several months. In 1572, astronomer Tycho Brahe and many others watched as a previously unknown star in the constellation Cassiopeia blasted out gobs of light and then eventually disappeared. And 30 years ago, the world witnessed a similar blaze of light from a small galaxy that orbits the Milky Way. In each case, humankind stood witness to a supernova — an exploding star — within or relatively close to our galaxy (representative border in gray, below).

Here’s a map of six supernovas directly seen by human eyes throughout history, and one nearby explosion that went unnoticed. Some were type 1a supernovas, the detonation of a stellar core left behind after a star releases its gas into space. Others were triggered when a star at least eight times as massive as the sun blows itself apart.

Tap images below map for details
===============
When a nearby star goes supernova, scientists will be ready
Almost every night that the constellation Orion is visible, physicist Mark Vagins steps outside to peer at a reddish star at the right shoulder of the mythical figure. “You can see the color of Betelgeuse with the naked eye. It’s very striking, this red, red star,” he says. “It may not be in my lifetime, but one of these days, that star is going to explode.”

With a radius about 900 times that of the sun, Betelgeuse is a monstrous star that is nearing its end. Eventually, it will no longer be able to support its own weight, and its core will collapse. A shock wave from that collapse will speed outward, violently expelling the star’s outer layers in a massive explosion known as a supernova. When Betelgeuse detonates, its cosmic kaboom will create a light show brighter than the full moon, visible even during the daytime. It could happen tomorrow, or a million years from now.

Countless stars like Betelgeuse — any of which could soon explode — litter the Milky Way. Scientists estimate that a supernova occurs in our galaxy a few times a century. While brilliantly gleaming supernovas in far-flung galaxies are regularly spotted with telescopes trained on the heavens, scientists eagerly hope to capture two elusive signatures that are detectable only from a supernova closer to home. These signatures are neutrinos (subatomic particles that stream out of a collapsing star’s center) and gravitational waves (subtle vibrations of spacetime that will also ripple out from the convulsing star).

A star’s last moments Over millions of years, a large star fuses nuclei into increasingly heavy elements. Once iron forms in the core, fusion stops and gravity overwhelms the star, collapsing its core and setting up a shock wave (yellow) that travels outward. Neutrinos emanating from the core push the shock wave toward the star’s surface. The star explodes, flinging elements into space. 1. Star's core collapses. t=0 seconds 2. The core rebounds, producing a shock wave.

t=0.1 seconds 3. Neutrinos give the shock wave a boost, keeping it moving as outer material falls inward. t=0.3 seconds 4. Shock wave travels through the layers, expelling material outward. t>0.3 seconds 5. Shock wave reaches surface and star explodes.

t> 2 hours Images: E. Otwell Sources: H.-T. Janka et al/Prog. Theor. Exp. Phys. 2012; Bronson Messer

“These two signals, directly from the interior of the supernova, are the ones we are really longing for,” says Hans-Thomas Janka, an astrophysicist at the Max Planck Institute for Astrophysics in Garching, Germany. Unlike light, which is released from the star’s surface, stealthy neutrinos and gravitational waves would give scientists a glimpse of the processes that occur deep inside a collapsing star.

Supernovas offer more than awe-inspiring explosions. When they erupt, the stars spew out their guts, seeding the cosmos with chemical elements necessary for life to exist. “We clearly wouldn’t be here without supernovas,” says Vagins, of the Kavli Institute for the Physics and Mathematics of the Universe at the University of Tokyo. But the processes that occur within the churning mess are still not fully understood. Computer simulations have revealed much of the physics of how stars explode, but models are no substitute for a real-life nearby blast.

One inspiration for scientists is supernova 1987A, which appeared in the Large Magellanic Cloud, a satellite galaxy of the Milky Way, 30 years ago (SN: 2/18/17, p. 20). That flare-up hinted at the unparalleled information nearby supernovas could provide. With the detectors available at the time, scientists managed to nab just two dozen of 1987A’s neutrinos (SN: 3/21/87, p. 180). Hundreds of papers have been written analyzing that precious handful of particles. Calculations based on those detections confirmed scientists’ hunch that unfathomably large numbers of neutrinos are released after a star’s core collapses in a supernova. In total, 1987A emitted about 1058 neutrinos. To put that in perspective, there are about 1024 stars in the observable universe — a vastly smaller number.

Since 1987, neutrino detectors have proliferated, installed in exotic locales that are ideal for neutrino snagging, from the Antarctic ice sheet to deep mines across the globe. If a supernova went off in the Milky Way today, scientists could potentially nab thousands or even a million neutrinos. Gravitational wave detection has likewise come on the scene, ready to pick up a slight shift in spacetime stirred up by the blast. Detecting such gravitational waves or a surfeit of supernova neutrinos would lead to a distinct leap in scientists’ knowledge, and provide new windows into supernovas. All that’s needed now is the explosion.

Early warning

Despite estimates that a few stars explode in the Milky Way every century, no one has glimpsed one since the early 1600s. It’s possible the explosions have simply gone unnoticed, says physicist John Beacom of Ohio State University in Columbus; light can be lost in the mess of gas and dust in the galaxy. A burst of neutrinos from a supernova would provide a surefire signal.

These hermitlike elementary particles shun most interactions with matter. Produced in stars, radioactive decay and other reactions, neutrinos are so intangible that trillions of neutrinos from 1987A’s explosion passed through the body of every human on Earth, yet nobody felt a thing. For supernovas like 1987A, known as type 2 or core-collapse supernovas, about 99 percent of the explosion’s energy goes into the tiny particles. Another, less common kind of supernova, type 1a, occurs when a remnant of a star called a white dwarf steals matter from a companion star until the white dwarf explodes (SN: 4/30/16, p. 20). In type 1a supernovas, there’s no core collapse, so neutrinos from these explosions are much less numerous and are less likely to be detectable on Earth.

For scientists studying supernovas, neutrinos’ reluctance to interact is an advantage. The particles don’t get bogged down in their exit from the star, so they arrive at Earth several hours or even more than a day before light from the explosion, which is released only after the shock wave travels from the star’s core up to its surface. That means the particles can tip off astronomers that a light burst is imminent, and potentially where it is going to occur, so they can have their telescopes ready.

Most neutrino experiments (there are more than a dozen) weren’t built for the purpose of taking snapshots of unpredictable supernovas; they were built to study neutrinos from reliable sources, like the sun, nuclear reactors or particle accelerators. Nevertheless, seven neutrino experiments have joined forces to create the SuperNova Early Warning System, SNEWS. If neutrino detectors in at least two locations report an unexpectedly large burst of neutrinos, SNEWS will send an e-mail alert to the world’s astronomers. The experiments involved are a weirdly diverse bunch, including IceCube, a detector composed of light sensors frozen deep in the ice of Antarctica (SN: 12/27/14, p. 27); Super-Kamiokande, which boasts a tank filled with 50,000 tons of water stationed in a mine in Hida, Japan; and the Helium and Lead Observatory, or HALO — with the motto “astronomically patient” — made of salvaged lead blocks in a mine in Sudbury, Canada. Their common thread: The experiments are big to provide a lot of material for neutrinos to interact with — such as lead, water or ice.

With light sensors sunk kilometers deep into the ice sheet, IceCube’s detector is so huge that it could pick up traces of a million neutrinos from a Milky Way supernova. Because it was designed to capture only the highest energy neutrinos that are rocketing through space, it’s not sensitive enough to detect individual neutrinos emitted during a supernova. Instead, IceCube’s focus is on the big picture: It catalogs an increase of light in its detectors produced by neutrinos interacting in the ice in time slices of two billionths of a second, says IceCube leader Francis Halzen of the University of Wisconsin–Madison. “We make a movie of the supernova.”

Super-Kamiokande is the neutrino detector that can pinpoint the location of the impending stellar paroxysm. It is a successor to Kamiokande-II, one of a few detectors to spot a handful of neutrinos from 1987A. Shortly after a burst of neutrinos from a nearby supernova, the detector could direct astronomers to zero in on a few degrees of sky. If that happens, says neutrino physicist Kate Scholberg of Duke University, “I expect anybody with the capability will be zooming in.”

Story continues after slideshow

On the lookout

Various neutrino detectors await signals emitted from a supernova, including Super-Kamiokande in Japan, IceCube in Antarctica and HALO in Canada. They are joined by a gravitational wave observatory, LIGO, with detectors in Louisiana (shown) and Washington state.

Languages of a supernova

Light and neutrinos are two of several languages that a supernova speaks. In that sense, supernova 1987A was a “Rosetta stone,” Beacom says. By scrutinizing 1987A’s light and its handful of neutrinos, scientists began piecing together the theoretical physics that explains what happened inside the star. In a future supernova, another language, gravitational waves, could add nuance to the tale. But the explosion has got to be close.

If neutrinos are elusive, gravitational waves border on undetectable. Minute tremors in space itself, predicted by Einstein’s general theory of relativity, are generated when massive objects accelerate. In 2016, scientists with the Advanced Laser Interferometer Gravitational-Wave Observatory, LIGO, announced the first direct detection of gravitational waves, produced by two merging black holes (SN: 3/5/16, p. 6). That milestone required a pair of detectors so precise that they can sense quivers that squish the detectors’ 4-kilometer-long arms by just a tiny fraction of the diameter of a proton.

Gravitational waves from a supernova should be even harder to tease out than those from merging black holes. The pattern of ripples is less predictable. Surveys of the properties of the many supernovas detected in other galaxies indicate that the explosions vary significantly from one to the next, says astroparticle physicist Shunsaku Horiuchi of Virginia Tech in Blacksburg. “We ask, ‘Is there a standard supernova?’ The answer is ‘No.’ ”

Despite the challenges, finding gravitational waves from supernovas is a possibility because the explosions are chaotic and asymmetrical, producing lumpy, lopsided bursts. An explosion that expands perfectly symmetrically, like an inflating balloon, would produce no gravitational waves. The gravitational wave signature thus can tell scientists how cockeyed the detonation was and how fast the star was spinning.

Gravitational waves might also reveal some of the physics of the strange stew of neutrons that makes up a protoneutron star — the beginnings of an incredibly dense star formed in a supernova. Scientists would like to catalog the compressibility of the neutron-rich material — how it gets squeezed and rebounds in the collapse. “The gravitational wave signature would have an imprint in it of this stiffness or softness,” says computational astrophysicist Tony Mezzacappa of the University of Tennessee.

There’s a chance the supernova would collapse into an even more enigmatic state — a black hole, which has a gravitational field so strong that not even light can escape. When a black hole forms, the flow of neutrinos would abruptly drop, as their exit route is cut off. Detectors would notice. “Seeing the moment that a black hole is born,” says Vagins, “would be a tremendously exciting thing.”

While neutrinos can be oracles of supernovas, a stellar explosion could reveal a lot about neutrinos themselves. There are three types of neutrinos: electron, muon and tau. All are extremely light, with masses less than a millionth that of an electron (SN: 1/26/13, p. 18). But scientists don’t know which of the three neutrinos is the lightest; a nearby supernova could answer that question.

Supernovas, with all the obscure physics at their hearts, have a direct connection to Earth. They are a source of many of the elements from which planets eventually form. As stars age, they fuse together heavier and heavier elements, forging helium from hydrogen, carbon from helium and so on up the periodic table to iron. Those elements, including some considered essential to life, such as carbon and oxygen, spew out from a star’s innards in the explosion.

“All the elements that exist — that are here on Earth — that are heavier than iron were either made in supernovas or other cataclysmic events in astronomy,” says physicist Clarence Virtue of Laurentian University in Sudbury, Canada. Gold, platinum and many other elements heavier than iron are produced in a chain of reactions in which neutrons are rapidly absorbed, known as the r-process (SN: 5/14/16, p. 9). But scientists still argue about whether the r-process occurs in supernovas or when neutron stars merge with one another. Pulling back the curtain on supernovas could help scientists resolve the dispute.

Even the reason supernovas explode and sow their chemical seeds has been vigorously debated. Until recently, computer simulations of supernovas have often fizzled, indicating that something happens in a real explosion that scientists are missing. The shock wave seems to need an extra kick to make it out of the star and produce the luminous explosion. The most recent simulations indicate that the additional oomph is most likely imparted by neutrinos streaming outward. But, says Mezzacappa, “At the end of the day, we’re going to need some observations against which we can check our models.”

Hurry up and wait

Supernovas’ potential to answer such big questions means that scientists are under pressure not to miss a big break. “You better be ready. If it happens and you’re not ready, then you will be sad,” Scholberg says. “We have to be as prepared as possible to gather as much information as we possibly can.”

If a detector isn’t operating at the crucial moment, there’s no second chance. So neutrino experiments are designed to run with little downtime and to skirt potential failure modes — a sudden flood of data from a supernova could crash electronics systems in an ultrasensitive detector, for example. Gravitational wave detectors are so finicky that interference as subtle as waves crashing on a nearby beach can throw them out of whack. And in upcoming years, LIGO is scheduled to have detectors off for months at a time for upgrades. Scientists can only hope that, when a supernova comes, everything is up and running.

Some even hope that neighboring stars hold off a little longer. “It seems like I’m always telling people that I’d like Betelgeuse to go off one year from now,” jokes Bronson Messer, a physicist who works on supernova simulations at Oak Ridge National Laboratory in Tennessee. With each improvement of the simulations, he’s eager for a bit more time to study them.

Messer keeps getting his wish, but he doesn’t want to wait too long. He’d like to see a supernova in the Milky Way during his lifetime. But it could be many decades. Just in case, Vagins, who’s been taking those nightly peeks at Betelgeuse, is doing his part to prepare the next generation. He no longer scans the skies alone. “I’ve already taught my 6-year-old son how to find that star in the sky,” he says. “Maybe I won’t get to see it go, but maybe he’ll get to see it go.”

This article appears in the February 18, 2017, issue of Science News with the headline, "Waiting for a Supernova: When a nearby star explodes, observatories plan to be ready."
===============
Ebola vaccine proves effective, final trial results show
An experimental Ebola vaccine has triumphed in West Africa.

Of 5,837 people in Guinea who received a single shot of the vaccine, rVSV-ZEBOV, in the shoulder, none became infected with the virus 10 to 84 days after vaccination. That’s “100% protection,” researchers report December 22 in the Lancet.

World Health Organization researcher Ana Maria Henao-Restrepo and colleagues tested a “ring vaccination” approach, by immediately vaccinating family members and other contacts of people infected with Ebola. This strategy seemed to staunch the virus’s spread. Among 4,507 people never vaccinated or who got a delayed vaccine, 23 contracted Ebola.

The findings echo preliminary results reported in 2015, and offer a promising line of defense for future outbreaks. But scientists still do not know how long-lasting the vaccine’s protection would be.

In late 2013, West Africa saw the beginning of what would become the largest Ebola outbreak in history, with more than 11,300 deaths reported, and 28,616 cases in Guinea, Sierra Leone and Liberia. Since then, scientists have been racing to create a safe and effective vaccine.
===============
Before his early death, Riemann freed geometry from Euclidean prejudices
Bernhard Riemann was a man with a hypothesis.

He was confident that it was true, probably. But he didn’t prove it. And attempts over the last century and a half by others to prove it have failed.

A new claim by the esteemed mathematician Michael Atiyah that Riemann’s hypothesis has now been proved may also be exaggerated. But sadly Riemann’s early death was not. He died at age 39. In his short life, though, he left an intellectual legacy that touched many areas of math and science. He was “one of the most profound and imaginative mathematicians of all time,” as the mathematician Hans Freudenthal once wrote. Riemann recast the mathematical world’s view of algebra, geometry and various mathematical subfields — and set the stage for the 20th century’s understanding of space and time. Riemann’s math made Einstein’s general theory of relativity possible.

“It is quite possible,” wrote the mathematician-biographer E.T. Bell, “that had he been granted 20 or 30 more years of life, he would have become the Newton or Einstein of the nineteenth century.”

Riemann’s genius developed despite unpromising circumstances. Born in Bavaria in 1826 the son of a Protestant minister, he was poor and often sick as a child. Bernhard was homeschooled until his teenage years, when he moved to live with a grandmother where he could attend school. Later his mathematical aptitude caught the attention of a teacher who provided Riemann a nearly 900-page-long textbook by the legendary French mathematician Adrien-Marie Legendre to keep the precocious student occupied. Six days later, Riemann returned the book to the teacher, having mastered its contents.

When he entered the University of Göttingen, Riemann began (at his father’s urging) as a theology student. But Göttingen was the home of the greatest mathematician of the era, Carl Friedrich Gauss. Riemann attended lectures by Gauss and dropped theology for mathematics. More advanced math instruction was available at Berlin, where Riemann studied for two years before returning to Göttingen to finish his math Ph.D.

Nowadays a Ph.D. is generally considered impressive, but in Germany back then it was only step one toward qualifying for a job. Step two was conducting and reporting advanced work on a specialized topic, to be delivered as a lecture to a university committee. Gauss encouraged Riemann to report on a new approach to geometry. Riemann titled his lecture on the topic, presented in 1854, “On the Hypotheses which Lie at the Foundations of Geometry.”

In that lecture, Riemann cut to the core of Euclidean geometry, pointing out that its foundation consisted of presuppositions about points, lines and space that lacked any logical basis. As those presuppositions are based on experience, and “within the limits of observation,” the probability of their correctness seems high. But it is necessary, Riemann asserted, to “inquire about the justice of their extension beyond the limits of observation, on the side both of the infinitely great and of the infinitely small.” Investigating the nature of the world, he said, should not be “hindered by too narrow views,” and progress should not be obstructed by “traditional prejudices.”

Freed from Euclid’s preconditions, Riemann derived an entirely different (non-Euclidean) geometry. It was this geometry that provided the foundation for general relativity — Einstein’s theory of gravity — six decades later.

Riemann’s insights stemmed from his belief that in math, it was important to grasp the ideas behind the calculations, not merely accept the rules and follow standard procedures. Euclidean geometry seemed sensible at distance scales commonly experienced, but could differ under conditions not yet investigated (which is just what Einstein eventually showed).

Riemann’s geometrical conceptions extended to the possible existence of dimensions of space beyond the three commonly noticed. By developing the math describing such multidimensional spaces, Riemann provided an essential tool for physicists exploring the possibility of extra dimensions today.

He made many other contributions to a wide range of technical mathematical issues. And he took great interest in the philosophy of mathematics (as Freudenthal said, had he lived longer, Riemann might eventually have become known as a philosopher). Among his most famous technical ideas was a conjecture concerning the “zeta function,” a complicated mathematical expression with important implications related to the properties of prime numbers. Riemann’s hypothesis about the zeta function, if true, would validate vast numbers of additional mathematical propositions that have been derived from it.

Riemann performed many calculations leading him to believe in his hypothesis, but did not find a mathematical proof before his early death. In fact, he spent much of the last four years of his life under the duress of tuberculosis, seeking relief by long stays in the more comfortable climate of Italy. He died there on July 20, 1866, two months before he would have turned 40.

Had he lived as long as Michael Atiyah (age 89), maybe Riemann would have proved his hypothesis himself.
===============
Seven facts and a mystery about hand, foot and mouth disease
A few days ago, Baby V came down with a bug. Her nose ran, her appetite disappeared and her little body radiated heat. Over the course of a rough night, mean red spots cropped up on the sides and bottom of her feet. A little later, we found similar blisters on her hand. And then, as veteran parents might have suspected by now, we saw blisters around her mouth. Her rash eventually spread, but that initial pattern cinched the diagnosis, our doctor told us: Hand, foot and mouth disease.



Not to be confused with the foot (hoof) and mouth disease that afflicts cows, sheep and pigs, hand, foot and mouth disease is a common childhood ailment caused by a handful of different viruses. Because the disease spreads like wildfire in day cares, playgrounds and pools, most kids get infected before they’re 5.

HFMD is caused by a virus, so there’s not much to do other than ride it out. This lull gave me ample time to become a bit of a HFMD hobbyist, and given the rash of breakouts reported so far this summer, my reading material has been plentiful. Thus, I offer you seven facts and a mystery about this nasty disease:

Sadly, you can get HFMD more than once. Most of the time, the culprit is either coxsackievirus A16 or enterovirus 71. But other viruses can cause HFMD misery, too. Coxsackievirus A6 can wreak havoc on children with its high fever and unusual rash. During outbreaks of coxsackievirus A6, “we saw a lot more kids with blisters than what was usually expected,” says Eileen Schneider, a medical epidemiologist at the Centers for Disease Control and Prevention in Atlanta.

And a new subgroup of enterovirus 71 has been spotted in mainland China and Hong Kong. Viral variety means that having HFMD once doesn’t mean you’ll never get it again. Depending on the similarities of the viruses, a previous exposure might help your body fight off the new infection, but it’s no guarantee.

Like lots of other viruses, enteroviruses like it warm. That means that infections are more common in the south. In cooler climes, infections show a more cyclical pattern, peaking in summer and early fall.

An HFMD vaccine against enterovirus 71 is in the works. Inactivated forms of the virus showed strong protection in a study of thousands of Chinese children. The vaccine won’t protect against HFMD caused by coxsackieviruses, though.

These highly contagious infections can spread through secretions such as saliva and feces. Those bodily effluents are loaded with virus because it replicates in the mouth, throat and GI tract. Blisters can contain viral particles, but oozing vesicles aren’t thought to be a major source of transmission, says Tom Solomon of the University of Liverpool in England.

Don’t be completely surprised if fingernails fall off post-infection. It can happen.

The viruses stick around in the body for a long, long time. Coxsackievirus A16 has been found in the stool of infected children for six weeks. Enterovirus 71 can last for 10 weeks. Contagion diminishes during this time, and people argue about just how long children ought to be sidelined. Different doctors have different answers, as I’ve learned in my completely non-scientific poll of local pediatrician responses. Our pediatrician told us that Baby V could safely return to society about two days after her fever was gone. Another friend’s pediatrician said that the blisters all need to be gone before the kid went out in public. And yet another friend heard that contagion is gone as soon as the fever lifts. “There’s no magic number” of days that people are contagious, says Schneider. The best thing to do is to look at each child’s symptoms and use your best judgment.

If you’ve ever driven north on the 87 out of New York City, you’ve probably passed Coxsackie, the upstate New York town that provided the fecal samples that harbored the newly discovered coxsackievirus.

And finally, a puzzler: The reason that the viruses cause mayhem on hands, feet and mouth — and not elsewhere in the body — remains a mystery, at least to me and the virologists and pediatricians I asked. “I don’t think anybody knows for sure,” Solomon says. It’s possible that lesions in the mouth and throat might enable the virus to spread more effectively through a cough, Solomon says, but hand and feet blisters “might just be a quirk of the virus.” I’m eagerly awaiting answers, so please chime in if you have a guess. I’ll update this post if someone can solve the mystery.
===============
Here’s how dust mites give dermatitis sufferers the itch
House dust mites surround us. Burrowing cheerfully into our pillowcases, rugs and furniture, the mites feast on our dead skin cells, breaking them down into small particles they can digest.

Now that your skin is crawling, relax. If you’re like most people, you will never know they are there.

An unlucky minority, however, is very aware of dust mites. Some of these unfortunate folks have a simple dust allergy. But others have an additional condition called atopic dermatitis, often referred to as eczema. They react to the presence of dust — or rather, dust mites — with hideous itching and redness. It wasn’t totally clear what, exactly, caused people with dermatitis to react so badly to dust mites.

It turns out that these people react not to the dust mite, but to its dinner — to the breakdown products of the person’s own skin. The finding helps explain why people with atopic dermatitis react so badly to dust mites, and it provides several new options to help treat the itch. It also resolves a decade-long debate in dermatology — why people with dermatitis are scratching in the first place.

Inside out vs. outside in

Atopic dermatitis is known for producing red, cracked and dry skin and, of course, the itching. People usually get diagnosed in childhood. Sometimes it goes away as kids get older, but it still affects between 9 and 30 percent of adults in the United States. Patients with dermatitis who react to dust are told to avoid dusty places and use special pillowcases. For the worst outbreaks, they are often prescribed a steroid cream. In some cases, they can end up in the hospital.

But what causes the itch in the first place? For the past 10 years, scientists have been scratching away at two hypotheses — one called “inside out,” and the other called “outside in.”

The “inside out” hypothesis came first, explains Graham Ogg, a dermatologist with the Medical Research Council Human Immunology Unit at the University of Oxford. The idea was that the immune system was overreacting to normal things: Dermatitis was an inside problem with the immune system itself.

In 2006, however, researchers reported in Nature Genetics that deficiencies in a protein called filaggrin were associated with atopic dermatitis. Now, it’s estimated that 20 to 30 percent of people with atopic dermatitis are also deficient in filaggrin, a protein in the outermost layer of the skin.

“It’s important for moisturizing the skin, keeping the skin hydrated,” explains Ogg. If people with dermatitis are deficient in filaggrin, then “the primary problem isn’t the immune system, it’s the barrier function in the skin.” If the barrier breaks down, more irritants can get in, prompting the immune response and the intolerable itch. So, the “outside in” hypothesis was born. In this view, the immune system wasn’t overreacting; instead it was reacting properly to the avalanche of aggravations it was faced with.

But what if these two hypotheses weren’t at odds, Ogg wondered, and instead were two sides of the same coin? To find out, Ogg and his group began by looking at a molecule called CD1a. This molecule is produced in the skin, and specializes in presenting bits of foreign matter to T cells — the immune system responders that mount attacks against foreign invaders.

It turns out that the CD1a molecules responded to extract-of-house-dust-mite — the delightful concoction that people get scratched with when they are tested for a dust allergy. And when they react, it’s because of CD1a molecules.

To find out if people with dermatitis had more CD1a than people without the condition, the scientists used suction to give eczema sufferers and healthy volunteers large blisters on their arms. The blisters were harvested for their skin and blood cells. And in patients with atopic dermatitis, those skin and blood cells were stuffed with CD1a, far more than in healthy controls.

But what was the CD1a reacting to? Usually CD1a senses fat molecules, presenting bits of them to the immune system to prep it for attack. Ogg and his group assumed that if they analyzed house dust mites, they would find the lipid or fat responsible.

Not quite. Instead, they found a protein called phospholipase A2. Phospholipase is an enzyme that dust mites produce that breaks down skin cells, producing fat molecules the mites can digest. CD1a, it turns out, responds to those lipids — reacting to the house dust mite’s dinner. Reacting, really, to the breakdown products of human skin.

This seems like support for the “inside out” hypothesis. CD1a is part of the immune system, and the immune system does seem to be over-reacting.

Filaggrin also had a role to play. The protein doesn’t just create a barrier to keep the skin moisturized — it’s also anti-inflammatory, Ogg’s group showed. If a skin sample was challenged with essence of dust mite, adding filaggrin could damp down the immune response. But eczema patients with low or no filaggrin had no defense. Their skin was more permeable, and there was nothing to stop the inflammation. The “outside in” hypothesis —the idea that the barrier function is the broken part of the system – is true too. Ogg and his colleagues report their findings February 10 in Science Translational Medicine.

“It links together the observations very nicely,” says Muzalifah Haniffa, a dermatologist at Newcastle University in England. It never was a matter of “inside out” or “outside in.” The two are inextricably linked.

Eat like a dust mite, sting like a bee?

So, to recap: As dust mites chow down on human skin, they cause damage to the cells. People with dermatitis have immune systems that detect the products of the damage and react, causing itching and pain. Filaggrin, when present, can tamp down the response. But when absent, nothing stops the itch.

The study shows filaggrin is far more than a simple barrier protein. Instead it directly affects immune responses in the skin, something that’s never been seen before, Haniffa notes.

This isn’t the first time that Ogg’s group has come across phospholipase A2. “Bee venom also contains phospholipase. In fact it contains massive amounts,” Ogg explains. Knowing that bee venom and dust mites have something in common helps scientists to understand one of the ways that the immune system senses damage to skin — and gives them another option to consider for treatment.

Right now, clinical trials are focused on stopping the inflammatory proteins produced further down the line. But, Haniffa says, scientists might try methods to increase the amount of filaggrin in the skin — beefing up the barrier against dust mite incursions and reducing the immune response at the same time. Other drugs or creams could target phospholipase A2, inactivating it. Without phospholipase, dust mites wouldn’t be able to break down skin cells, halting any immune reaction.

And that means we can hope for a new day. One with, hopefully, no itch.
===============
40 years after the first IVF baby, a look back at the birth of a new era
At 11:47 p.m. on July 25, 1978, a baby girl was born by cesarean section at the Royal Oldham Hospital in England. This part of her arrival was much like many other babies’ births: 10 fingers and 10 toes, 5 pounds, 12 ounces of screaming, perfect newborn. Her parents named her Louise. But this isn’t the most interesting part about Louise’s origins. For that, you have to go back to November 12, 1977, also near midnight. That’s when Louise Joy Brown was conceived in a petri dish.



Louise was the first baby born as a result of in vitro fertilization, or IVF, a procedure that unites sperm and egg outside of the body. Her birth was heralded around the world, with headlines declaring that the first test-tube baby had been born. The announcement was met with excitement from some, fear and hostility from others. But one thing was certain: This was truly the beginning of a new era in how babies are created.

To celebrate Louise’s 40th birthday, I took at look at IVF’s origins, its present form and its future. IVF’s story starts around 1890, when scientist Walter Heape transferred a fertilized egg from an Angora rabbit into a different breed, and saw that Angora bunnies resulted.

Scientists soon began to work on other animals before turning eventually to humans. A fascinating account of the early days, written by IVF pioneer Simon Fishel in the July issue of Fertility and Sterility, recounts some of the more lively — and shocking — aspects of the nascent field. For example, IVF researcher Robert Edwards, who won a 2010 Nobel Prize for his work, used to carry eggs between labs in Oldham and Cambridge in a container strapped to his body. And some of the early experiments involved inseminating the eggs with the researchers’ own sperm. There was a steep learning curve that led to many failures: More than 300 women had oocytes, or egg cells, removed without success before Louise was conceived.

Bu then things turned around. On November 9, Lesley Brown began to ovulate (naturally, since the researchers hadn’t had success using hormones to stimulate ovulation in many women). The next day, researchers saw that her left ovary contained a single follicle, the structure that holds an oocyte. Along with the surrounding fluid, that follicle was aspirated and carried by a nurse to another researcher and then finally to Edwards, who was waiting at a microscope. The egg was fertilized with sperm and allowed to mature into an 8-cell embryo. At midnight on the 12th, it was ready for the fateful transfer back to Lesley.

From there, the research took off, often with dicey funding and public outcry. Along with colleague Patrick Steptoe, Edwards and other pioneers opened the first private IVF clinic in 1980. Today, clinics exist worldwide. That brings us to more modern numbers. In 2016 in the United States, an estimated 76,930 babies were born via assisted reproductive technologies. The vast majority of those babies were born via IVF. Over the past decade, assisted reproductive technology birth rates have doubled over the past decade, the CDC estimates. Today, about 1.7 percent of all babies born in the United States each year are conceived via the technology. Worldwide, millions of babies have been born with IVF.

The method has been hugely successful in helping families who otherwise wouldn’t be able to have children. And overall, the procedure has a good safety record. A study of Israeli teenagers born via IVF, for instance, didn’t turn up any problems when the teens were compared with those conceived the old-fashioned way. The teenagers all had comparable mental health, physical health and brainpower, researchers reported in 2017 in Fertility and Sterility.

But that doesn’t mean the technology will stay in its current form forever. Evolving biological capabilities might one day lead to better genetic screenings of embryos before they are implanted. And genetic tweaks might one day be possible, given the rapid rise of gene editing technology. Already, scientists have repaired a gene related to a heart defect in human embryos.

Other improvements might come too, such as making it easier on women to produce eggs for extraction. Less extreme hormone regimens might one day become more standard. With advances in stem cell technology, eggs may no longer be needed at all. Scientists may one day be able to coax skin cells into gametes. Scientists have already turned mouse skin cells into eggs and combined them with sperm to produce pups.

As I mull over the past and present of IVF, I’m amazed at how much progress has been made, both in labs and in clinics, and I suspect that the most exciting advances are yet to come. I also think about all of these well-loved babies, born to families destined to treasure them for the masterpieces of biology that they are.
===============
There are benefits to prenatal yoga, but lingering questions remain
Pregnant women are on the receiving end of a long to-do list when it comes to maintaining their own health and that of their fetus. Don’t lift too much, eat this, drink that, lie or sit this way for too long. Exercise is on that list of orders, too. Pregnant women without certain complications are encouraged to exercise, but anyone watching their midsection slowly obscure their toes will tell you that the types of exercise you can and want to do winnow as pregnancy progresses.

During my first pregnancy, I got tired of just walking. Without a gym membership, I wasn’t keen on water aerobics. Lifting weights was never my thing. So midway through I found myself attending a prenatal yoga class. Immediately I felt I was on to something good: My brain and body felt better after the breathing exercises, meditation and stretches that are tailored for pregnant women.

I had some reservations about a few of the poses: Was it OK to lie on my back or do a downward-facing dog? There’s still some debate over whether it’s safe for pregnant women to do these things, and, at the time — in 2015 — studies on prenatal yoga were just starting to emerge in some number.

A little digging into the research shows that most pregnant women would be wise to consider prenatal yoga. The studies find numerous and varied benefits. But researchers caution that more studies that meet more rigorous standards need to be conducted.

One finding, not surprisingly, is that the practice can lessen physical pain, including in the lower back, a particularly hard-hit area of the body during pregnancy. Multiple studies have also shown that prenatal yoga may help alleviate mood disorders such as anxiety and depression. That’s an important finding, considering that close to a quarter of women may experience depression during pregnancy.

But the content of a prenatal yoga class seems to make a difference when it comes to depression, reported a 2015 review of studies on the subject (a study of studies, if you will). It found that pregnant women with depression who practiced prenatal yoga experienced a significant decrease in depression levels only when their practice included breath work, meditation or deep relaxation in addition to the physical exercise of performing poses.

And a pregnant woman’s stress levels and immune system may also benefit from prenatal yoga. A recent study looked at practitioners’ levels of cortisol, an indicator of stress, and immunoglobulin A, an indicator of immune function, from 16 to 36 weeks gestation, and found both measurements to be better (lower and higher, respectively) than those in pregnant women receiving only routine prenatal care during the same time period.

Reducing pain and stress and boosting immunity are big benefits for women who are restricted in which medications they can take for at least nine months.

Recently a research team in Iran reported that prenatal yoga can reduce the intensity of labor pain as well as the likelihood of labor induction and Cesarean section. For instance, at one point during labor, at least four hours in, the mean self-reported pain-intensity score of the women who hadn’t practiced prenatal yoga was twice as high as the mean score of those women who did.

Practitioners of the study’s prenatal yoga program began in the 26th week of pregnancy and continued through the 37th week. And they practiced a lot: Their regimen included three 60-minute supervised classes a week. In addition to poses, the classes included breath work, chanting, deep relaxation and other aspects often referred to as part of an “integrated” style of yoga.

For a different study, researchers asked pregnant women to perform 26 poses while monitoring their and their fetuses’ heart rates, among other vital signs. One of the poses tested, corpse pose, puts practitioners on their backs for a few minutes. The American College of Obstetricians and Gynecologists advises avoiding standing still or lying on your back for long periods of time, so the pose is sometimes questioned. So, too, is the popular downward-facing dog, also included in the study, because it’s a type of inversion.

But the study found no adverse effects on the women or fetuses for any of the poses. The study has some limitations, though. Its size is small — only 25 participants — and the average BMI of the participants, from the United States, was healthier than that of the average American pregnant woman. Still, the preliminary results are encouraging.

There are some general caveats to consider if you’re pregnant and interested in prenatal yoga. ACOG says exercise isn’t safe for pregnant women with certain conditions, so a conversation with health care providers prior to any class is warranted for everyone. ACOG also recommends avoiding “hot yoga” classes, which could cause you to overheat. Another thing to keep in mind: Ease into stretches. Hormones make ligaments and muscles looser during pregnancy, and overstretching could cause problems. And, as you can probably guess, poses that put you on your belly aren’t OK, except for perhaps early on in pregnancy.

Despite all those qualifiers, you’ll still find me — now at age 40, pregnant with my second child and a toddler to run after — on my yoga mat twice a week.

Laura Sanders is away on maternity leave. Emily Krieger is a freelance writer, editor and fact-checker in Seattle.
===============
Finally, a plan on how to include pregnant women in clinical trials
Among the stark changes for a woman during pregnancy is what she sees when she opens the medicine cabinet. The medications she wouldn’t have given a second thought to months earlier may now prompt worry and doubt. With any drug on the shelf, she may wonder: Is this medicine safe? Do I need to adjust the dose? Avoid it altogether? An expectant mom with just a cold or a headache will find drug labels suggesting she ask a health professional before use.

Turns out, those health professionals are in the same boat as their pregnant patients: There is very little, if any, information about whether many drugs are safe to give to expectant mothers. And that can lead to situations that put fear of harm to the fetus over the health of the mother.

Case in point: As a medical resident, bioethicist and obstetrician/gynecologist Anne Lyerly got a call from a friend and fellow resident working at a different hospital. The chief resident there was trying to resuscitate a dying woman who was pregnant, and instructed Lyerly’s friend to find out what drugs were safe to use. “I said, ‘you need to tell your chief resident that he needs to go save his patient’s life,’” Lyerly recalls, and not worry about possible harms from the drugs required to do so.

The widespread reluctance to provide medication to pregnant women stems from a dearth of data on treating illnesses with drugs during pregnancy. But because pregnant women have been generally excluded from clinical trials that study drugs, there isn’t much data to be had.

Now there’s a knock on the door that has been largely closed to this research. In April, the U.S. Federal Drug Administration released a draft of guidance on when and how to include pregnant women in clinical trials for drugs and therapies. It addresses considerations such as the effect pregnancy has on the absorption of drugs, nonclinical studies that should be conducted, and appropriate data collection and safety monitoring.

The key concern with pregnant women participating in clinical trials is safety of the fetus. The terrible birth defects that resulted from the wide use of the sedative thalidomide in the 1950s and ’60s weighed heavily on the eventual decision to largely exclude pregnant women from trials that test drugs. But that tragedy didn’t happen because pregnant women were studied, Lyerly says — it was because they weren’t studied.

“If you don’t study a drug in a highly controlled research setting,” Lyerly says, “it’s not like the risk that would be imposed on those individuals goes away.” Instead, the risk gets shifted to women who need the drug or women who get pregnant while on the drug. “If you are not going to conduct these research studies, it’s not like you’re off the hook, morally speaking, at all. It means the risk is going to be put somewhere else.”

The lack of clinical data doesn’t mean pregnant women aren’t using medications. A 2011 study found that 94 percent of more than 25,000 pregnant women had taken at least one over-the-counter or prescription medication during their pregnancy. The same study reported the average number of medications used at any time during pregnancy increased from 2.5 in 1976-1978 to 4.2 in 2006-2008. Looking only at prescription medications, 70 percent of pregnant women had used at least one.

But for 172 drugs approved by the FDA from 2000 to 2010, a different 2011 study found that there wasn’t enough data to determine the risk of harm to the fetus for 168 — or 98 percent — of the drugs.

“There is widespread exposure, and people want to know if the drug they were taking is safe,” Lyerly, of the University of North Carolina at Chapel Hill, says.

Although there are unknowns regarding drug treatment, there are diseases that, if untreated, can harm a pregnancy. Women who have asthma attacks during pregnancy are three times as likely to have babies with a low birth weight compared with asthmatic women who don’t have attacks, yet undertreatment of the condition is common, researchers report. Pregnant women with uncontrolled diabetes have more than four times the risk of late miscarriages or stillbirths as nondiabetic pregnant women. Treating the diabetes brings that risk down.

“Drugs can be extremely protective of fetal health by treating maternal health conditions,” Lyerly says.

Lyerly’s own research suggests that pregnant women are interested in participating in clinical research. She and colleagues published a small study in 2012 of 22 pregnant women enrolled in clinical trials to study dosing of an H1N1 influenza vaccine. The researchers interviewed the women about why they participated; the trials had filled quickly, Lyerly says.

The women’s reasons for enrolling ranged from worries about the risk of H1N1 and the benefit of receiving the vaccine early, to the preference for receiving the vaccine in a research setting with close medical monitoring, to the desire to be part of a study that would help other pregnant women. Lyerly just finished another study of 140 pregnant women in the U.S. and Malawi who have HIV or are at risk of the disease and their views about participating in research.

Lyerly argues that the issue of including pregnant women in clinical trials is not just about fetal safety, “it’s also about maternal benefit,” such as the benefit of a vaccine or a treatment for a risky illness. “Drug treatment is not the enemy,” she says, “it’s a tool in a tool chest that is important to ensuring healthy moms and healthy babies.”

The FDA guidance is a welcome signal of support for conducting research in pregnant women, Lyerly says. During the public comment period, which closes June 8, Lyerly and her colleagues plan to offer their views, especially regarding the criteria for including pregnant women in clinical studies. The goal is to encourage the progress of research in this area, she says, and the guidance “is an important beginning to the conversation.”
===============
When should babies sleep in their own rooms?
When we brought our first baby home from the hospital, our pediatrician advised us to have her sleep in our room. We put our tiny new roommate in a crib near our bed (though other containers that were flat, firm and free of blankets, pillows or stuffed animals would have worked, too).

The advice aims to reduce the risk of sleep-related deaths, including sudden infant death syndrome, or SIDS. Studies suggest that in their first year of life, babies who bunk with their parents (but not in the same bed) are less likely to die from SIDS than babies who sleep in their own room. The reasons aren’t clear, but scientists suspect it has to do with lighter sleep: Babies who sleep near parents might more readily wake themselves up and avoid the deep sleep that’s a risk factor for SIDS.

That’s an important reason to keep babies close. Room sharing also makes sense from a logistical standpoint. Middle of the night feedings and diaper changes are easier when there’s less distance between you and the babe.

But babies get older. They start snoring a little louder and eating less frequently, and it’s quite natural to wonder how long this room sharing should last. That’s a question without a great answer. In November 2016, the American Academy of Pediatrics task force on SIDS updated its sleep guidelines. The earlier recommendation was that babies ought to sleep in parents’ bedrooms for an entire year. The new suggestion softens that a bit to say infants should be there for “ideally for the first year of life, but at least for the first 6 months.”

Rachel Moon, a SIDS expert at the University of Virginia in Charlottesville who helped write the revised AAP guidelines, says that the update “gives parents a little more latitude after the first 6 months.” The vast majority of SIDS deaths happen in the first six months of life, but the studies that have found benefits for room sharing lumped together data from the entire first year. That makes it hard to say how protective room sharing is for babies between 6 and 12 months of age.

But a new study raises a reason why babies ought to get evicted before their first birthday: They may get more sleep at night in their own rooms. Babies who were sleeping in their own rooms at ages 4 or 9 months got more nighttime sleep than babies the same ages who roomed with parents, researchers reported online June 5 in Pediatrics.

The team asked hundreds of mothers to take sleep surveys when their children were 4, 9, 12 and 30 months old. Some of the 230 children slept in their own rooms when they were younger than 4 months, others moved to their own rooms between 4 and 9 months, and the rest were still sharing their parents’ rooms at 9 months.

At 9 months, babies who had been sleeping alone since 4 months of age slept an average of 40 minutes more than room sharers. The researchers found no differences in sleep duration between the groups of babies at age 12 months. By 30 months of age, though, children who had been sleeping in their own rooms by either 4 or 9 months of age slept on average 45 minutes longer at night than children who had been sharing their parents’ rooms at 9 months. (Important caveat: At 30 months, total daily sleep time didn’t differ between the groups. The former room sharers were making up for missed nighttime sleep with naps.)

Parents who want their babies age 6 months and older to sleep in their own room ought to be encouraged to make the move, says study coauthor Ian Paul, a pediatrician at Penn State. “The guidelines should reflect data, not opinion,” Paul says.

He suspects that sharing a bedroom with babies interferes with everyone’s sleep because normal nocturnal rustlings turn into full-blown wake-ups. Babies and adults alike experience brief arousals during sleep. But when parents are right next to babies, they’re more likely to respond to their children’s brief arousals, which then wakes the baby up more. “This then sets up the expectation from the baby that these arousals will be met with a parent reaction, causing a bad cycle to develop,” he says.

There was another difference that turned up between the two groups of babies. Babies who roomed with parents were four times more likely to be moved into their parents’ beds at some point during the night than babies who slept in their own rooms. Bed sharing is a big risk factor for sleep-related infant deaths.

But Moon cautions that the Pediatrics study is preliminary, and doesn’t warrant a change in the AAP guidelines. She and coauthors point out in an accompanying commentary that other factors might be behind the difference in sleep between the two groups of babies. For instance, babies who slept in their own room were more likely to have consistent bedtime routines, be put to bed drowsy but awake, and have bedtimes of 8 p.m. or earlier. Those are all signs of good “sleep hygiene” for babies, and might be contributing to the longer sleep times. “We know that consistent bedtime routine and consistent bedtime are very important in terms of sleep quality in children,” Moon says. “They could very well make a difference.”

So that’s where we are. Some things are clear, like putting your baby to sleep on her back on a flat, firm surface clear of objects and having your baby nearby during the first six months. But other decisions come with skimpier science, and whether to evict your 6-month-old is one of them. Because science can take you only so far, it may just come down to the snoring, stirring and sleep deprivation.
===============
Sugar doesn’t make kids hyper, and other parenting myths
Baby shoes didn’t feature prominently into Baby V’s wardrobe for quite some time. Tiny Chuck Taylors are adorable, obviously, but I questioned their utility for a baby who didn’t use her feet except as wiggly pacifiers. So Baby V spent a lot of time barefoot — a fashion statement that I didn’t really consider until she started toddling around in public.

Well-meaning observers were quick to tell me that I needed to get that baby some nice stiff shoes. Hard soles will help her get the hang of walking and protect her delicate baby feet, I was told. But when I started looking into this advice, I actually found the opposite is true: These days, people recommend that babies learning to walk wear soft, flexible shoes, or better yet, go barefoot. The minimalist footwear allows the nascent walkers the most sensory feedback from their sweet little feet as they move across the earth.

I offer the shoe advice as just one tiny glimpse into the life of a parent of a young kid. Over the last year, I’ve come to learn that much of the advice I’ve heard, while well-intentioned, might just be wrong. Or at the very least, questionable. So here are my top five parenting myths (shoes didn’t make the cut), with a little dash of science.

1. Sugar makes kids hyper.

Lots of parents swear that a single hit of birthday cake holds the power to morph their well-behaved, polite youngster into a sticky hot mess that careens around a room while emitting eardrum-piercing shrieks. Anyone who has had the pleasure to attend a 5-year-old’s birthday party knows that the hypothesis sounds reasonable, except that science has found that it’s not true.

Sugar doesn’t change kids’ behavior, a double-blind research study found way back in 1994. A sugary diet didn’t affect behavior or cognitive skills, the researchers report. Sugar does change one important thing, though: parents’ expectations. After hearing that their children had just consumed a big sugar fix, parents were more likely to say their child was hyperactive, even when the big sugar fix was a placebo, another study found.

Of course, there are plenty of good reasons not to feed your kids a bunch of sugar, but fear of a little crazed sugar monster isn’t one of them.

2. Listening to Mozart makes babies smarter.

My colleague Rachel Ehrenberg busted this “Mozart Effect” myth in her 2010 feature. The original observation, that 10 minutes of classical music made college students briefly perform better on a paper-folding task, was twisted so out of context that the governor of Georgia used tax money to buy a classical music CD for every baby born in the state.

Many babies adore music, and there’s evidence that suggests music might help soothe babies. There’s also evidence that playing an instrument might be beneficial to brain development, as Ehrenberg points out. But scientists haven’t found that classical music makes your baby smarter. So play music to your child because she loves it and you love it, not because you’re looking to grub a few extra IQ points.

3. Feeding a baby solid food will help her sleep through the night.

Your baby is waking up in the night? Just put some rice cereal in the last bottle before bed, well-intentioned observers urge. The solid food will fill baby’s tummy and keep her satisfied longer, which translates to fewer wakeups. Except that it doesn’t.



Babies fed rice cereal before bedtime slept no better than babies fed only breast milk or formula, a study found. In fact, early introduction to solid food (before 4 months) has been associated with worse infant sleep. The magical cure of feeding a baby solids before bedtime belongs at the top of the heaping pile of sleep miracles that sound great but don’t really work. And speaking of rice cereal…

4. Rice cereal is the ideal first food for babies.

Lots of people assumed that Baby V would have bland rice cereal when she was ready to try solid food around four months. But our doctor assured us that there’s no reason to choose it over delectable fruits or veggies such as bananas or avocados. Other foods taste way better than starchy rice cereal, and have some good nutrients and fat, too. Baby V’s doctor urged us to be adventurous with those first tastes and venture beyond the cereal.

For most babies, there’s no reason to wait a long time to introduce foods considered to be at high risk of causing an allergic reaction, the American Academy of Pediatrics now believes. So gradually start rotating in fish, eggs, yogurt and peanut butter anytime after your little one starts eating solids.

5. Sippy cups cause speech problems.

Some innocent Googling about which cup I ought to buy landed me on some awfully frightening websites that urged me to forego the sippy cup. The reasoning goes that sippy cups encourage immature mouth and tongue movements, which stunts the development of muscles needed for clear speech.

The problem is that I haven’t found any studies that back that claim up. (As always, please let me know if I’ve missed something.) Sippy cups can contribute to cavities if babies are allowed to constantly drink milk or juice from them, and sippy cups can injure children if they fall while drinking, but I haven’t seen any data to make the case that moderate sippy cup usage get in the way of normal speech.

So there you have it: My completely non-comprehensive top five list of parenting myths. Please chime in below with your own favorite gems. I’m already working on my next list and would love to hear your ideas.
===============
The planet-hunting Kepler space telescope is dead
NASA’s premier planet-hunting space telescope is out of gas.

The Kepler space telescope can no longer search for planets orbiting other stars, ending the 9½-year mission, officials from the agency announced in a news conference on October 30.

“Because of fuel exhaustion, the Kepler spacecraft has reached the end of its service life,” said Charlie Sobeck, a project system engineer at NASA’s Ames Research Center in Moffett Field, Calif. “While this is a sad event, we are by no means unhappy with this remarkable machine.”

Kepler’s discoveries have forever changed the way astronomers think about planets in other solar systems. Before the spacecraft launched in 2009, only about 350 exoplanets were known to exist in the galaxy, and nearly all of them were the size of Jupiter or larger.

As of October 30, there are more than 3,800 known exoplanets, and Kepler was responsible for discovering 2,720 of them. The spacecraft found planets in all shapes, sizes and configurations: seven planets orbiting one star, planets orbiting at jaunty angles, planets with two suns, planets more than twice as old as Earth. “These planets formed at the beginning of the formation of our galaxy,” says astronomer William Borucki, who was Kepler’s principal investigator until he retired in 2015. “Imagine what life might be like on such planets.”

What’s more, astronomers have used Kepler’s exoplanet haul to predict that every one of the hundreds of billions of stars in the Milky Way should have at least one planet, on average (SN: 2/25/12, p. 12). And billions of those exoplanets’ sizes and temperatures, scientists think, might make them friendly to life (SN: 11/30/13, p. 13).

Planets, planets everywhere The Kepler space telescope launched in 2009. By the end of 2017, it had discovered more than 2,500 planets (yellow), about 70 percent of all exoplanets known. Known exoplanets range in sizes, from those comparable with Earth to the sizes of Neptune or Jupiter or even larger. Exoplanet discoveries

Kepler was declared dead once before. In 2013, the telescope lost the use of a second of its four reaction wheels, which helped keep the telescope pointed steadily at the same patch of the sky. That consistent pointing was crucial for Kepler’s planet-hunting strategy: spotting a slight dip in stars’ light as planets cross in front of them. It seemed like the mission was over (SN: 6/15/13, p. 10).

But engineers soon revived the telescope in a new observing mode, called K2 (SN: 6/28/14, p. 7), which used the pressure of sunlight on Kepler’s solar panels to keep it pointing straight. “I always felt like it was the little spacecraft that could,” said astronomer Jessie Dotson, a Kepler project scientist at NASA Ames. “It always did everything we asked of it, and sometimes more. That’s a great thing to have in a spacecraft.”

Kepler’s official end came two weeks ago, when the telescope’s fuel pressure dropped by 75 percent in a matter of hours, Sobeck said. Before it shut down, Kepler transmitted all of its remaining data back to Earth. “In the end, we didn’t have a drop of fuel left,” he said.

The spacecraft’s legacy lives on. The next planet-hunting telescope, TESS, or the Transiting Exoplanet Survey Satellite, launched in April and has already spotted some exoplanets (SN: 5/12/18, p. 7).

For its final acts, the Kepler team will remotely turn off the telescope’s radio transmitters to avoid contaminating airwaves, and turn off protective systems that would turn those transmitters back on.

“The spacecraft will then be left on its own to drift away in a safe and stable orbit around the sun,” Sobeck said.
===============
Artificial intelligence crowdsources data to speed up drug discovery
A new cryptographic system could allow pharmaceutical companies and academic labs to work together to develop new medications more quickly — without revealing any confidential data to their competitors.

The centerpiece of this computing system is an artificial intelligence program known as a neural network. The AI studies information about which drugs interact with various proteins in the human body to predict new drug-protein interactions.

More training data beget a smarter AI, which was a challenge in the past because drug developers generally don’t share data due to intellectual property concerns. The new system allows an AI to crowdsource data while keeping that information private, which could encourage partnerships for speedier drug development, researchers report in the Oct. 19 Science.

Identifying new drug-protein interactions can uncover potential new treatments for various diseases. Or it could reveal whether drugs interact with unintended protein targets, which might indicate if a medication is likely to cause particular side effects, says Ivet Bahar, a computational biologist at the University of Pittsburgh not involved in the work.

In the new AI-training system, data pooled from research groups get divvied up among multiple servers, and the owner of each server sees what appear to be only random numbers. “That’s where the crypto-magic happens,” says computer scientist David Wu of the University of Virginia in Charlottesville, who wasn’t involved in the work. Although no individual participant can see the millions of drug-protein interactions that compose the training set, the servers can collectively use that information to teach a neural network to predict the interactivity of previously unseen drug-protein combinations.

“This work is visionary,” says Jian Peng, a computer scientist at the University of Illinois at Urbana-Champaign not involved in the study. “I think [it] will lay the groundwork for the future of collaborations in biomedicine.”

MIT computational biologist Bonnie Berger and colleagues Brian Hie and Hyunghoon Cho evaluated their system’s accuracy by training a neural network on about 1.4 million drug-protein pairs. Half of these pairs were drawn from the STITCH database of known drug-protein interactions; the other half comprised drug-protein pairs that don’t interact. When shown new drug-protein pairs known to interact or not, the AI picked out which sets interacted with 95 percent accuracy.

To test whether the system could identify hitherto unknown drug-protein interactions, Berger’s team then trained the neural network on nearly 2 million drug-protein pairs: the entire STITCH dataset of known interactions, plus the same number of noninteracting pairs. The fully trained AI suggested several interactions that had never before been reported or that had been reported but were not in the STITCH database.

For instance, the AI identified an interaction between estrogen receptor proteins and a drug developed to treat breast cancer called droloxifene. The neural network also found a never-before-seen interaction between the leukemia medication imatinib and the protein ErbB4, which is thought to be involved in different types of cancer. The researchers confirmed this interaction with lab experiments.

This secure computing network may also encourage more collaboration in areas outside of pharmaceutical development. Hospitals could share confidential health records to train AI programs that predict patient prognoses or devise treatment strategies, Peng says.

“Whenever you want to do a study on a large number of people on behavior, on genomics, on medical records, legal records, financial records — anything that’s privacy-sensitive, these kinds of techniques can be very useful,” Wu says.
===============
Medicine Nobel honors cancer therapies that unleash immune system
Stopping cancer by removing brakes on the immune system has earned James P. Allison of the University of Texas MD Anderson Cancer Center in Houston and Tasuku Honjo of Kyoto University in Japan the Nobel Prize in physiology or medicine.

“Allison’s and Honjo’s discoveries have added a new pillar in cancer therapy,” Nobel committee member Klas Kärre said in an Oct. 1 news conference announcing the prize. “It’s a new principle.”

Other therapies, such as surgery, radiation and chemotherapy, target tumor cells themselves. The two laureates’ strategy was to persuade the patient’s own immune system to go after the cancer (SN: 7/11/15, p. 14). “The seminal discoveries by the two laureates constitutes a paradigmatic shift and a landmark in the fight against cancer,” Kärre said.

The newly minted laureates will equally share the prize of 9 million kronor, equivalent to just over $1 million.

Both men have made substantial contributions to basic research in immunology beyond their work in cancer, says Norman “Ned” Sharpless, director of the National Cancer Institute in Bethesda, Md. The Nobel Assembly at the Karolinska Institute in Stockholm chose to honor the men’s achievement in cancer immunotherapy, but they “also deserve lifetime achievement awards for their contributions to science,” Sharpless says.

Immune cell mob A patient with non-small cell lung cancer received a treatment targeting the T cell protein PD-1, unleashing the cells to fight the cancer. At two months, the T cells have infiltrated the tumor (red arrows), making it appear bigger. But at four months, the immune cells’ continued attack has shrunk the tumor.

The prize honors Allison’s and Honjo’s work on proteins that help the immune system recognize invading organisms and cancer cells, turning those proteins into therapeutic targets. Allison discovered that CTLA-4, a protein on the surface of immune cells called T cells, holds those cells back from attacking tumors. Allison’s lab developed an antibody against CTLA-4 to release the brake and allow T cells to kill tumor cells. In a series of experiments, Allison and colleagues cured mice of cancer by releasing the CTLA-4 brake.

The therapy proved especially effective against melanoma in people (SN: 9/25/10, p. 12). In 2011, the U.S. Food and Drug Administration approved ipilimumab, sold under the brand name Yervoy, to treat melanoma, and it is being tested against other cancer types. Although effective, the therapy can sometimes have serious side effects when the unfettered T cells attack other organs in the body.

Allison learned of the prize from his son, who called him in his hotel room in New York City, where he is attending a cancer research conference. Soon, friends were calling and coming to his hotel room to celebrate. “We had a little party in the room this morning,” he said during a news conference. He spoke to the Nobel committee later.

“I did not get into these studies to try to cure cancer. I got in to them because I wanted to know how T cells worked,” Allison said in a news conference. That basic knowledge was instrumental in developing this type of immune therapy. Other approaches such as cancer vaccines hadn’t been as successful, perhaps because “people started with insufficient knowledge,” he said.

Honjo discovered another brake on the surface of T cells called PD-1. Antibodies to block PD-1 have had even more dramatic effects than CTLA-4 blockers, even helping people with cancer that has spread (SN: 12/27/14, p. 8). Such spreading, or metastatic, cancers were previously untreatable. In 2014, the FDA approved the first “PD-1 blockade” antibodies. Now there are several antibodies against both PD-1 and its partner protein PD-L1 on tumor cells approved for use against melanoma, non-small cell lung cancer, kidney cancer, bladder cancer, head and neck cancers, and Hodgkin lymphoma. Those drugs include Keytruda and Tecentriq, among others. PD-1 blockade also has side effects, but they are generally milder than those caused by CTLA-4 blockers.

Free the cells Tasuku Honjo identified a protein, called PD-1 (teal Y), on the surface of T cells. When a tumor cell’s protein PD-L1 (light green) latches on to PD-1, it halts the T cell’s ability to fight the tumor (top). Honjo’s work led to the development of drugs (action shown in magenta) that block the proteins from interacting (bottom), freeing the T cells to attack.

“At the time of PD-1’s discovery in 1992, it was purely a matter of basic scientific research,” Honjo said at a news conference on October 1. “But as this then led to actual treatments and I then eventually began to hear from patients, such as, ‘This treatment has improved my condition and given me strength again, and it is all thanks to you,’ I really began to understand the meaning of what my work had accomplished.”

These brake-release therapies, known as immune checkpoint therapies, or checkpoint inhibition, have been a boon for cancer patients, says Sharpless. “We’re not curing everybody,” he says, “but in some cancers 20 to 30 percent of patients will have substantial benefit, whereas before we had nothing for those people.”

Some cancers, such as glioblastoma brain cancers and pancreatic cancer, don’t respond to checkpoint inhibition. But tumors with many mutations, such as melanoma — which may contain mutations in thousands of genes — are more likely to be attacked (SN: 7/8/17, p. 7), Allison said. Multiple mutations give T cells more targets for tracking down tumor cells.

Allison gave a shout-out to cancer patients. “We’re making progress,” he said. He wants to increase the number of people who can be helped by immunotherapy, possibly by combining checkpoint inhibitors with traditional radiation and chemotherapy approaches. “We know how to do it, we’ve just go to learn to do it better.”

Editor's note: This story was updated October 10, 2018, to correct when the FDA approved the first PD-1 blockade antibody. That approval was in 2014, not 2012, and for two antibodies, not one.
===============
Pregnant women’s use of opioids is on the rise
Pregnant women aren’t immune to the escalating opioid epidemic.

Data on hospital deliveries in 28 U.S. states shows the rate of opioid use among pregnant women has quadrupled, from 1.5 per 1,000 women in 1999 to 6.5 per 1,000 women in 2014, the U.S. Centers for Disease Control and Prevention reports.

The highest increases in opioid use among pregnant women were in Maine, New Mexico, Vermont and West Virginia, according to the CDC study, published online August 9 in Morbidity and Mortality Weekly Report.

“This analysis is a stark reminder that the U.S. opioid crisis is taking a tremendous toll on families,” says coauthor Jean Ko, a CDC epidemiologist in Atlanta.

In this first look at opioid use during pregnancy by state, Washington, D.C. had the lowest rate in 2014, at 0.7 per 1,000 women, and Vermont had the highest, at 48.6 per 1,000. However, the data from the U.S. Health and Human Services Department represents only the 28 states that record opioid use at childbirth during the studied time frame.

“We knew the incidence was increasing” as the number of babies going through opioid withdrawal has also gone up, says Matthew Grossman, a pediatrician at Yale University. Overall, the number of U.S. deaths attributed to opioids has also been steadily rising (SN: 3/31/18, p. 18). In 2014, there were 14.7 opioid deaths per 100,000 people, up from 6.2 per 100,000 in 2000, according to the CDC.

Taking opioids during pregnancy, especially in the last trimester, increases the risk of preterm birth and stillbirth, as well as infant opioid withdrawal (SN: 6/10/17, p. 16). Pregnant women should tell their doctors if they are taking opioids, so complications can be addressed, says Alison Holmes, a pediatrician at Dartmouth-Hitchcock Medical Center in Lebanon, N.H. Mothers may be prescribed methadone, a synthetic opioid which is safer for the fetus and protects it from going through withdrawal in the womb. “What’s not safe for the child is active opioid misuse,” she says.

Only eight U.S. states require that pregnant women be tested for opioids if substance abuse is suspected, the CDC says. In Cincinnati, all pregnant women are tested at delivery, but it would be even better to test women in the first trimester, says pediatrician Scott Wexelblatt at the Cincinnati Children’s Hospital. “If we could identify a mom at 12 weeks instead of 40 weeks, then we could get her into medicated assisted treatment.”

Editor's note: This story was updated August 9, 2018, to clarify opioid testing of pregnant women in Ohio.
===============
Virtual reality therapy has real-life benefits for some mental disorders
Edwin adjusted his headset and gripped the game controller in both hands. He swallowed hard. The man had good reason to be nervous. He was about to enter a virtual environment tailor-made to get his heart pumping way more than any action-packed video game: a coffee shop full of people.

Determined to overcome his persistent fear that other people want to hurt him, Edwin had enrolled in a study of a new virtual reality therapy. The research aimed to help people with paranoia become more comfortable in public places. In this program, described in March in the Lancet Psychiatry, Edwin could visit a store or board a crowded bus.

Virtual strangers can be scary, just like real people. Edwin, who had been diagnosed with paranoid schizophrenia, often found simple errands like grocery shopping overwhelming and exhausting.

But facing simulated crowds came with perks. At a nearby computer sat clinical psychologist Roos Pot-Kolder of VU University Amsterdam. She could customize the number of avatars and set their friendliness levels in each scene. That way, Edwin could progress at his own pace.

During one session, Pot-Kolder coached Edwin to challenge his own paranoid assumptions. If he saw an angry-looking avatar, she asked, “What could be other reasons for looking mad, besides wanting to hurt you?” Edwin offered: The person could be tired or having personal problems.

After three months of VR treatment, public outings were easier, said Edwin, who asked that his last name not be used. “I felt more freedom, more relaxed.” He even performed a poem for 500 people at a talent show, which he “would not have dared” before.

Researchers have been developing virtual reality systems that help people overcome specific phobias since the 1990s. VR therapy has since expanded to address more complex anxiety disorders, such as social anxiety and post-traumatic stress, and even the anxiety associated with paranoid schizophrenia for people like Edwin.

31 percent U.S. adults who experience an anxiety disorder at some point in life Source: National Institute of Mental Health

“The key ingredient to an effective treatment for anxiety disorders is … you need to face your fears,” says Stéphane Bouchard, a clinical cyberpsychologist at the University of Quebec in Outaouais, Canada. He’s referring to what’s known as exposure therapy. With emotional support from a therapist, exposure therapy helps desensitize the patient to whatever the fear is. Patients typically face their fears in real life or, if their fear is a traumatic memory, repeatedly relive the event in their imagination.

But confronting fears can be easier in a virtual setting. A flight-phobic patient can take off and land many times in a single VR session without the cost and hassle of actual flights. Veterans with post-traumatic stress who can’t remember a traumatic memory in great detail can reenact a close proxy in VR for a more potent therapeutic experience. The same goes for those who repress painful memories.

Until recently, the price and complexity of VR equipment, which could run tens of thousands of dollars, limited VR therapy to a few research labs and clinics. Now, there are computer-based headsets like the Oculus Rift that cost only a few hundred dollars, as well headsets such as the Samsung Gear VR that turn smartphones into virtual reality displays for about 100 bucks.

With cheaper, more user-friendly systems poised to make virtual reality therapy available to many more patients, researchers are testing the bounds of VR’s therapeutic powers to treat a broader range of disorders or, in some cases, replace the therapist altogether.

EXPOSURE THERAPY A VR system that helps people with paranoia get more comfortable in public places, such as a café or grocery store (shown in the clip above), uses avatars that can be made to look friendlier or more hostile, depending on a patient’s progress.

Courtesy of R. Pot-Kolder/VU Univ.

Real feel

The power of VR therapy comes from the fact that people automatically react to fear cues, even in an environment they consciously know isn’t real. That’s because the brain’s emotional command center, or limbic system, responds to stressors in a matter of milliseconds — way faster than logic can kick in (SN: 2/26/11, p. 22).

As a result, patients who confront their fears in VR have shown increased levels of the stress hormone cortisol, higher heart rate and higher skin conductivity, says Barbara Rothbaum, a clinical psychologist at Emory University in Atlanta. Those are all telltale signs of a fight-or-flight response (SN Online: 2/2/17).

Back when Rothbaum and colleagues began to study VR treatment for a psychological disorder in the early 1990s, the researchers weren’t sure that a computer simulation could provoke those reactions. But their VR program, which took height-phobic patients onto bridges, balconies and for a ride in a glass elevator, worked almost too well.

Rothbaum recalls the very first patient test. “We were so excited, and she was getting anxious. We just kept her going in it, and she ended up throwing up.” The patient, it turned out, was susceptible to motion sickness — a problem that still plagues VR (SN: 3/18/17, p. 24). “We thought that was going to be the end of the study right there.”

But Rothbaum’s team forged ahead. The group learned to give patients a break after about 40 minutes in VR, dial the thermostat down and warn nausea-prone patients not to move their heads so much. In that first study, reported in 1995 in the American Journal of Psychiatry, 10 participants showed a substantial decrease in fear of heights after seven weekly sessions of VR therapy compared with seven patients who received no therapy. Two decades on, studies have shown that VR treatments for specific phobias can soothe people’s fears about as well as real-life exposure.

Social stress Among patients with social anxiety, those treated with VR therapy dropped an average of 33 points on an anxiety scale of 0 to 144. Anxiety among patients receiving traditional therapy dropped about 19 points. These improvements held six months later. VR therapy eases anxiety Source: S. Bouchard et al/British Journal of Psychiatry 2017

More recently, researchers have designed and tested VR systems to help people with more nuanced and diverse fear triggers, such as social anxiety or obsessive-compulsive disorder. For social anxiety, Bouchard and colleagues tested a VR system that allowed patients to work through tense social situations, such as a job interview or declining to purchase something from a persistent store clerk. The researchers assigned 17 socially anxious people to VR therapy and another 22 to typical exposure treatment that involved exercises like talking to strangers in public. A third group assigned to a waiting list got no therapy.

Before and after 14 weekly therapy sessions, participants reported their fear and avoidance of social situations from 0 to 144, with higher scores indicating more severe anxiety. Starting scores averaged between 75 and 85. Participants who got VR treatment dropped an average of 33 points, whereas real-life exposure participants dropped an average of 19. The no-treatment group stayed about the same. These results, reported in the April 2017 British Journal of Psychiatry, suggest that VR is at least as effective as real-life exposure for social anxiety.

Helping patients with post-traumatic stress disorder confront their fears is often more complex than simulating a generic high-rise or spider. One system that provides a broad menu of fear cues to patients with PTSD, created by VR therapy developer Albert “Skip” Rizzo and colleagues at the University of Southern California in Los Angeles, helps people suffering from post-traumatic stress after military duty in Iraq and Afghanistan.

To relive a traumatic memory in this VR system, the patient first chooses the setting, like a roadway checkpoint or a hospital. As the patient narrates the memory aloud, the therapist customizes the scene. If “the patient is saying, ‘I’m driving down a roadway,’ the therapist sets it up,” Rizzo says. If the memory happened around noon, the therapist sets the virtual clock accordingly. If the patient recalls the rumble of a Humvee, “Rrrrrp. Crank up the vehicle sound.”

More to come Scientists are testing VR programs to help with a broadening list of challenges, such as: Depression: Learn to practice self-compassion Addiction: Reduce cravings for people in recovery Eating disorders: Improve body image Stroke: Help survivors relearn motor skills Medical procedures: Distract during painful procedures Wheelchair users: Practice getting around

Rizzo’s team tested an earlier version of the system by randomly assigning 162 military personnel to either a waiting list, 10 sessions of therapy that involved using the virtual Iraq/Afghanistan system or 10 sessions of traditional therapy. For traditional treatment, therapists coached patients through traumatic memories in their imaginations, and helped patients put themselves in everyday situations that they’d come to fear because of their trauma, such as crowded public places. Immediately after the study, both treatment groups showed substantial improvement in PTSD symptoms compared with those on the wait list, the researchers reported in November 2016 in the Journal of Consulting and Clinical Psychology.

“The real question is, if VR is as good as traditional therapies, which one should we do for which patients, and why?” says Greg Reger, a clinical psychologist at the University of Washington and VA Puget Sound Health Care System in Seattle.

By analyzing a subset of military personnel from the 2016 study, Reger and colleagues identified a few factors — like being younger and not taking antidepressants — that seemed to point to people who would fare better with VR. It makes sense that younger folks would be more responsive to tech-heavy treatment, but researchers have no idea why medication use would be relevant. Further investigations like Reger’s, reported in the June Depression and Anxiety, could help therapists decide when to pull out the headset.

Through the looking glass

To clinical psychologist Daniel Freeman of the University of Oxford, “the beauty of VR” is that it goes beyond rendering realistic experiences. “You can do stuff that you can’t do in real life.”

For instance, coaching a socially anxious patient through a conversation often involves redirecting that person’s attention away from themselves and toward their environment, he says. In VR, a therapist can direct a patient’s attention to particular aspects of the virtual world to help him or her forget their self-consciousness.

Gerard Jounghyun Kim, a computer scientist at Korea University in Seoul, and colleagues are testing a mix of real and fantastical elements to help people with panic disorder. In the researchers’ new VR program, a user can visit a potentially panic-inducing situation, like a parade or a crowded elevator. If a panic attack ensues, the user can hit an escape button and be transported to a peaceful beach. In that safe haven, patients get instructions to calm their breathing, while they hold a thumping device in their hand and see a virtual heart that pumps in time with their own.

Seeing, hearing and feeling the pulsation of this virtual heart is supposed to help patients focus their attention and recognize that they can get their heart rate under control, Kim says. Of five patients with panic disorder who tested the system, three reported that they found the heart-in-hand scenario helpful for recovering from panic.

This pilot trial, presented in 2017 in Gothenburg, Sweden, at the ACM Symposium on Virtual Reality Software and Technology, was much too small to show exactly how helpful this system might be for patients with panic disorder. Kim and colleagues are now doing a more comprehensive analysis with dozens of patients.

Face time

While Kim’s team is creating virtual versions of patients’ hearts, a group in Canada is rendering virtual bodies for the voices inside the minds of patients with schizophrenia.

Many people who take antipsychotic medication for schizophrenia continue hearing voices, says Alexandre Dumais, a psychiatrist at the University of Montreal. Traditionally, therapists advise patients to ignore these residual hallucinations, but recent research has shown that engaging the voices in conversation may actually help reduce patients’ sense of helplessness.

So Dumais’ team built a VR system in which a patient designs an avatar that embodies a bothersome hallucinatory voice. The therapist voices this avatar using patient-suggested sentences and gradually makes the avatar friendlier, encouraging the patient to get more comfortable and assertive in addressing the voice.

Dumais’ team tested this system, described in July in Schizophrenia Research, on 19 patients with schizophrenia. Four dropped out after the first session because they either didn’t like the program or found it too scary. The remaining 15 rated how scary they found each VR therapy session from 0 to 10, with 10 being the most distressing. Scores dropped over six weekly sessions. Moreover, at the end of treatment, the patients’ scores on a 0 to 20 scale measuring general hallucination-related distress dropped from an average of 16.1 to 10.9.

Head games Schizophrenia patients who heard voices and conversed with them in VR rated how scary and anxiety-provoking six weekly therapy sessions were. Over time, patients gradually found the VR experience less stressful. Schizophrenia stress drops in VR therapy Source: O. Percie du sert et al/Schizophrenia Research 2018

“We’re very much in the early days” of simulating impossible situations in VR for therapeutic purposes, Freeman says. But as VR becomes more pervasive, more researchers may have the opportunity to develop creative new treatments that exploit virtual unreality.

DIY therapy

Virtual avatars, good for filling simulated coffee shops, may also serve as therapists, transforming VR from a tool available only in a clinic to a new type of self-help. This may be especially useful for patients who are averse to visiting a therapist, such as people with social anxiety or agoraphobia, or for people living in remote areas without access to specialists.

The first fully automated virtual reality therapy, designed for fear of heights, was described in the Lancet Psychiatry in August (SN: 8/4/18, p. 15). In this program, an animated therapist guides a patient up a 10-story office complex. The user performs increasingly difficult tasks, from standing near a drop-off to going out on a platform over a central atrium. The virtual therapist periodically checks how the patient is feeling and offers encouragement. Freeman and colleagues tested this program on 100 patients: Forty nine were randomly assigned to two weeks of VR treatment; the other 51 got no treatment.

“I anticipated it was just going to be like a game,” one VR participant said, but the program “pushed the limits in terms of what I thought I would be able to achieve.”

On a scale measuring fear of heights from 16 to 80, the scores of people who used the VR program dropped, on average, about 25 points after treatment. The no-treatment group kept about the same scores as before. While results are encouraging, researchers don’t yet know how this program measures up to real-life therapy.

SUCH GREAT HEIGHTS In one fully automated VR therapy program, a virtual therapist helps users overcome their fear of heights by guiding them through a tall building with floors overlooking an atrium. Patients must complete challenges on each floor, including crossing a rope bridge.

Oxford VR

Another self-led treatment, this one to calm fear of spiders, has been tested against face-to-face therapy. The three-hour VR program involves various arachnids — a cartoonish, slipper-wearing spider to a realistic tarantula. The spiders approach the user while a virtual therapist offers instructions and encouragement.

“I’m not sure if anyone ripped the headset off, but a lot of people definitely started crying,” says Philip Lindner, a clinical psychologist at Stockholm University. One patient who was virtually sitting in a living room with a lot of spiders crawling around on the floor “physically put up her legs and sat like that for, like, 15 minutes.”

Researchers tested this system on 97 arachnophobia patients and described the results last November in San Diego at the Annual Association for Behavioral and Cognitive Therapies Convention. Half of the volunteers were randomly assigned to receive VR therapy and then encouraged to try approaching spiders in the real world. The other half completed a three-hour session of normal exposure therapy, where participants worked up from catching spiders in cups to holding a spider in each hand.

Before treatment, both sets of participants generally wouldn’t go near a spider in a clear container, Lindner says. After treatment, VR participants could stand next to or even put their hands inside the container, and real-world exposure patients could touch the spider. One year later, though, some VR patients could touch the spider too.

Lindner suspects that the VR experience reduced patients’ fears enough for them to try real-world exposure on their own, so they caught up with the normal exposure group.

Despite the early successes for specific phobias, it’s unclear whether therapist-free VR therapy for more complex disorders could be used at home.

In simulated social interactions, therapists carefully control virtual avatars’ responses to address each patient’s idiosyncratic anxieties. Computer-generated therapists aren’t yet so versatile that they can have conversations with patients that go in any direction, Bouchard says. He does believe, however, that virtual humans will eventually reach that level of sophistication. Even if virtual therapists are up to the job, many patients may not be driven enough to complete treatment on their own, Lindner says. “There was a lot of hype about [smartphone] mental health apps, and very few of them saw any kind of extensive real-world use.”

Motivation isn’t the only barrier to self-help. In some cases, self-led therapy may simply be too stressful. For patients using the personified-hallucinations program, “it’s really difficult to do at the beginning, because you’re hearing really bad things, like, ‘You’re an asshole, go kill yourself,’ ” Dumais says. “I don’t think a person can manage that alone.”

But developers shouldn’t discount potential stand-alone treatments before they’ve been tested, Reger says. These systems may make therapy, at least for some disorders, accessible to many patients who can’t or don’t want to see a human therapist. If automated treatments for complex disorders like PTSD were found safe and effective, he says, “I would certainly be a fan.”

Back in Iraq Wilfredo Serrano Waters didn’t know what to expect when he began VR therapy at Emory Healthcare in Atlanta. He’d played games and watched movies on his VR headset at home, but the 34-year-old wasn’t sure how it would feel to relive some of his own worst memories in virtual reality. Would VR help relieve the post-traumatic stress he’d suffered since deployments to Iraq from 2007 through 2009? The experience was “extremely immersive,” Serrano Waters says of the two-week course. While wearing a headset and headphones, he sat in a chair on a platform that could vibrate. “If there’s an explosion, not only do you see it and hear it, but you feel it.” As Serrano Waters reenacted each traumatic experience, with his therapist encouraging him to focus on his thoughts and feelings in the moment, the virtual world felt increasingly realistic (a typical combat scene shown below). His heart pounded, his hands got sweaty, his mind raced. “It was stressful,” he says. “But that was the purpose of it … so I could learn how to tolerate and manage my emotions.” The treatment also involved putting himself in real, everyday situations that were taxing due to his post-traumatic stress, like being around crowds. “I would have to spend … time just sitting at the mall or a Starbucks in the middle of lunch hour,” he says. Since finishing treatment in June, Serrano Waters says he’s less anxious and more comfortable around crowds. And he’s not as frightened by sudden noises. “I was very easily startled,” he recalls. “Still [am] a little bit, honestly, but not as much.”

This article appears in the November 10, 2018 issue of Science News with the headline, "Erasing Fear: Virtual reality therapy has real-life benefits for some disorders."
===============
Warm tropical Atlantic waters juiced the 2017 hurricane season
Very warm waters in the tropical Atlantic Ocean were the primary cause behind the region’s many strong hurricanes last year, including powerhouse storms Harvey and Maria, a new study finds. And that pattern of ocean warming is likely to become more common in the future, fueling more strong hurricanes, the researchers say.

Climate scientist Hiroyuki Murakami, now at the at the University Corporation for Atmospheric Research and based in Princeton, N.J., and colleagues used climate simulations to investigate whether several factors might have influenced the busy 2017 hurricane season, which included six major storms with intensities of category 3 or higher. That’s about double the average number of major hurricanes observed each year from 1979 to 2017. The simulations suggest that the relative warmth of waters in the tropical Atlantic, rather than factors such as the onset of a La Niña climate pattern, was the strongest driver of the storms, the researchers report online September 27 in Science.

La Niña is a cyclical phenomenon — the meteorological flipside to El Niño — that brings cooler waters to the tropical Pacific Ocean and causes a change in the wind patterns over the Atlantic that can help strengthen hurricanes (SN Online: 6/9/16).

To suss out which was the most influential factor, the team ran several experimental forecasts using a climate model known as HiFLOR. Each experiment began with different sea-surface temperatures. In one scenario, the team used mean sea-surface temperatures for the tropical Pacific from 1982 through 2012 — essentially removing the effects of La Niña. “The model still simulated a very active season of strong hurricanes,” Murakami says. In another scenario, the team removed warmer-than-average temperatures seen along the U.S. East Coast last summer. That, too, didn’t seem to affect the number of major hurricanes.

But when the team removed the sea-surface temperature anomaly observed in the tropical Atlantic last year, “the major hurricanes had gone,” Murakami says. That disappearance suggests it was the temperature anomaly in the tropical Atlantic that was primarily responsible for the glut of major hurricanes.

Hot water In 2017, Atlantic Ocean waters were as much as a degree Celsius warmer than the average temperature for the same period from 1982 to 2012. New simulations indicate that warm waters in the tropical Atlantic Ocean (box B) were the primary cause behind 2017’s large number of intense hurricanes, rather than anomalously cold Pacific waters due to the onset of La Niña (box A) or warm waters along the U.S. East Coast (box C).

Furthermore, he says, the simulations suggest a correlation not just between the warm tropical waters and storm intensity, but also between how much warmer the Atlantic’s tropical waters were compared with other tropical waters, such as those of the Pacific. When tropical waters are warm everywhere, Murakami says, atmospheric layers tend to be more stable, which isn’t favorable for hurricane formation. By contrast, anomalous warming in the tropical Atlantic Ocean compared with other parts of the world help hurricanes form.

The team’s projections of climate change and warming waters also suggest that this relative warmth will increase in the future, leading to larger numbers of major hurricanes.

Climate change attribution studies seek to determine if and how human-induced climate change is affecting weather events. Such studies have garnered increasing attention recently, with scientists last December saying three extreme events in 2016 would not have happened without climate change (SN: 1/20/18, p. 6).

Because Murakami’s team began its investigation even as the 2017 hurricane season was still under way, this work could be considered a real-time attribution study, he says. Attribution studies conducted in real time could ultimately help emergency planners better anticipate threats to communities, he adds. Another team of scientists conducted a real-time attribution study this month on Hurricane Florence, which made landfall September 14 in North Carolina. That study suggests the storm’s size and heavy rainfall were influenced by unusually warm ocean waters (SN Online: 9/13/18).

Some researchers disagree that the relative warmth between the tropical Atlantic and the Pacific is more important than just how hot the water got. The new study “is solid work” that demonstrates the importance of the tropical Atlantic compared with natural climate patterns to 2017’s hurricanes, says Michael Mann, a climate scientist at Penn State. “But I have a different interpretation” of the data, he adds. He and others have previously shown that the absolute warmth of those waters is better at predicting seasonal storm activity, he says.

Mann notes that Penn State’s own predictions for the 2017 hurricane season used absolute sea-surface temperatures and predicted a total of 15 storms for the season — close to the final tally of 17. That demonstrates that those absolute temperatures are a better predictor, he adds. “I prefer to go with what we know is true in the real world.”
===============
New devices could help turn atmospheric CO2 into useful supplies
New chemical-recycling devices might help combat climate change by making good use of heat-trapping gas produced by burning fossil fuels.

These electrochemical cells convert carbon monoxide into useful compounds much more efficiently than their predecessors, researchers report online October 25 in Joule. If combined with existing technology that harvests carbon monoxide from carbon dioxide, the devices could help transform CO 2 captured from pollution sources, like power plant flue gas stacks. That could reduce the warming effect of carbon emissions and produce chemical supplies for manufacturing and space travel.

In the new device, an anode and a cathode are separated by a substance called electrolyte. Carbon monoxide that enters a copper catalyst layer in the cathode combines with electrons as well as charged particles called ions driven from the anode through the electrolyte by an electric voltage. That combination produces new carbon-based compounds, like ethylene gas and liquid acetate. Until now, these types of cells have converted only a small fraction of incoming CO into desired chemicals.

In the new cells, the cathode is backed by a titanium block engraved with tiny channels that feed carbon monoxide directly into the copper catalyst layer of the cathode. Supplying CO to the catalyst through these passageways, rather than simply exposing the cathode to a CO stream where gas would either diffuse into the catalyst layer or bounce off the cathode unreacted, increases the CO’s likelihood of reacting inside the cathode to produce new chemicals.

Chemist Matthew Kanan and colleagues at Stanford University built an electrochemical cell with this design that harnessed as much as 84 percent of incoming CO. The cell generated gas streams containing up to 29 percent ethylene. Other cells have produced gas streams of about only 12 percent ethylene.

In another version of the cell, Kanan’s team replaced the liquid electrolyte with a polymer film, which helped the cell generate less diluted liquid products. This cell pumped out a sodium acetate solution about 1,000 times more concentrated than what other devices have produced. With its 1-centimeter-wide cathode, the cell produces just a few milliliters of liquid in 24 hours.

The researchers still need to investigate the long-term stability of these cells and how well they scale up to industry-size machines, says Chengxiang “CX” Xiang, an electrochemist at Caltech not involved in the work.

Combining this technology with devices that separate carbon monoxide from carbon dioxide could help transform the climate-changing CO 2 into useful chemical feedstock, says Christina Li, a chemist at Purdue University in West Lafayette, Ind., who was not involved in the work. For instance, ethylene is used to make various polymers and sodium acetate is a food additive (SN: 4/16/16, p. 5). This could complement efforts to store CO 2 gas from the atmosphere in various carbon-hoarding materials, like carbonate minerals (SN: 9/15/18, p. 9).
===============
A ghost gene leaves ocean mammals vulnerable to some pesticides
A gene that helps mammals break down certain toxic chemicals appears to be faulty in marine mammals — potentially leaving manatees, dolphins and other warm-blooded water dwellers more sensitive to dangerous pesticides.

The gene, PON1, carries instructions for making a protein that interacts with fatty acids ingested with food. But that protein has taken on another role in recent decades: breaking down toxic chemicals found in a popular class of pesticides called organophosphates. As the chemicals drain from agricultural fields, they can poison waterways and coastal areas and harm wildlife, says Wynn Meyer, an evolutionary geneticist at the University of Pittsburgh.

An inspection of the genetic instructions of 53 land mammal species found the gene intact. But in five marine mammal species, PON1 was riddled with mutations that made it useless, Meyer and colleagues report in the Aug. 10 Science. The gene became defunct about 64 million to 21 million years ago, possibly due to dietary or behavioral changes related to marine mammal ancestors’ move from land to sea, the researchers say.

The team also gauged the rate at which two organophosphate chemicals — chlorpyrifos oxon and diazoxon — broke down in blood samples from five land mammal species and six marine or semiaquatic mammal species. While blood from the terrestrial species, including sheep, goats and ferrets, showed a decrease in toxic molecules over time, the marine species’ blood showed almost no change. Mice genetically engineered to lack the gene couldn’t break down the chemicals either.

No show Blood samples from marine and semiaquatic mammals (blue) with a faulty version of a gene called PON1, as well as a mouse with the gene knocked out, showed that most of the animals didn’t break down two organophosphate pesticide chemicals — chlorpyrifos oxon and diazoxon. Land mammals with a working version of PON1 better metabolized the compounds.

A nonfunctional PON1 doesn’t necessarily mean marine mammals are helpless against organophosphates, says environmental toxicologist Andrew Whitehead at the University of California, Davis who was not involved in the work. The animals may have other defense mechanisms, but in this study, “they aren’t stepping up to the plate to metabolize these organophosphates,” he says.

It’s unclear if organophosphates build up in marine mammals’ bodies in a way similar to DDT, a type of pesticide that doesn’t break down easily in the environment. DDT, which is banned in dozens of countries, can accumulate in marine mammals’ tissues and cause nervous system damage and birth defects (SN Online: 1/19/16).

What’s more, “even though organophosphates don’t stick around as long in the environment as DDT, there’s persistent input,” Whitehead says. The chemicals are often used on crops and to kill mosquitoes and other pests.

The researchers plan to collect blood samples from dolphins and manatees in coastal areas suffused with agricultural runoff, says study coauthor Nathan Clark, an evolutionary biologist also at the University of Pittsburgh. That could help scientists monitor if the animals have been exposed to the pesticides and if that corresponds with levels of the chemicals in the environment, he says.

Editor’s note: This story was updated August 17, 2018, to correct the number of marine mammal species with a mutated PON1 gene.

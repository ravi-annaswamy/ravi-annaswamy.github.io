 NewScientis 
THE COLLECTIO 
THE 
HUMAN 
DECODING  CONSCIOUSNESS 
ULTIMATE GUIDE  TO MEMORY 
THE MIND-BODY  CONNECTION 
' ■' 
'u^HOW TO FINE-TUNE 
INTELLIGENCE 
EXPLAINED 
YOUR BRAIN 
SLEEP &  DREAMING 
AUS $14.95 /NZ$19.95 

 DON’T LET  THEM FIGHT 
IT ALONE Cure Bre)p 
\r*~^ FOUNO/snoM ^ 

Brain cancer  kills more children  than any other disease. 
in Cancer 
IN 
0 
ante 
Get to your Feet this November 
walk^braincancer.com.au 
NewScientist 
THE COLLECTION 
Get inside your  own head 
VOL TWO / ISSUE ONE 
THE HUMAN  BRAIN 
NEWSCIENTIST  THE COLLECTION 
Tower 2, 475 Victoria Avenue,  Chatswood, NSW 2067  +61(0)2 94222893  enquiries@newscientist.com 
Editor-in-chief Graham Lawton  Editor Catherine de Lange  Art editor Craig Mackie 
Picture editor Prue Waller  Subeditor Chris Simms  Graphics Nigel Hawtin  Production editor Mick O'Hare  Project manager Henry Comm  Publisher John MacFarlane 
© 2015 Reed Business  Information Ltd, England  New Scientist The Collection is  published fourtimes peryear by Reed  Business Information Ltd  ISSN 1032-1233 
Newsstand 
Network Services  Tel 1300 131 163 (Australia only)  Netlink Distribution Company  Tel +64 9 366 9966 
Printed in Australia by Offset Alpine  Printing, 42 Boorea St Lidcombe,  NSW 2141 
Display advertising 
+61(0)29422 2083  displayads@newscientistcom 
Cover image Bruno Vergauwen 
T he answers to the biggest questions of  our existence - what is consciousness,  what makes people behave the way  they do, what is intelligence, and why do  you sleep and dream - are all rooted in the  1.4 kilograms of soft stuff between your ears.  These are questions about what it means to  be human, about what makes you “you”. 
For millennia, questions like these  have driven the pursuit to understand the  human brain. It has been 2500 years since  Hippocrates suggested that the mind resides  not in the heart, but in our heads. And yet  until relatively recently, the brain remained  a black box, inaccessible without cracking  open the skull or studying people with  brain damage. 
Modern technology is changing that. If the  19th century was dominated by chemistry  and the 20th saw the birth of modern physics,  we might rightly consider the 21st century to  be that of neuroscience. Imaging techniques  are allowing us to see the brain in action. We  now know it is the place where our thoughts,  desires, habits and personalities originate,  not to mention the conscious state of being. 
There are many questions still to answer  about just how this tangle of 100 billion  neurons pulls off such a feat, but science is  now shining a light into this black box, and  the findings are remarkable. 
This fifth issue of New Scientist: The  Collection is dedicated to the most complex  object in the known universe - the human  brain. A compilation of classic articles from  New Scientist, it will show you what’s going on  inside your head, and what happens when the  workings of the mind go awry. 
Chapter 1 lays the foundations of our  knowledge - from our early understanding  of the brain’s structure to the technical  challenges of building a more detailed picture.  Chapter 2 dives inside your mind to tackle 
the slippery subject of thought, and the inner  speech that often accompanies thinking. 
If thinking about thinking is a challenge.  Chapter 3 might be helpful. It deals with  intelligence. What makes someone smart? 
And are we all getting more stupid? 
Chapter 4 looks at what happens when  the mind goes wrong, and how our knowledge  of the brain might help treat conditions like  schizophrenia, depression and dementia. 
Chapter 5 charts how your brain changes  during your life. Why can’t you remember  much from your early years, what do babies  think about, and how do male and female  brains differ? 
Chapter 6 tackles one of the trickiest  questions in science - how the physical  processes of the brain generate the feeling  of being conscious. 
Chapter 7 reveals how the mind extends  beyond the body to the world around us,  and shows how you can use your body to  fine-tune your mind. 
Chapter 8 examines one of the most  crucial functions of the brain - memory,  which dictates how we think, act and make  decisions, and even defines our identity. 
Chapter 9 gives you the tools to get the best  out of your brain, from how to stay attentive,  to boosting your creativity and learning. 
And finally. Chapter 10 switches off the  lights to look at one of the most mysterious  and intimate brain states - sleep. What is it for,  why do we dream and can we glean meaning  from the unconscious meanderings of our  minds? It’s time to get to know yourself better. 
Catherine de Lange, Editor 
The Human Brain | NewScientist: The Collection! 1 
CONTENTS 
NewStientlst 
THE COLLECTION 
CONTRIBUTORS  Anil Ananthaswamy 
is a consultant for New Scientist 
Harald Baaycn 
is professor of quantitative linguistics at the  University of Tubingen in Germany 
Colin Barras 
is a writer based near Ann Arbor in Michigan 
Tim Bayne 
is a professor of philosophy at the University  of Manchester, UK 
Michael Brooks 
is a science writer and A/ewSc/enfet consultant 
Ingfei Chen 
is a writer based in Santa Cruz, California 
John Cryan 
is a professor of anatomy and neuroscience at  University College Cork in Ireland 
Catherine de Lange 
is a feature editor at New Scientist 
Derk-jan Dijk 
is a professor of sleep and physiology at the  University of Surrey in Guildford, UK 
Timothy Dinan 
is a professor of psychiatry at University College Cork  in Ireland 
Liam Drew 
is a neurobiologist and science writer at University  College London 
Charles Fernyhough 
is a psychologist at Durham University, UK 
James Flynn 
is professor of political science at the University of  Otago, in New Zealand 
Douglas Fox 
is a freelance writer based in San Francisco 
Linda Geddes 
is a consultant for New Scientist based in Bristol, UK 
Linda Gottfredson 
is Professor Emeritus of education at the University  of Delaware in Newark 
Bob Holmes 
is a consultant for New Scientist based in  Edmonton, Canada 
ChristofKoch 
is a professor at the California Institute of  Technology in Pasadena 
Graham Lawton 
is deputy ed itor of New Scientist 
Lambros Malafouris 
is a research fellow at the University of Oxford 
Samantha Murphy 
is a journalist based in Lancaster, Pennsylvania 
Michael O'Shea 
is a professor of neuroscience at the University of  Sussex, UK 
Michael Ramscar 
is in the department of linguistics at the University  of Tubingen in Germany 
David Robson 
is features writer at BBC Future 
KaytSukel 
is a science and travel writer based in Texas 
MaxTegmark 
is a professor of physics at the Massachusetts  Institute of Technology 
Kirsten Weir 
is a freelance writer based in Minneapolis 
Caroline Williams 
is a writer based in Surrey, UK 
Clare Wilson is a news reporter at New Scientist 
Raphaelle Winsky-Sommerer 
is a senior lecturer in sleep and circadian rhythms at  the University of Surrey in Guildford, UK 
Emma Young 
is a writer based in Sheffield, UK 
The articles in this collection were first  published in A/ewSc/ent/st between  February 2011 and October 2014.  They have been updated and revised. 
VOL TWO / ISSUE ONE 
THE HUMAN  BRAIN 
How it works 
6 Milestones of neuroscience  10 The greatest map of all  14 Like clockwork  18 Mind maths  24 Hidden depths 
 Thought 
30 Thought: A user's guide to thinking  38 Life in the chatter box 
 Intelligence 
42 What is intelligence? 
46 The intelligent approach to IQ  48 Are we getting stupider? 
 Mental health 
52 Down with dementia  56 Out of the shadows  61 Rebuilding broken brains  64 Fits of rapture  68 Alightonpsychobiotics 
 2 1 NewScientist: The Collection | The Human Brain 

 Extended mind 
98 Mind into matter  100 Your clever body 
Ages and sexes  of the brain 
70 Our forgotten years  74 Into the minds of babes  78 Cognitive decline? Pah! 
81 Pink brains, blue brains, purple people 

Memory 
104 Memory: The ultimate guide 
106 When I was a chick 
108 A likely story 
110 Fade to black 
112 Stuck in the present 
Consciousness 
84 Closing in on consciousness  86 The consciousness connection  90 Into the void  96 Solid. Liguid. Consciousness 
 Q 
Make the  most of it 
114 A user's guideto the mind 
 Sleep 
122 The origins and purpose of sleep  124 The I in dreaming 

The Human Brain I NewScientist: The Collection I 3 

u ...we are on a  10 year mission 

Cure Brain Cancer 
FOUNDATION  Many minds, one purpose 
'IF we can put a man on the  moon in 10 years then it stands  to reason we have the ingenuity  to dramatically improve brain  cancer survival rates in that  time. Our mission is to increase  Five-year survival to 50% within  10 years. We will succeed/ 
- A/ProF Charlie Teo, Founder  Cure Brain Cancer Foundation 
Find out more about us  curebraincancer.org.au 
BRAIN CANCER 
KILLS MORE CHILDREN THAN ANY OTHER DISEASE 

 Yet 90% oF Australians are unaware oF this Fact 
*S0URCE CURE BRAIN CANCER RESEARCH OF 1,010 NATIONALLY REPRESENTATIVE AUSTRALIAN ADULTS AGED 18+ JULY 201A 
55 51 
® 0 o 
Heart Heart Thyroid 
Disease Defect Cancer 
Number of deaths between 2008-2012 
*SOURCE AUSTRALIAN BUREAU OF STATISTICS (2010 - 201A), 3303.0 CAUSES OF DEATH, AUSTRALIA (2008 - 2012) TABLE 1.3: UNDERLYING  CAUSE OF DEATH, SELECTED CAUSES BY AGE AT DEATH, NUMBERS AND RATES, AUSTRALIA (2008 - 2012) 

150 
Brain 
Cancer 
117 
 Leukaemia 
It gets worse. For some Forms,  such as Glioblastoma or DIPG,  survival is much lower. 
5% 
OF PEOPLE SURVIVE 
GLIOBLASTOMA 
*S0URCE (OSTROM, Q, GITTLEMAN, H, FARAH, P, ONDRACEK, A, CHEN, Y, WOLINSKY, Y, STROUP, N.E, KRUCHKO, C B BARNHOLTZ-SLOAN, IS,  (2013). CBTRUS STATISTICAL REPORT: PRIMARY BRAIN AND CENTRAL NERVOUS SYSTEM TUMOURS DIAGNOSIS IN THE UNITED STATES IN 
2006-2010. NEURO-ONCOLOGY, P50-51. 
Brain cancer kills more Australians  under ^0 than any other cancer. 
Brain cancer costs more  per patient than  any other cancer 

Jo tfas? 
Index of lifetime cost per patient  *SOURCE THE COST OF CANCER NSW- REPORT BY ACCESS ECONOMICS, AUSTRALIA WIDE, APRIL 2007 
4 
Yet it recieves just 3% oF NHMRC/  Government Cancer Research Funding 
*S0URCE NHMRC GRANT APPLICATION ROUND 2009-2013 
 Prostate Cancer Bowel 
’*■ SOURCE: INCREASE IN SURVIVAL , 
 1982 - 2010. AIHW NATIONAL CANCER STATISTICS 
We need a huge increase  in research Funding to 
accelerate new treatments  and improve survival 
Don’t let children Fight brain cancer alone 
Help Cure Brain Cancer in their mission to increase  Five-year survival to 50% within 10 years. 
w 
Cure Brain Cancer 
FOUNDATION 
Many minds, one purpose 
FIND OUT MORE & DONATE  curebraincancer.org.au  or 1300 362 965 

 CHAPTER ONE 
HOW IT WORKS 
Milestones of  neuroscience 
We now have a good understanding of the brain's building block  - the neuron. But it's taken us 2500 years, says Michael O'Shea 
A bout 250,000 years ago, something 
quite extraordinary happened. Animals  with an unprecedented capacity for  thought appeared on the savannahs of Africa.  Eventually, they were smart enough to start  questioning the origins of their own  intelligence. We are finally close to getting some  answers, but it has not been a smooth journey. 
THE BEGINNINGS 
The birth of neuroscience began with  Efippocrates some 2500 years ago. While  his contemporaries, including Aristotle,  believed that the mind resided in the heart,  Hippocrates argued that the brain is the seat  of thought, sensation, emotion and cognition. 
It was a monumental step, but a deeper  understanding of the brain took a long time to  follow, with many early theories ignoring the  solid brain tissue in favour of fluid filled  cavities, or ventricles. The 2nd-century  physician Galen - perhaps the most notable  proponent of this idea - believed the human  brain to have three ventricles, with each one  responsible for a different mental faculty:  imagination, reason and memory. According  to his theory, the brain controlled our body’s  activities by pumping fluid from the ventricles  through nerves to other organs (for more on  this, see ‘‘Like clockwork”, page 14). 
Such was Galen’s authority that the idea  cast a long shadow over our understanding  of the brain, and fluid theories of the brain  dominated until well into the 17th century.  Even such luminaries as French philosopher  Rene Descartes compared the brain to a  hydraulic-powered machine. Yet the idea had a  major flaw: a fluid could not move quickly  enough to explain the speed of our reactions. 
A more enlightened approach came when a  new generation of anatomists began depicting 
Santiago Ramon y Cajal is considered by many  to be the father of modern neuroscience 
the structure of the brain with increasing  accuracy. Prominent among them was the  17th-century English doctor Thomas Willis,  who argued that the key to how the brain  worked lay in the solid cerebral tissues, not  the ventricles. Then, 100 years later, Luigi  Galvani and Alessandro Volta showed that  an external source of electricity could  activate nerves and muscle. This was a crucial  development, since it finally suggested why  we respond so rapidly to events. But it was  not until the 19th century that German  physiologist Emil Du Bois-Reymond  confirmed that nerves and muscles  themselves generate electrical impulses. 
All of which paved the way for the modern  era of neuroscience, beginning with the work  of the Spanish anatomist Santiago Ramon y  Cajal (pictured) at the dawn of the 20th  century. His spectacular observations  identified neurons as the building blocks of  the brain. He found them to have a diversity of  forms that is not found in the cells of other  organs. Most surprisingly, he noted that insect  neurons matched and sometimes exceeded  the complexity of human brain cells. This  suggested that our abilities depend on the way  neurons are connected, not on any special  features of the cells themselves. 
Cajal’s “connectionist” view opened the  door to a new way of thinking about  information processing in the brain, and it  still dominates today. > 
 Neurons are some of the  most diverse cells in the  human body, though  they all share the same  basic features 


MOTOR 
Send signals to parts of  the body, such as muscle,  to direct movement 
T 

 INTER 
Provide a connective  bridge between other  neurons 
T 

 SENSORY 
Transmit signals to the  brain from the rest of  the body 
 PYRAMIDAL  Involved in many areas  of cognition, such as  object recognition  within the visual cortex 
6 1 NewScientist: The Collection | The Human Brain 
 WIREDTOTHINK 
While investigating the anatomy of  neurons in the 19th century,  Santiago Ramon y Cajal proposed  that signals flow through neurons  in one direction. The cell body and  its branched projections, known as  dendrites, gather incoming  information from other cells.  Processed information is then  transmitted along the neuron's long  nerve fibre, called the axon, to the  synapse, where the message is  passed to the next neuron (see  diagram, below). 
It took until the 1940s and 50s  for neuroscientists to get to grips  with the finer details of this  electrical signalling. We now know 
that the messages are transmitted  as brief pulses called action  potentials. They carry a small  voltage - just 0.1 volts - and last  only a few thousandths of a second,  but they can travel great distances  during that time, reaching speeds of  120 metres per second. 
The nerve impulse's journey  comes to an end when it hits a  synapse, triggering the release of  molecules called 
neurotransmitters, which carry the  signal across the gap between  neurons. Once they reach the other  side, these molecules briefly flip  electrical switches on the surface of  the receiving neuron. This can 
either excite the neuron into  sending its own signal, or it  can temporarily inhibit its activity,  making it less likely to fire in  response to other incoming  signals. Each is important for  directing the flow of information  that ultimately makes up our  thoughts and feelings. 
The complexity of the resulting  network is staggering. We have  around 100 billion neurons in our  brains, each with 1000 synapses.  The result is 100 trillion inter-  connections. If you started to count  them at one per second you would  still be counting 30 million years  from now. 
 THE PLASTIC BRAIN 
Unlike the electronic components of  a computer, our networks of  neurons are flexible thanks to a  special class of neurotransmitter.  These "neuromodulators" act a bit  like a volume control, altering the  amount of other neurotransmitters  released at the synapse and the  degree to which neurons respond to  incoming signals. Some of these  changes help to fine-tune brain  activity in response to immediate  events, while others rewire the brain  in the long term, which is thought to  explain how memories are stored. 
Many neuromodulators act on  just a few neurons, but some can  penetrate through large swathes of 
brain tissue creating sweeping  changes. Nitric oxide, for example,  is so small (the 10th smallest ^  molecule in the known universe, in  fact) that it can easily spread away  from the neuron at its source. It  alters receptive neuroos by  changing the amount of  neurotransmitter released witB' -  each nerve impulse, kicking off the  changes that are necessary for  memory formation in the  hippocampus. 
Through the actions of a  multitude of chemical transmitters  and modulators, the brain is  constantly changing, allowing us  to adapt to the world around us. 


 MAPPING  THE MIND 
Advanced imaging techniques have  given us a detailed map of where  different skills arise in the brain 
O ur billions of neurons, joined by trillions  of neural connections, build the most  intricate organ of the human body.  Attempts to understand its architecture began  with reports of people with brain damage.  Localised damage results in highly specific  impairments of particular skills - such as  language or numeracy - suggesting that our  brain is modular, with different locations  responsible for different mental functions. 
Advanced imaging techniques developed in  the late 20th century gave a more nuanced  approach by allowing researchers to peer into  healthy brains as volunteers carried out  different cognitive tasks. The result is a detailed  map of where different skills arise in the brain -  an important step on the road to  understanding our complex mental lives. 
FOREBRAIN 
Many of our uniquely human capabilities  arise in the forebrain, which expanded rapidly  during the evolution of our mammalian  ancestors. It includes the thalamus, a relay  station that directs sensory information to  the cerebral cortex for higher processing; 
2 the hypothalamus, which releases hormones  I into the bloodstream for distribulion la  I the rest of the body; the amygdala, which  ^ deals with emotion; and the hippocampus,  j which plays a major role in the formation of  I spatial memories. 
§ Among the most recently evolved parts  g are the basal ganglia, which regulate the  I speed and smoothness of intentional  i movements initiated by the cerebral cart^X-  I Connections in this region are modulated by  5 the neurotransmitter dopamine, provided by  I the midbrain’s substantia nigra. A deficiency  ^ in this source is associated with many  s of the symptoms of Parkinson’s disease, such 
enters and leaves the cortex through about  a million neurons, but it has more than  10 billion internal connections, meaning the  cortex spends most of its time talking to itself. 
Each of the cortical hemispheres have four  principal lobes (see upper diagram, right). 
The frontal lobes house the neural circuits  for thinking and planning, and are also  thought to be responsible for our individual  personalities. The occipital and temporal lobes  are mainly concerned with the processing of  visual and auditory information, respectively.  Finally, the parietal lobes are involved in  attention and the integration of sensory  information. 
The body is ‘‘mapped” onto the cortex many  times, including one map representing the  senses and another controlling our  movements. These maps tend to preserve the  basic structure of the body, so that neurons  processing feelings from your feet will be  closer to those dealing with sensations from  your legs than those crunching data from your  nose, for example. But the proportions are  distorted, with more brain tissue devoted to  the hands and lips than the torso or legs.  Redrawing the body to represent these maps  results in grotesque figures like Penfield’s  homunculus (left). 
The communications bridge between the  two cerebral hemispheres is a tract of about a  million axons, called the corpus callosum.  Cutting this bridge, a procedure sometimes 
as slowness of movement, tremor and  impaired balance. Although drugs that boost  levels of the neurotransmitter in the basal  ganglia can help, a cure for Parkinson’s is still  out of reach. 
Finally, there is the cerebral cortex - the  enveloping hemispheres thought to make us  human. Here plans are made, words are formed  and ideas generated. Home of our creative  intelligence, imagination and consciousness,  this is where the mind is formed. 
Structurally, the cortex is a single sheet of  tissue made up of six crinkled layers folded  inside the skull; if it were spread flat it would  stretch over 1.6 square metres. Information 
8 1 NewScientist: The Collection | The Human Brain 

performed to alleviate epileptic seizures, can  split the unitary manifestation of “self”. It is as  if the body is controlled by two independently  thinking brains. One smoker who had the  surgery reported that when he reached for a  cigarette with his right hand, his left hand  would snatch it and throw it away! 
As we have seen, different tasks are carried  out by different cortical regions. Yet all you  have to do is open your eyes to see that these  tasks are combined smoothly: depth, shape,  colour and motion all merge into a 3D image  of the scene. Objects are recognised with no  awareness of the fragmented nature of the  brain’s efforts. Precisely how this is achieved  remains a puzzle. It’s called the “problem of  binding” and is one of the many questions left  to be answered by tomorrow’s neuroscientists. 
MIDBRAIN 
The midbrain plays a role in many of our  physical actions. One of its central structures  is the substantia nigra, so-called because it is a  rich source of the neurotransmitter  dopamine, which turns black in post-mortem  tissue. Because dopamine is essential for the  control of movement, the substantia nigra is  said to “oil the wheels of motion”. Dopamine  is also the “reward” neurotransmitter and is  necessary for many forms of learning,  compulsive behaviour and addiction. 
Other regions of the midbrain are  concerned with hearing, visual information  processing, the control of eye movements and  the regulation of mood. 
 BASAL GANGLIA  THALAMUS  HYPOTHALAMUS  AMYGDALA  HIPPOCAMPUS - 
 MEDULLAOBLONGATA-- 
CEREBAL CORTEX 
SUBSTANTIA 
PONS- 
CEREBELLUM 
HINDBRAIN 
As its name suggests, the hindbrain is located  at the base of the skull, just above the neck.  Comparisons of different organisms suggest it  was the first brain structure to have evolved,  with its precursor emerging in the earliest  vertebrates. In humans it is made up of three  structures: the medulla oblongata, pons and  cerebellum. 
The medulla oblongata is responsible for  many of the automatic behaviours that keep  us alive, such as breathing, regulating our  heartbeat and swallowing. Significantly, its  axons cross from one side of the brain to the  other as they descend to the spinal cord, which  explains why each side of the brain controls  the opposite side of the body. 
A little further up is the pons, which also  controls vital functions such as breathing,  heart rate, blood pressure and sleep. It also  plays an important role in the control of  facial expressions and in receiving  information about the movements and  orientation of the body in space. 
The most prominent part of the hindbrain  is the cerebellum, which has a very distinctive  rippled surface with deep fissures. It is richly  supplied with sensory information about the  position and movements of the body and can  encode and memorise the information  needed to carry out complex fine-motor skills  and movements. ^ 
The Human Brain I NewScientist: The Collection I 9 
 Charting the connections between nerve cells could one  day give us a read-out of our brains, says Douglas Fox 
A STRANGE contraption, a cross  between a deli meat slicer and a  reel-to-reel film projector, sits in  a windowless room in Cambridge,  Massachusetts. It whirs along unsupervised  for days at a time, only visited occasionally  by Narayanan Kasthuri, a mop-haired  postdoc at Harvard University, who  examines the strip of film spewing out. 
It may seem unlikely, but what’s going on  here signifies a revolution in neuroscience.  Spaced every centimetre along the film are  tiny dots, each of which is a slice of mouse  brain, one-thousandth the thickness of a sheet  of aluminium foil. This particular roll of film  contains 6000 slices, representing a speck of  brain the size of a grain of salt. 
The slices of brain will be turned into digital  images by an automated electron microscope.  A computer will read those images, trace the  outlines of nerve cells and stack the pictures  into a 3D reconstruction. 
In the jargon, they are building the mouse  “connectome”, named in line with the term  ‘‘genome” for the sequence of all of an  organism’s genes, and “proteome” for all its  proteins, and so on. 
It’s an epic undertaking. Using this  technique, the full mouse connectome  would produce hundreds of times more  data than can be found on all of Google’s  computers, says Jeffrey Lichtman, the  neuroanatomist leading the Harvard team.  And yet it’s just the beginning. Their efforts  could be seen as a dry run for a project that is  at least four orders of magnitude greater:  mapping the human connectome. 
With about 100 billion neurons, each with  up to 10,000 connections, or synapses, the  human brain is the most complex object in  the known universe. To map the entire thing  would arguably be the most ambitious project  we have attempted and, for now, lies out of  reach. Yet thanks to the constant acceleration  of our computing and biotechnological  capabilities, the first steps towards the  roughest of drafts are already being taken. 
In line with the scale of the challenges,  the pay-offs could be huge. Even the most  rudimentary blueprint of the brain could  reveal how genes and experience shape  our wiring, which in turn determines our  individual differences. It would advance our  understanding of conditions such as autism,  schizophrenia and addiction - all of which  are increasingly viewed as “connectopathies”.  It could even shed light on such mysteries as  intelligence and consciousness. “Now is the  time we’re going to answer stuff that we’ve  been waiting half a century to deal with,”  says Robert Marc, a vision scientist at the  University of Utah in Salt Lake City. 
It is only in the past few decades that  scanning techniques have allowed scientists  to peer inside living brains. Magnetic  resonance imaging (MRI) provides detailed  anatomical images, and an enhanced  version called functional MRI measures  fluctuations in blood supply to different  parts of the brain as people carry out  specific mental tasks. On the assumption  that blood supply reflects how hard  neurons are working, this effectively lets  neuroscientists watch the brain in action. 
These techniques have led to a wealth of  new insights, but they reveal nothing about  neural connections. Brain tissue in these  images looks more like the filling of a cream  cake than the trillions of criss-crossing  neural wires that are really there. 
The first animal to have those wires mapped  was a millimetre-long, dirt-dwelling  roundworm called Caenorhabditis elegans,  which turned out to have 302 neurons and  9000 synapses. Incredibly, this work started  in the 1970s, with little of today’s equipment.  The worm was cut into several thousand slices  before being imaged under an electron  microscope. 
In those days, the delicate slices were  floated on a bead of water and manipulated  using a toothpick with a human eyelash  glued to the end. Touching the slices with  the eyelash destroyed them, so the team had  to gingerly brush the surrounding water to > 
"Our memories, and many  other things that make us  individuals, may be encoded  in our connectomes" 
10 1 NewScientist: The Collection | The Human Brain 
 The Human Brain | NewScientist: The Collection! 11 
 nudge them into place - and for good  measure, the slices were almost invisible on  the water. Understandably, it took 14 years to  assemble the wiring diagram, published in a  landmark 446-page paper dubbed “The mind  of a worm”. 
Technology has moved on since the days  when an eyelash was part of the laboratory  toolkit, and the Harvard team is not alone in  coming up with a film-projector-like brain  imager. The equipment is improving all the  time. Even so, mapping an entire mammal  brain in intricate detail is still a painstaking  task, so a more pragmatic approach has been  to focus on questions that can be answered by  mapping discrete areas of the brain. 
Kasthuri, for example, has been working  on a connectome for the mouse cerebellum,  a cauliflower-shaped structure at the base  of the brain that has fine control over  movements. Marc is concentrating on the  retina, the patch of nerve-rich tissue at the  back of each eyeball that is seen as an  extension of the brain, in a bid to understand  common causes of blindness such as  glaucoma and retinitis pigmentosa. “Our  connectomes are pouring out data faster than  we imagined possible,” he says. 
Sebastian Seung, a computational  neuroscientist now at Princeton University  has been charting part of a zebra finch’s  brain to try to read the bird’s song from  its connectome. It may sound like an  eccentric goal, but it would be an important  proof of concept: that we could one day read  a brain’s memories. 
Mouse brain 
 neurons 
 synapses 
Projects such as these have attracted the  attention of “transhumanists”, people who  want to harness technology to live forever.  They see connectomics as the first step to  downloading their brains into computers. 
The Brain Preservation Foundation has  offered a scaled-down version of the XPrize:  up to $106,000 for the first lab to develop a  way to preserve a whole mammal brain at  the moment of death, so that its connectome  could be read. 
Whether because of the modest nature of  the reward - the original XPrize for private 
"Miswired brains could  be identified and treated  years before symptoms  begin to emerge" 
space flight was $10 million - or the  transhumanists’ oddball reputation, most  neuroscientists seem indifferent to the prize.  “It doesn’t motivate me at all,” says Kasthuri.  “I’m much more interested in using  connectomics to understand biology.” 
The cell-by-cell approach has its critics,  though. Charles Gilbert, a neurobiologist at  Rockefeller University in New York, points out  that synapses constantly change. He has  found that in a mouse cortex, they turn over  at the rate of 7 per cent per week. “You may  take a snapshot of the connections,” he says.  “That doesn’t necessarily mean that those are  the connections that exist all the time.” 
Human brain 
 neurons 
 synapses 
Another drawback is that a typical speck of  brain being mapped will have thousands of  neurons coming in from distant areas, which  are lopped off at the edge of the sample with  no clue to their origin. 
That’s why other groups have taken a step  back to look at the bigger picture. Instead of  trying to map every single nerve cell, they are  mapping just the long-distance connections. 
The brain is organised so that the outermost  cortex contains the main bodies of the nerve  cells and the short branches that connect to  nearby cells. Underneath the cortex lie the  cells’ long projections, or axons, which  connect distant areas. Axons are swaddled in  a fatty coating called myelin, which improves  electrical conduction. As the myelin is pale,  the underlying part of the brain is known as  the “white matter”, in contrast with the “grey  matter” of the cortex. 
One technique for mapping axons is more  than 100 years old: injecting dye into cells in  one spot in the brain, and watching as it  spreads to distant areas. Partha Mitra of Cold  Spring Harbor Laboratory in New York is using  an automated version of this technique to  inject dye at 500 locations in a mouse brain  and trace its course. He says he is well on his  way to producing a draft of the mouse brain  and hopes to map the brains of human  cadavers in the same way. “We are trying to do  the pragmatically defined project that will  take us to the whole brain,” he says. 
Such a feat was announced by the Allen  Institute for Brain Science in Seattle in 2014.  Using a similar approach, a group there  became the first to publish a full mouse  connectome, but the picture is far from  complete, says Marc. He calls the Allen map “a  tour-de-force”, but likens it to a national map  of highways between cities. “Our approach  involves mapping on a much higher  resolution scale, analogous to tracking every  street, house and house number, sidewalk,  water line and power cable in a city,” he says.  “Both are critical for a richer understanding of  neural information processing. You can’t  really dispense with either.” 
There are also newer ways to trace long-  distance connections that do not entail  injecting harmful dyes or slicing brains into  prosciutto. These mean that, finally, we can  start to look at living brains. 
One method takes advantage of the fact  that water molecules can diffuse more freely  along axons lengthwise than they can pass  through the fatty myelin coating. In 2007,  it was reported that a technique called  diffusion MRI could show how the trillions 
12 1 NewScientist: The Collection | The Human Brain 
 of water molecules in the brain are jostling  against one another (see ‘‘Mind readers”,  below right). Their direction of movement  indicates the paths of hundreds of axon  bundles in a living brain. 
While still in its infancy, diffusion MRl is  leading to important advances. For example,  Heidi johansen-Berg and Timothy Behrens of  the University of Oxford are using it to study  the effects of stroke, in which bleeding or a  blood clot in the brain causes local nerve tissue  to die from lack of oxygen. They have found  that the death of the area affected by the  stroke can have knock-on effects on other  areas connected by axon bundles. Seeing  those changes is important, because  techniques are being developed to strengthen  brain connections by applying electric  currents or magnetic fields to the skull. 
At Harvard, Van Wedeen, one of the  original developers of diffusion MRl, has  used it to reconstruct how the human brain  rewires itself over time. His team’s results  show that between toddlerhood and  adolescence, the brain becomes more  centrally organised around a few major  hubs, which might allow signals to traverse  the brain more rapidly. 
Rewiring problems may well underlie the  tendency of mental illness to arise during  adolescence and early adulthood. Diffusion  MRl studies have already identified specific  axon bundles that are altered in schizophrenia,  alcoholism and other conditions. The hope is  that miswired brains could be identified and  treated years before symptoms emerge. 
The insights are extending beyond  medicine. Several studies show that the 
strength of specific axon bundles seems to  correlate with skills such as arithmetic and  rapid word recall. It may also shed light on  how experiences shape minds, and how  memories form. “My memories, many things  that make me an individual, may be encoded  in my connectome,” says Seung. “The  hypothesis is that 1 am my connectome.” 
In 2009, johansen-Berg and Behrens  showed that diffusion MRl could detect the  effects of just six weeks of juggling practice,  for example. Learning the new skill thickened 
MIND READERS 
MAGNETIC RESONANCE IMAGING (MRl) 
Showing detailed anatomical images, it is like  an X-ray for soft tissues 
FUNCTIONAL MRl (f MRl) Displays changes in  blood supply - assumed to correlate with local nerve  activity - to different brain areas during mental tasks  such as arithmetic or reading 
DIFFUSION MRl (also called diffusion imaging,  tractography) Reveals the brain's long-distance  connections; works by tracking water molecules,  which can diffuse along the length of axons more  freely than escaping out through their fatty coating 
FUNCTIONAL CONNECTIVITY MRl (resting-state  MRl) Also shedding light on long-distance  connections, it measures spontaneous fluctuations  in activity in different brain areas, which reveals the  degree to which they communicate 
the connections in several axon bundles  involved in hand-eye coordination. 
Another new scanning technique has  developed from functional MRl, which was  originally designed to see which parts of the  brain crank up their workload when people  carry out specific mental tasks. It was later  found that even when people lie resting in  the scanner, the activity of individual brain  areas seems to gently fluctuate over a 10 to  30-second cycle. Crucially, many areas known  to have strong connections have cycles that  are in sync with each other, either in or out  of phase. Discovering which areas match  up in this way, using a technique known as  functional connectivity MRl, is another  source of information about the brain’s  long-distance connections. 
It is thanks to developments such as these  that the US National Institutes of Health (NIH)  was able to launch its “human connectome  project”. In September 2010, it announced  grants to two consortia of labs, worth  $40 million over five years, to roughly map  the brains of 1200 people using diffusion  MRl, functional connectivity MRl and  other techniques. Some will also undergo  genetic and psychological tests to measure  working memory, arithmetic skills and other  mental abilities. 
So far, the teams involved in the project  have made some key findings, not least that  the brain is not a spaghetti-like tangle of  connecting nerve fibres, but is organised in a  grid like format, more reminiscent of a 3D  representation of New York City. This could  help explain how brains develop - following  simple grid-based rules that help the nervous  system to develop in the early embryo. 
The launch of the project hinted back to the  scale and importance of the human genome  project. An NIH press release called it “a grand  and critical challenge: to map the wiring  diagram of the entire, living human brain.” 
Some researchers remain sceptical about  whether the project will prove its worth within  the five-year deadline. But perhaps the most  relevant lesson from the human genome  project is how fast technology can advance. 
It took 10 years and $3 billion to complete  the first draft of the human genome. Now  some 15 years later, there are firms claiming  they will soon be able to read someone’s  genome in less than a day for $100. 
Connectome researchers are convinced this  field will generate as yet unimagined rewards.  “You will see new hypotheses about how the  nervous system works,” says Kasthuri. “No  one has ever seen data like this before.” ■ 
The Human Brain I NewScientist: The Collection 1 13 
 14 1 NewScientist: The Collection | The Human Brain 

Like clockwork 
Our brains may run mechanically, like the springs and cogs in  a finely tuned watch, says Anil Ananthaswamy 
O NE of the first things William ‘‘Jamie”  Tyler does when 1 meet him is show me  a video of “one of the most devastating  knockouts ever in boxing”. In a 1990 clash,  American pugilist Julian Jackson knocked  his English counterpart Herol Graham  unconscious with a right hook. Graham’s  lights went out before he hit the floor. 
Tyler is a boxing fan who once worked out  at the Harvard Boxing Club. But that’s not why  he’s showing me the video. Instead, as a  neuroscientist at Virginia Tech, Tyler uses it to  highlight a problem: such knockouts are a bit  of a mystery in our accepted understanding  of the brain. We think of the brain as a  biochemical and electrical organ, so how can  a mechanical event, such as a punch to the  face, cause unconsciousness? “We know  without a doubt that there is no electrical  transfer from that boxer’s leather glove to  that man’s face. It’s a mechanical impulse  wave and [yet] he’s unconscious,” says Tyler.  “Granted it’s extreme, but it demonstrates  how mechanically sensitive the brain is.”  While no one is questioning whether brain  cells use electrical and biochemical signals to  talk to each other, Tyler and others think that’s  only part of the story. It seems neurons are  also hooked together in a mechanical network,  like the cogs in a finely tuned clock. The forces  that pass between them might be an unknown  way for our brains to store memory and adapt  quickly to new circumstances - ensuring that  they always run like well-oiled machines. 
Not only could this help us probe age-old  questions about what makes our thoughts go  round, it also offers immediate practical  benefits. For one thing, understanding  disruptions to these mechanical processes  might help us address certain types of brain 
injury. It could even be possible to tinker with  the brain’s mechanics using sound waves,  which promises to lead to non-invasive  therapies for disorders such as epilepsy. 
The notion of a mechanical brain has its  origins in the mistaken ideas of the  2nd century Greek physician Galen, who  proposed that ventricles in the brain pumped  fluids through nerves to control the body’s  functions (see “Milestones of neuroscience”,  page 6). Even as recently as the 17th century,  Rene Descartes propounded a similar theory  for how the brain functions. It wasn’t until the  18th and 19th centuries that it became clear  that nerves carried electrical signals. This  culminated in the 1950s when Alan Hodgkin  and Andrew Huxley showed how these  electrical signals, called action potentials,  are transmitted along nerve fibres. 
"Tyler used to play loud  music in the lab. To his  surprise, he saw a spike  in neural activity when  the bass boomed, as if  vibrations were causing  brain changes" 
But around the time that Hodgkin and  Huxley were doing their Nobel prizewinning  work, hints began to emerge that mechanical  processes may be involved after all. The first  clue came from observations of cuttlefish  nerves, which seemed to shrink and swell  when stimulated by a small electric current.  The finding went largely unnoticed for  decades until, in 1980, Ichiji Tasaki of the  National Institute of Mental Health in  Bethesda, Maryland, and colleagues saw  something similar in nerves taken from the  claws of blue crabs: as the action potential  travelled along the nerve, so did a mechanical  wave. 
The finding helped to explain the energy  exchange as a neuron fires. Hodgkin and  Huxley had modelled the action potential as  an electrical circuit. Such a circuit dissipates  heat, but this is not what was experimentally  observed: there is no overall heat loss during  the propagation of a nerve impulse. However,  if the nerve impulse could be treated as a  mechanical wave in which heat is both  released and absorbed (with no net loss),  the energy accounting squared up nicely. 
Perhaps more importantly, it showed  that our nervous system is buzzing with  movement - albeit at the nanometre scale -  setting the scene for a mechanical  understanding of the brain. 
As well as mechanical waves moving  along nerves, researchers have looked at  the forces passing between neurons in the  synapses. Here signals travel from one  neuron to another through the release of  charged ions and neurotransmitters. These  molecules cross the gap to reach a small  mushroom-shaped spine on the “dendrite”  of the next neuron (see diagram, overleaf) . > 
The Human Brain I NewScientist: The Collection 1 15 
The buzz of thought 
Brain cells were once thought to communicate  using only electricity, but we now know that  tiny mechanical forces can also play a part 
NEURON 
 SYNAPSE 0 
DENDRITE 
AXON 
 NEURON 
 NERVE SIGNAL 
When they receive a nerve impulse via  chemical neurotransmitters, dendritic  spines bend and sway, pulling on coupling  molecules. These move the axon, altering  the release of neurotransmitters 
Dendritic spines may also communicate  with their neighbours by transferring  forces through a bed of proteins 
 This then relays the message onwards, starting  a new chain of activity. 
Importantly, dendritic spines are flexible -  a fact that piqued the interest of Francis Crick,  the co-discoverer of the structure of DNA. 
In the early 1980s, he hypothesised that the  spines might twitch as neurons exchanged  information, and that the changes in their  shapes might somehow alter the strength of  the signal passed between two neurons. These  movements, he speculated, could even play a  role in storing memories. No one had the  technology to watch the synapse in action at  the time but, by 1998, films made by powerful  microscopes showed that dendritic spines do 
indeed move and change shape within  seconds, just as Crick had predicted. 
The cogs and wheels driving these  movements were elusive, but a decade of  research has suggested several possibilities,  which Tyler recently outlined in a paper. 
We now know, for instance, that dendrites  are full of a protein called actin, which can  either assemble into large polymers or fall  apart into smaller units, depending on the  circumstances. This process generates forces  that may be strong enough not just to bend  the dendritic spine, but also to make it  contract or expand. 
Crucially, the dendritic spine on one side  of a synapse is linked to the axon terminal  on the other side by a chain of adhesive 
By controlling the flow of information  in this way, such mechanisms could be  crucial to tuning the neural networks that  make our brain hum. But finding out for  sure will be fiddly work - typically the  forces extend over just 10 micrometres. 
So neuroscientists are turning to cutting-edge  techniques such as magnetic particles, or  laser beams that exert minuscule forces to  tinker with these structures. 
Tyler’s interests, however, lie in a technique  that may allow him to tweak the mechanics in  the living brain. It started with a serendipitous  observation while he was a graduate student.  To liven up the long hours, Tyler played loud  music, with a subwoofer placed next to the  equipment recording electrical activity in 
"After 15 seconds of brain stimulation, it felt  like the buzz of a martini, and he continued  to feel really good for about 2 hours" 
proteins. This means that when a dendritic  spine moves, so does the axon terminal  -with potentially important consequences.  Taber Saif of the University of Illinois at  Urbana-Champaign and colleagues have  shown that the greater the force applied to an  axon terminal, the greater the number of  neurotransmitter molecules that are available  for release across the synapse. In this way, the  movements could alter the strength of the  signal and consequently the plasticity of the  synapse - key changes that might be a means  of storing information during learning  and memory. 
That’s not all. There could even be  communication between neighbouring  synapses. Tyler points out that neighbouring  dendritic spines lie upon the same bed of actin  and small rods called microtubules, which can  store elastic energy like a spring. As one spine  is stimulated, it seems to release chemicals  that trigger changes to this structure, which  pushes or pulls its neighbouring spines,  shifting the balance of forces in their synapses. 
No one has yet measured this transfer of  movement in action, but there is indirect  evidence that actin and microtubules do move  in response to a spine’s activity - and the scale  and speed of these movements would be more  than enough to tug or prod the neighbours,  says Tyler. If so, the mechanism would add  another route for signalling, perhaps helping  synapses to coordinate their activity as we  adapt to the situation at hand. 
neurons. To his surprise, he noticed spikes in  neural activity each time the subwoofer  boomed. '‘You’d see these synaptic events that  seemed to correlate with the bass,” says Tyler.  “It was saying, ‘Look! Mechanical vibrations  in brain tissue can cause changes in neural  activity’.” But it didn’t seem to be work worth  publishing, so Tyler let it be. 
Once in charge of his own lab at the Arizona  State University in Tempe, Tyler revisited the  issue. In 2008, his team took slices of mouse  hippocampus and subjected them to low  intensity, low frequency ultrasound waves -  pressure waves that should jiggle the brain’s  mechanical structures. As suspected, it  stimulated the neurons to fire, and increased  the amount of neurotransmitter released at  synapses. 
Ultrasound therapy 
The team next turned to live mice. By  stimulating the motor cortices of the mice  with pulses of ultrasound, they caused the  mice to twitch their tails, forepaws and  whiskers. They even implanted electrodes  in the brains of the mice to confirm that  spikes in neural activity accompanied the  ultrasound stimulation. 
The results seem to confirm the suspicions  that external mechanical forces can interfere  with processes in the brain, potentially  answering that mystery of the boxing  knockouts. If our synapses and neurons are 
16 1 NewScientist: The Collection | The Human Brain 
 tuned to fine mechanical forces, then a blow  to the head might disrupt their signalling,  forcing them to open up ion channels and  activate receptors. ‘‘One theory is that it  instantaneously opens all the potassium  channels or all the sodium channels,” says  Tyler. “That would render you unconscious.” 
The idea of the mechanical brain is  beginning to draw interest from other  researchers. Randy King, now at the US Food  and Drug Administration recently replicated  Tyler’s experiment to stimulate mice with  ultrasound when he was at Stanford  University in California. He believes that the  low intensity of the waves rules out the  possibility that the ultrasound is influencing  brain activity via other mechanisms, such as  heating. Instead, a real mechanical interaction  must be taking place. “It’s showing that we can  activate the brain non-invasively. And that  would be just huge for the entire field of  neuroscience,” says King. 
One reason for excitement is the possibility  of using ultrasound to treat brain disorders.  Unlike deep brain stimulation, which uses  implanted electrodes to treat Parkinson’s 
disease and depression, it wouldn’t require  surgery. It can also stimulate deeper areas  of the brain than other non-invasive  methods, such as transcranial magnetic  stimulation or transcranial direct current  stimulation. That’s because they use  electrodes on the scalp to pass electric or  magnetic fields through the skull, both of  which have a fairly shallow reach. 
Tyler has so far investigated whether  ultrasound stimulation could stop epileptic  seizures, in which lots of brain regions start  firing in synchrony. In one of their first  experiments along these lines, Tyler’s team  induced seizures in mice before applying  ultrasound pulses to their skulls. The sound  waves broke up the synchronous firing,  ending the seizure. He has high hopes that the  technique could be used to treat people with  head injuries, who often have seizures. “What  if you could develop a device that was an  automatic external defibrillator, except for  the brain, to treat brain injury?” says Tyler.  “That’s my vision.” 
The work has inspired Stuart Hameroff  to test the technique on himself. An 
anaesthesiologist and consciousness  researcher at the University of Arizona Health  Sciences Center in Tucson, Hameroff first  suggested to a colleague that they try the  therapy to treat chronic pain. The colleague  agreed, on one condition. “He looked at me  and said, ‘you have a nice shaped head, why  don’t we try it on you’,” says Hameroff. 
Mood lifter 
So they did. They applied ultrasound to  Hameroff’s temple for 15 seconds. Nothing  happened immediately. “But about a minute  later, I started to get a buzz, like I had a martini,  and felt really good for about 2 hours.” 
This led to a pilot study in which 31 people  who had chronic pain received 15 seconds  of ultrasound over their posterior frontal  cortex. Neither the doctor administering  the treatment nor the volunteer knew  whether they were using ultrasound or a  placebo. Those who received ultrasound  reported a slight improvement in their pain,  and their mood was enhanced for 40 minutes  after the treatment. 
Even so, Tyler and King agree there  are safety issues to be worked out before  ultrasound can be used as a treatment. 
King thinks we should be particularly  careful. “If you damage the brain, it can  be permanent. It’s not like muscle, which  if you damage might heal,” says King. 
“It has huge implications if something  goes wrong, and that would be bad for  the whole field.” 
Tyler is impatient to resolve those safety  issues quickly, because he believes the  benefits of fiddling with the mechanical  brain could stretch beyond the therapeutic  applications. For example, ultrasound can be  so finely focused that it should be possible to  study tiny regions individually. So you could  put a subject in an fMRI scanner and stimulate  an area to see how it talks to other parts of the  brain. That could help us to build maps of the  brain’s connectivity, and the functionality of  different regions, with unprecedented  resolution. 
So far, however, progress has been slow,  and Tyler is frustrated with the difficulties  of finding funding for big projects. “If you  want to change something, you can do it in  200 years making very small steps, or you  can do it in 10 to 15 years, making very large  leaps,” he says. And large leaps are what he  is after. “We could be on the cusp of having a  technique that will redefine the way we go  about conducting human neuroscience.” ■ 
 The Human Brain I NewScientist: The Collection 1 17 
 18 1 NewScientist: The Collection | The Human Brain 

Can mathematics help us find elegant  order behind the apparent pandemonium  of our minds, asks Colin Barras 
H ow could an equation or formula ever  hope to capture something as complex  and beautiful as the human mind? In a  sense we Ve long been describing the brain  with numbers - 86 billion neurons, 1200 cubic  centimetres, 1400 grams. But you might  expect that more ambitious attempts  to explain the brain with mathematics would  be doomed to failure. 
Yet over the last few years, neuroscientists  have built a mathematical framework for  understanding many aspects of the brain. 
In the same way that Newton’s laws of motion  describe the dance of the stars and planets  in the night sky, mathematical principles  are now revealing telling patterns in the  melee of our minds. What’s surprising is  just how often the brain’s dynamics mimic  other natural phenomena, from earthquakes  and avalanches to the energy flow in a  steam engine. 
The equations we end up with describe  everything from the brain’s structure to the  generation of our thoughts and feelings. They  may even help us begin to understand the  nature of consciousness itself, join us as we  explore the five laws that rule the mind. 
SMALL WORLD, BIG CONNECTIONS 
If you stretched out all the nerve fibres  in the brain, they would wrap four times  round the globe, Crammed into the skull,  you might thinkthis wiring is a tangled  mess, but in fact mathematicians know  its structure well - it is a form of the  "small-world network", 
The hallmark of a small-world  network is the relatively short path  between any two nodes. You've  probably already heard of the famous  "six degrees of separation" between  you and anyone else in the world, which  reflects the small-world structure of  human societies. The average number  of steps between any two brain regions  is similarly small, and slight variations in  this interconnectivity have been linked  to measures of intelligence. 
That may be because a small-world  structure makes communication  between different areas of a network  rapid and efficient. Relatively few long-  range connections are involved - just  1 in 25 nerve fibres connect distant  brain regions, while the rest join  neurons in their immediate vicinity. 
Long nerve fibres are costly to build and  maintain, says Martijn van den Heuvel  at the University Medical Center in  Utrecht, the Netherlands, so a small-  world-network architecture may be  the best compromise between the 
cost of these fibres and the efficiency  of messaging. 
The brain's long-range connections  aren't distributed evenly over the brain,  though. Van den Heuvel and Olaf  Sporns of Indiana University  Bloomington recently discovered that  clusters of these connections form a  strong "backbone" that shuttles traffic  between a dozen principal brain regions  (see diagram, page 21), The backbone  and these brain regions are together  called a "rich club", reflecting the  abundance of its interconnections. 
No one knows why the brain is home  to a rich club, says van den Heuvel, but  it is clearly important because it carries  so much traffic. That makes any  problems here potentially very serious,  "There's an emerging idea that perhaps  schizophrenia is really a problem with  integrating information within these  rich-club hubs," he says. Improving rich-  club traffic flow might be the best form  of treatment, though it is not easy to  say how that might be achieved. 
What is clear for now is that this  highly interconnected network is the  perfect platform for our mental  gymnastics, and itforms a backdrop for  many of the other mathematical  principles behind ourthoughts  and behaviour, > 
The Human Brain I NewScientist: The Collection 1 19 
TEETERING ON THE EDGE OF CHAOS 
The familiar chords of our favourite song reach  the ear, and moments later a neuron fires.  Because that neuron is linked into a highly  connected small-world network, the signal can  quickly spread far and wide, triggering a cascade  of other cells to fire. Theoretically it could even  snowball chaotically, potentially taking the brain  offline in a seizure. 
Thankfully, the chances of this happening  are slight. "Perhaps 1 per cent of the population  will experience a seizure at one time in their  lives," says John Beggs at Indiana University  Bloomington. This suggests there is a healthy  balance in the brain - it must inhibit neural signals  enough to prevent a chaotic flood without  stopping the traffic altogether. 
The sweet spot 
An understanding of how the brain hits  that sweet spot emerged in the 1970s, when  Jack Cowan, now at the University of Chicago,  realised that this balance represents a state  known as the critical point or "the edge of chaos"  that is well known to theoretical physicists.  Cascades of firing neurons - or "neural  avalanches" - are the moments when brain  cells temporarily pass this critical point, before  returning to the safe side, he said. 
Avalanches, forest fires and earthquakes  also result from systems lying at the critical  point, and they all share certain mathematical  characteristics. Chief among them is the  so-called "power law" distribution, which  means that bigger earthquakes or forest fires  happen less often than smaller ones according  to a strict mathematical ratio; an earthquake  that is 10 times as strong as another quake is also  just one-tenth as likely to happen, for instance. 
How does the brain compare? In Z003, Beggs  and Dietmar Plenz, both then at the National  Institute of Mental Health in Bethesda, Maryland,  checked whether neural activity matches  Cowan's theory by using a grid-like array of  electrodes hooked to a chunk of rat cortex. 
Sure enough, they found that an excited neuron  passed its signal to just one neighbour on  average, which is exactly what you would  expect of a system on the edge of chaos: any  more and the system would lie in permanent. 
 full-blown disorder. Importantly, larger neural  avalanches do occur, but they are much rarer. Like  earthquakes and forest fires, their frequency  drops with size according to the precise ratio  predicted by a power law. 
Since Beggs's initial work, further functional  MRI scans have suggested that the same kind of  edge-of-chaos activity can be found at much  larger scales, across the whole human brain;  indeed, computer models suggest it might be a  result of the small-world structure of the brain. 
Balancing on the edge of chaos may seem  risky, but the critical state is thought to give the  brain maximum flexibility - speeding up the  transmission of signals and allowing it to quickly  coordinate its activity in the face of a changing  situation. Some of the researchers are beginning  to wonder whether certain disorders might arise  when the brain veers away from this delicate  balance. "There's now some evidence that people  with epilepsy are not at this critical point," says  Beggs. "Just as there's a healthy heart rate and  a healthy blood pressure, this may be what you  need for a healthy brain." 
"Avalanches, forest fires i  cascades of firing neuroi  share certain mathemat  characteristics" 

 86 
billion neurons 
ZO I NewScientist: The Collection | The Human Brain 

hyper-connected  hubs that help direct  traffic flow 
The rule of the rich 
The brain's wiring allows for the rapid transmission of  information, with a set of particularly well-connected  hubs, known as the rich club, directing much of the  traffic between different parts of the brain 
 This group may be crucial for integrating all  the thoughts and feelings that make up our  conscious experience 

 KNACK FOR THE FUTURE 
From its crackling electrical storm of activity, the  brain needs to predict the surrounding world in a  trustworthy way, whether that be working out  which words are likely to crop up next in a  conversation, or calculating if a gap in the traffic  is big enough to cross the road, What lies behind  its crystal-ball gazing? 
One answer comes from an area of mathematics  known as Bayesian statistics, Named after an  18th-century mathematician, Thomas Bayes, the  theory offers a way of calculating the probability of a 
future event based on what has gone before, while  constantly updating the picture with new data. For  decades neuroscientists had speculated that the  brain uses this principle to guide its predictions of  the future, but Karl Friston at University College  London took the idea one step further, 
Friston looked specifically at the way the brain  minimises the errors that can arise from these  Bayesian predictions; in other words, how it avoids  surprises. Realising that he could borrow the  mathematics of thermodynamic systems like a 
steam engine to describe the way the brain  achieves this, Friston called his theory "the free  energy principle". Since prediction is so central  to almost everything the brain does, he believes  the principle could offer a general law for much,  if not all, of our neural activity - the brain's  eguivalent of E-mc^ in terms of its descriptive  power and elegance. 
So far, Friston has successfully used his  free energy principle to describe the way  neurons send signals backwards and forwards  in the visual cortex in response to incoming  sights. Fie believes the theory could also explain  some of our physical actions. For instance, he  has simulated our eye movements as we take  in familiar or novel images, suggesting the way  the brain builds up a picture with each sweep of  our gaze to minimise any errors in its initial  perception. In another paper he turned his  attention to the delicate control of our arm as  we reach for an object, using the free energy  principle to describe how we update the muscle  movements by combining internal signals from  the turning joints with visual information. 
Others are using the concept to explain some of  the brain's more baffling behaviours, Dirk De Bidder  at the University of Otago's Dunedin School of  Medicine in New Zealand, for instance, has used the  principle to explain the phantom pains and sounds  people experience during sensory deprivation. 
Fie suggests they come from the neural processes  at work as the brain casts about wildly to predict  future events when there is little information  to help guide its forecasts, 
Friston points out that the brain's ability to  update its thoughts and make predictions about  the world depends on a finely tuned system,  "Signals in the brain decay," he says, and if the  decay is too fast, an important hypothesis may  disappear by the time the brain makes its next  observation and generates a new prediction," For  this reason, the free energy principle relies on the  brain's ability to hang in that "critical state" on the  edge of chaos, "Criticality is almost mandated by  the Bayesian brain," says Friston, > 
The Human Brain I NewScientist: The Collection I Z1 
170 
kilometres of  nerve fibres 
,000 
PREYING ON YOUR MIND 
As your mind flits from thought to thought it  may seem as if dozens of sensations and ideas  are constantly fighting for your attention. In fact  that's surprisingly close to the mark; the way  different neural networks compete for dominance  echoes the battle for survival between a predator  species and its prey, and the result may be your  wandering mind. 
Mikhail Rabinovich at the University of  California in San Diego and Gilles Laurent then at  the California Institute of Technology in Pasadena,  were the first to notice this strange dynamic. 
They were studying the neuronal activity in  the antennal lobe - the insect equivalent of the  olfactory bulb in the mammalian brain - as locusts 
 experienced different odours. Rabinovich  expected the activity to flatline when they got  used to each smell, but he was wrong. "Even when  the scent stimulus was constant, the activity of  the principal neurons in the antennal lobe  changed with time," he says. 
Looking closely, Rabinovich noticed that the  pattern of activity was not random, but similar  to the form described by mathematicians Alfred  Lotka and Vito Volterra in the early ZOth century.  The Lotka-Volterra equations, also known as  predator-prey equations, are a key ecological  tool for predicting fluctuations in populations  of interacting species. A predator near-exhausts  its supply of prey, and so starves while its prey  recovers, and the cycle starts again. 
Rabinovich dubs such perpetual fights  "winnerless competitions" and he says they occur  in the brain as well. Here, though, the fight is  not between just two competitors, but between  multitudes of cognitive patterns. None ever  manages to gain more than a fleeting supremacy,  which Rabinovich thinks might explain the  familiar experience of the wandering mind. 
"We can all recognise that thinking is a process,"  he says. "You are always shifting your attention,  step-by-step, from one thought to another  through these temporary stable states." 
People with psychiatric conditions  might benefit from the work. In the past,  conditions like attention-deficit hyperactivity  disorder (ADHD) were studied by looking at  quick snapshots of neural activity. But  Rabinovich's work gives neuroscientists a tool  to make sense of the brain's responses as they  evolve with time, potentially explaining why  the attention drifts in unusual ways. Working  with Alexander Bystritsky at the University  of California in Los Angeles, Rabinovich has  already shown that his equations can accurately  describe the neuronal activity associated with  both ADHD and obsessive compulsive disorder.  "They are very convenient for diagnosing the  disorders," he says. 
The competing activity between brain  regions resembles the perpetual fight  between predator and prey" 
ZZ I NewScientist: The Collection | The Human Brain 
THE SUM OF CONSCIOUSNESS 
 "An experience's  colours, smells and  sounds are impossible  to isolate from  one another" 
10 " 
synapses in 
the brain 
Getting to grips with consciousness may seem  like a step into the unknown, or even the  unknowable, but Giulio Tononi at the University  of Wisconsin-Madison was not daunted, 
The first challenge was to find a good definition  of consciousness by boiling it down to its most  essential elements, He reasoned that each  moment of awareness is a fusion of information  from all of our senses. An experience's colours,  smells and sounds are impossible to isolate from  one another, except through deliberate actions  such as closing your eyes. At the same time, each  conscious experience is a unigue, never-to-be-  repeated event. In computational terms, this  means that a seat of consciousness in the brain  does two things: it makes sense of potentially vast  amounts of information and, just as importantly,  it internally binds this information into a single,  coherent picture that differs from everything we  have ever - or will ever - experience. 
Perhaps the best way to understand this  is to consider the difference between the brain  and a digital camera. Although the screen seems  to show a complete image to our eyes, the  camera just treats the image as a collection of  separate pixels, which work completely  independently from one another; it never  combines the information to find links or patterns.  For this reason, it has very low "integration", and  so according to Tononi's theory, it isn't conscious.  The brain, on the other hand, is constantly  drawing links between every bit of information  that hits our senses, which allows us to be aware  of what we see. 
Physicists haven't paid much attention to  measuring how much information a physical  system can hold on to and integrate, so Tononi  worked out the eguations himself The result is a  guantity known as "phi", "Now I could go back to  neurobiology with this tentative theory: any seat  of consciousness must have a high level of phi, and  other systems must not," says Tononi, 
Some accepted anatomical findings gel with this  tentative theory. For instance, we know that the  cerebral cortex is crucial for conscious experience -  any damage to the brain here will have an effect on  your mental life. Conversely, the cerebellum is not 
necessary for conscious awareness, which was  something of a puzzle given that it contains more  than twice as many neurons as the cerebral cortex.  When Tononi analysed the two regions using his  theory, it all made sense: the cerebral cortex may  have fewer neurons, but the cells are very well  connected to one another. They can hold large  amounts of information and also integrate it to  generate a single coherent picture - the level of  phi is very high. The cerebellum is more like the  digital camera: it may contain more neurons  than the cerebral cortex, but there are fewer  interconnections and so no coherent picture -  the level of phi is low, in other words, 
"I've been studying consciousness for 25 years,  and Giulio's theory is the most promising," says  Christof Koch at the California Institute of  Technology in Pasadena, "It's unlikely to be the final  word, but it goes in the right direction - it makes  predictions. It moves consciousness away from  the realm of speculative metaphysics," (For more on  consciousness, see Chapter 6, page 84,) 
Lights out 
Tononi's theory can also explain what happens  when we fall asleep or are given an anaesthetic -  through experiments he has shown that the level  of phi in the cerebral cortex drops as our  consciousness fades away. 
This makes sense when we consider all of the  ideas emerging from the field of computational  neuroscience. The cerebral cortex is home to  many of the highly interconnected "rich club" hubs,  which may explain why it is so good at integrating  incoming information. Neural signals zip freely  through these interconnections to generate  conscious experiences. Fall asleep, though, and  the neural signals within the cerebral cortex  slip further away from the critical point vital  for neural communication. The physical  interconnections remain, but traffic no longer  flows through them. The Bayesian brain loses  its ability to make sense of the world around it - all  of the thoughts engaged in the brain's winnerless  competitions fade to black. 
The various strands of the computational  neuroscience story come together powerfully. 
Are they the final word in our understanding of  the brain? "They're undoubtedly flawed in some  way - no one is being naive," says Beggs,  Nevertheless, he and others think neuroscience  is poised to become a numbers game, "We'll find  out in a few years," he says, "In the meantime,  it's certainly a fun journey," ■ 
The Human Brain I NewScientist: The Collection I 23 
PAWELJONCA 
 ?■ -’*■ 4V 



 Hidden depths 
The vast majority of brain research is drowning  in uncertainty. It's time to build a more complete  understanding of the mind, says Ingfei Chen 
I T’S FOUR in the afternoon when I meet John  loannidis, but lines of fatigue are deepening  under Jiis eyes. He’s exliausted witJi jet lag  after a whirlwind tour of 20 European cities,  where he’s been lecturing and brainstorming  with colleagues. In a corner of his office, I spot  two oddly shaped bags, which hold gear for his  sport of choice, epee fencing. It seems a fitting  hobby for this soft-spoken professor, who is a  crusader for good science. 
Statistical logic and careful scrutiny of  evidence are the weapons that loannidis  nimbly wields. His previous targets have  included spurious claims about drugs and  other medical treatments from clinical trials  backed by the pharmaceutical industry. Now  his gaze has turned to the brain. Joining a  growing army of critics, he has documented  serious flaws in the ways that many - if not  the vast majority of- neuroscience studies  are designed, analysed and reported. 
That should perhaps be a warning whenever  we read headlines about studies capturing  snapshots of the brain on “love”, “fear”,  “religion” or “politics”. It turns out that  many of those colourful brain scans may  offer little more than mirages, obscuring  the true picture of the human mind in action. 
Worse still, the problems are not just  confined to a few misleading brain-scan  reports. From experiments investigating the  action of genes and individual molecules to  studies linking brain structure to mental 
health, question marks are now hanging over  the whole field of neuroscience. “Currently, 
I wouldn’t put much trust in most of the  literature,” says loannidis, who is an  epidemiologist at the Stanford University  School of Medicine in California. 
Amid these concerns, it might seem as if our  understanding of the brain is set to disappear  in a fog of uncertainty, and you will find many  observers in the popular press who are now  bashing “neuromania”. But it’s important not  to forget the advances of the last century. And  while the tough conclusions of loannidis and  his colleagues are certainly reason to reassess  our knowledge, their insights should only lead  to more fruitful efforts in uncovering the  mind’s mysteries. “Neuroscience is moving  forward,” says Chris Baker of the US National  Institute of Mental Health (NIMH). As the fog  clears, more nuanced theories should, in time,  emerge in sharp relief. 
Although philosophers have long pondered  the origins of thought, it was the invention  of functional magnetic resonance imaging  in 1991 that really sparked our love affair  with neuroscience. fMRI is based on studying  the flow of blood in the brain, with more  blood rushing to the areas working hardest.  The scans reveal bright splotches of neural  activity inside people’s heads as they engage  in different tests of their capacity to see,  feel, remember or think. We were instantly  seduced by these technicolour insights. > 
The Human Brain | NewScientist: The Collection | 25 
92 PER CENT 
OF SCANS 
EXAMINING THE ANATOMY OF CONDITIONS 
LIKE AUTISM MIGHT HAVE MISSED THE T _  ANSWER, WITH MANY REPORTING Ll~  THAT WEREN'T REALLY THERE ^ ^ 

But there were always some quiet  grumblings about whether transient neural  activity could reveal much about complex  mental processes or behaviours. But the brain-  scan backlash only really exploded into public  view in late 2008, when psychology researchers  Edward Vul and Harold Pashler at the  University of California, San Diego, published a  critique of what they cheekily dubbed “voodoo  correlations”. The pair had been baffled by a  profusion of highly implausible fMRl results  strongly linking behaviours or traits to one  or just a few specific areas of the cortex.  Examining 53 fMRl studies, Vul, Pashler and  their colleagues concluded that half of them  reported untrustworthy results that were  simply too good to be possible, thanks to  “seriously defective” methods. 
Double dipping 
To understand why, first consider that a  typical fMRl scan of the whole brain contains  as many as 100,000 three-dimensional pixels,  called voxels - a vast amount of data to  analyse. Researchers use specialised software  to find clusters of voxels that light up when  participants view images that trigger, say,  empathy or emotional responses. However,  the challenge is that true signals can be  obscured by underlying random fluctuations  in those voxels - a bit like the static noise on an  untuned TV. fMRl software tries to filter that  out but it cannot work miracles, so many areas  will inevitably show some increased activity  simply by fluke. 
Ideally, neuroimagers should use two  sets of scans. One set is for identifying which  voxel clusters are highly activated during the  experiment. Having found these regions, you  then look at them specifically in the second set  of scans to confirm that the response wasn’t  due to random fluctuations, and then measure  its size. But Pashler and Vul found that many 
researchers instead made the mistake of using  just one data set for both the initial and final  analysis, which allows the random noise to  inflate an apparent link to a behavioural  response or trait. Such “double-dipping” led  researchers to some exciting but premature  conclusions, including overly simplistic  ideas about the origin of personality traits.  Neuroticism, for instance, was chalked up to  stronger activity in a pair of almond-shaped  regions called the amygdalae, which are  known to be involved in fear and other  negative emotions. 
Confirming that the problem was spread  far and wide. Baker and colleagues at NIMH  looked at all the fMRl studies published in five  top journals in 2008. Of the 134 papers, 42 per  cent had made double-dipping errors. The  flawed method is also common in studies of  single-neuron responses in animals, as well as ,  in genetic analyses. Baker’s team noted. ! 
Neither critique went as far as overturning |  the broader conclusions of the studies in | 
question. “It doesn’t invalidate everything,” | 
Baker says of his work, “but it raises question :  marks.” | 
The voodoo correlations study, in particular, ;  set off an angry back-and-forth of rebuttals. ? 
One cause of criticism was that Vul and Pashler f  named offending studies, which some said  was overly aggressive. “It came across as a little  bit nasty,” says fMRl specialist Russell Poldrack  of the Stanford School of Medicine in  California, although he admits that it got  people’s attention. “I don’t know that a paper  that was written more nicely would have  necessarily had as much impact.” 
“We spoke frankly and just kind of had a  little fun,” says an unrepentant Pashler, while  acknowledging that since he and Vul do not  themselves do brain-scanning research, they  had not needed to worry about how their next  studies or grant applications would be received. 
After the furore died down, many fMRl 


 researchers realised that the critiques were  essentially right. Voodoo correlations and  double-dipping appear to be less common  now, and the idea that you can map complex  personality traits to a few specific regions like  the amygdalae is increasingly considered to be  “a pipe dream”, says cognitive neuroscientist  Tal Yarkoni, also at the University of Texas at  Austin. Personality traits are now thought  to be associated “with lots of different brain  regions interacting in complex ways”, he says. 
But as researchers patched up those holes  in their methods, other equally serious 
Z6 1 NewScientist: The Collection I The Human Brain 
Brain scans promised  to pinpoint our  personality traits 
 concerns began to emerge. A jaw-dropping  study from the University of Michigan  published in 2012, for instance, demonstrated  that an fMRI experiment could be analysed  in nearly 7000 ways - and the results could  vary hugely. With so much flexibility,  neuroimagers can unintentionally (or indeed  deliberately) analyse their experiments in a  way that yields the most favourable results.  One tongue-in-cheek report showed that even  a dead salmon’s brain could appear to be  ‘‘thinking” inside a scanner if the wrong  techniques were used (see image, page 28). 
The most alarming wake-up call came when  loannidis published a paper showing that the  problems run much deeper than flawed fMRI  studies. Working with Katherine Button and  Marcus Munafo of the University of Bristol,  UK, and others, he analysed 48 review papers  that collectively had scrutinised 730 studies  examining the risk factors and treatments for  neurological disorders such as Alzheimer’s  disease and chronic pain. The experiments  used many different methods, including  measures of cognitive functioning, gene  testing and clinical trials. From this, the team  estimated the odds that each study was able to  detect something that was truly there to be  discovered - otherwise known as its  “statistical power”. 
The results were grim. The average overall  power was about 20 per cent, largely because  the number of subjects used in the experiments  was simply too small for reliable results to  come out of them, even if they passed the  standard statistical tests. In other words, four  out of five studies might have been missing  the actual biological effect or mechanism  sought, and therefore reported false negatives. 
But that’s not all. The low power delivers  a double whammy of uncertainty: not only  are you likely to be missing the evidence  even if it’s under your nose, but “if you do  detect something that seems to be significant. 
it has a higher chance of being a false  positive”, loannidis says. 
The picture was even more troubling when  looking specifically at structural MRl studies  that investigated the physical anatomy of the  brain (as opposed to the changing neural  activity that shows up in functional MRI). The  average statistical power of studies linking  structural abnormalities to mental health  conditions such as depression or autism was a  feeble 8 per cent - meaning that 92 per cent of  the investigations would have failed to make  true discoveries and in many cases detected  something that was not really there. 
Data dredging 
As in many fields, published studies in  neuroscience tend to show more positive  results than would be expected, something  loannidis and his colleagues confirmed  through further work examining bias in fMRI  investigations and animal studies of  neurological illnesses. Some of this bias arises  simply because negative studies are not  published very often. But another possibility,  loannidis says, is “data dredging” - researchers  fishing through and analysing subsets of their  results until they find something favourable. 
To know exactly which or how many of the  reports are right or wrong would mean  attempting to replicate all the findings, which  usually isn’t done. But based on his experience  with other fields, loannidis thinks the vast  majority of neuroscience studies published in  recent years are likely to be incorrect.  “Neuroscience is in serious trouble,” he says. 
What is to be made of this damning  assessment? For a start, it does not mean  ditching everything. Conclusions that have  stood the test of time are more believable,  and loannidis is not questioning textbook  knowledge of brain anatomy and function.  Injury from a stroke in Broca’s area, for > 
THERE ARE 7000 WAYSof 
ANALYSING BRAIN SCAN DATA, LEADING  TO CONFUSING OR CONFLICTING RESULTS 
The Human Brain I NewScientist: The Collection I Z7 
 Fishy findings: a dead  salmon seems to spring  to life in a scanner 
instance, obviously impairs the ability  to speak, so we can be sure of its role in  language production. Such big effects can  be discerned even by studying just a  small number of people, and have been  corroborated by many strands of evidence. 
It is probably the newer findings that we  should take with a pinch of salt, particularly as  neuroscientists tease apart the finer processes  that are likely to underlie many complex  mental tasks, behaviours or differences in  personality traits. Such phenomena are  much harder to measure, and because the  patterns of brain activity are so faint, a lot more  data must be collected before the true signal  can be detected above the background noise. 
Unsurprisingly, loannidis has ruffled many  feathers, although many neuroscientists agree  with the gist of his findings. The big concern  is that he is being too alarmist. In 2014, a  report by neuroscientist Martha Farah at the  University of Pennsylvania in Philadelphia  picked apart the criticisms and concluded that  whilst they have some validity, they should  not cast doubt on neuroimaging as a whole.  Similarly, Poldrack is concerned that the ideas  may be “spun into this kind of global nihilism  that all of neuroscience is bullshit”. Certainly,  no one should be saying that. Many fMRI  findings have held up over time, including  observations that the frontal cortex is always  activated during short-term recall and that the  hippocampi are active during sleep, perhaps  as they work to consolidate memories. 
“I wouldn’t keep doing science if every time  I found something, I later found that it was  unreliable,” Poldrack says. While there are  certainly problems, he adds, “many of us are  doing what we can to try to address them”. But  some researchers worry that if governments  get the wrong message, they may starve labs  of funding, killing revolutionary research. 
For his part, loannidis is adamant that  transparency is the best way to keep the  public’s confidence in science. “I don’t like  hiding things under the carpet. I prefer to  identify issues and solve them.” 
And he does have a prescription to cure  many of those ills. For example, bigger sample  sizes - such as in rigorous multi-centre  studies - are often the most obvious way to  increase statistical power when looking at  small, hard-to-detect effects. Alternatively, for  some research questions, studying a few  subjects can still produce reliable results, if 
you gather enough data from each person.  Increasing the size of fMRI studies can be  challenging, however, because it costs around  $500 per hour to use a machine, though  arguably the funds are better spent on larger  but fewer studies. 
Another approach is to encourage brain  scientists to disclose their data and replicate  others’ findings and so weed out some of  the false positives. For instance, in 2010,  Poldrack and several colleagues launched  the web-based Open fMRI Project, which lets  investigators upload their raw data sets so that  others can reanalyse and validate their results.  Replication is a thankless task, though, since  researchers don’t get promoted for being right  or confirming ideas - they get promoted for  publishing intriguing new results. 
If all this seems like a struggle,  neuroscientists may take heart from genetics  research, which faced a similar upheaval a 
$4.5 BILLION - 
THE FUNDING FOR AN INITIATIVE  TO PROBE THE BRAIN'S CIRCUITS 
28 1 NewScientist: The Collection | The Human Brain 
 decade ago after a flood of small studies  overemphasised the role of particular genes  in disease and personality traits. Now, with  much bigger studies and consistent rules for  reporting and sharing data, that field has gone  from a replication rate of i per cent to more  than 90 per cent reliability, loannidis says. 
Indeed, a couple of large initiatives are  already tackling these challenges. In the  $40 million Human Connectome Project,  neuroscientists across a dozen institutions  are building a detailed wiring diagram of  the brain's circuitry (see ‘‘The greatest map of  all", page 10). They are scanning a large  sample - 1200 people - using fMRI and  a technique called diffusion imaging, and the  data will be openly shared. It promises to give  us our best view yet of the way the brain's  anatomy shapes thought and behaviour. 
Building bridges 
The BRAIN Initiative, meanwhile, is getting  $4.5 billion of US government funding to  develop techniques that will pick out the finer  circuits in the brain, bridging the gaps  between studies examining single neurons  and the large-scale fMRI maps. That will  include rethinking or refining existing  techniques, such as “optogenetic" methods  that allow you to control neuronal activity  with pulses of light, as well as inventing  entirely new technologies. 
All this may mean we will finally be able  to appreciate the complexity of the brain,  lack Gallant at the University of California,  Berkeley, for instance, points out that there  is so much more for us to see if only we  pay attention to the bigger picture. At the  moment, it's as if we've been peering at the  brain through a lousy microscope, he says -  partly due to the fact that most MRI data  is thrown away to focus on a few selective  results. “We're missing huge things," he says. 
Consider our understanding of face  recognition - a knotty task for the brain,  given just how much our expressions can  vary. Typical fMRI experiments would  compare just two conditions, such as showing  volunteers pictures of faces versus places.  Based on such investigations, neuroscientists  used to think that one region of the brain - the  so-called fusiform face area (FFA) - uniquely  responds whenever a person sees a face. But  the story has grown more complicated as  further research turned up a network of other  regions that cooperate to recognise faces. 
And as neuroimaging grows more  sophisticated, so too does our view of the 
A SCEPTICS GUIDE  TO NEUROMANIA 
While the neuroscientist's toolkit comes  into question (see main article), there  are also many common pitfalls in the  way the results are interpreted to  explain complex traits and behaviours. 
For instance, you will often read  about differences in brain activity or  structure that appear to be linked to  psychopathic tendencies, with studies  showing that convicted murderers have  reduced activity in areas associated  with empathy when they see images  of people suffering. Defence lawyers  might use this as evidence that a  defendant had diminished  responsibility, and some pundits have  even pondered whether it might be  possible to identify people who are  more likely to commit a crime. But  there are probably plenty of people  who show similar quirks in the brain  scanner, with no criminal intentions.  (Indeed, doctors are thought to tone  down their own empathic response to  pain to help them manage a patient's  distress.) And differences in a 
murderer's brain may be the result of  their past brutality, not the cause. 
Similar"neurocentric" arguments are  sometimes used when talking about  drug abuse as a "brain disease". There  is no doubt that addictive substances  do create long-lasting changes to our  neural circuity, but as psychiatrist Sally  Satel and clinical psychologist Scott  Lilienfeld point out in their Z013 book  Brainwashed, this view can devalue  many other factors, including stress,  the influence of friends, and access to  drugs. In this way, it might distract  addicts from psychological strategies  such as avoiding cues that trigger a  craving. Satel and Lilienfeld also point  out that placing all the blame on the  brain's circuits could diminish people's  belief in their own self-control - about  80 per cent of addicts do manage to  kick the habit. 
Brain science clearly has big potential  for medicine and the law. But it is crucial  to realise that our neurology need not  ruleourfate. David Robson 
brainscape. Gallant's experiments, for instance,  collect hours of brain-scan data from a few  subjects as they watch movie trailers, which  allows the team to track the brain's changing  reaction to an immensely wider range of  stimuli. The researchers then skip some of the  usual processing steps that lead to data loss, so  that they can draw as much meaningful data  as possible from the experiment. 
Their results show the FFA to be even more  intricate than previously imagined,  suggesting that it can be subdivided into three  separate areas. While these all respond to  faces, each is also involved in processing  different categories of other objects, such as  flags, crucifixes and snakes - making the FFA  something like a Swiss Army knife for visual  recognition. That doesn't mean the initial  view of the “face area" was wrong. It was just  incomplete. Gallant says. (Although others  point out more studies will be necessary to  confirm that the same principles apply to the  average brain.) 
Such deepening complexity in the  understanding of the brain is to be expected  as one's microscope gets better and better. But  Gallant says it just goes to show that we are 
still at the tip of the iceberg when it comes to  the prevailing theories. Even with all the work  on visual perception, for instance, no one can  yet build a robot that sees like a human, let  alone a machine that accurately recognises  people. And phenomena like emotions or  moral judgement are even murkier. 
Will the understanding come eventually?  Gallant remains an optimist, pointing out that  fMRI was invented only 20 years ago. Back  then, nobody knew how best to design the  experiments and analyse the vast data they  generated, but he thinks the lessons of past  mistakes, such as double-dipping, will be  learned. The study of the brain just gets better  all the time, he says. 
Even the epee-fencing loannidis agrees that  our understanding of the brain will eventually  correct itself where wrong. But the question  is, how quickly? “If it takes several years for  something to be refuted, that could be a real  waste of effort." As he points out, “the brain is  more complex than almost any other system". 
Few questions are as profound as the  mysteries between our ears, and there is no  doubt that solving them will need the finest  tools wielded with the greatest skill, 
The Human Brain I NewScientist: The Collection I Z9 
 30 1 NewScientist: The Collection | The Human Brain 

CHAPTER TWO 
THOUGHT 
T ry, if you can, to imagine a life  without thought. For a human  being it wouldn’t be much of an  existence. Thoughts fill our every waking  moment, and whether they are insightful,  banal, playful or bizarre, there is no  denying that thinking comes naturally to  us. We might say that thought is to  human beings what flight is to eagles and  swimming is to dolphins. 
But it is one thing to think and quite  another to understand the nature of  ° thought, lust as eagles fly without any  I grasp of aerodynamics and dolphins  I swim without understanding fluid  o mechanics, so most of us think without 
having any insight into its nature.  Thinking maybe commonplace, but it is  quite rare to think about thought itself. 
So what is thought? That is a  surprisingly difficult question to answer.  Neuroscience, psychology, philosophy  and other disciplines have approached it  from their various perspectives, but  thought has not received as much  sustained attention as it deserves. 
Perhaps part of the explanation for this  is that thought is an extremely varied and  complex phenomenon. We can think  about an incredible variety of things: 
objects, people, places, relationships,  abstract concepts, the past, the future,  real things and imaginary things. We  can think about nothing at all, and  even think about thought itself. 
The exercise of thought is also elusive,  although there are some things we can  say about it. We use thought to solve  problems and invent things - but how  much control do we have over it? And is  there a limit to what we can think of? 
To make some progress with these > 
Conscious or unbidden, thoughts fill our heads  from morning to night. But what are they, and what  exactly is thinking? Join philosopher Tim Bayne on  a journey into the fantastic, elusive and ceaseless  world our minds create 
The Human Brain I NewScientist: The Collection I 31 
 THOUGHT 
EXPERIMENTS 
questions we first need to make some  distinctions, because the term  ‘‘thought” can refer to three quite  different features of mental life. 
In one sense, thought refers to a type of  mental event. To think of something is to  bring it to mind in some way. In another  sense it refers to a certain kind of mental  faculty, lust as there are faculties  associated with perception and language,  so too there is a mental faculty - or  perhaps faculties - associated with the  capacity to think. And in a third sense it  refers to a certain kind of mental activity,  lust as you can be engaged in the activity  of looking for something or listening to  something, so too you can be engaged in  the activity of thinking about something. 
Let’s first consider thought as a mental  event. What are thoughts, and what  distinguishes them from other kinds of  mental events, such as perceptual  experiences and bodily sensations? 
Suppose you are having a bonfire. You  can see the flames and feel the heat. These  are purely perceptual events. You may  also find yourself wondering what would  happen if the wind changed direction, or  how combustion works. These events are  prompted by your perceptual experience,  but they are not themselves forms of  perception. They are thoughts. 
More than a feeling 
Although the distinction between  perception and thought is intuitive, no  one has been able to characterise it  unequivocally. One way is to argue that  thoughts involve the deployment of  concepts, whereas sensory states do not. 
It is possible to see a bonfire without  possessing the concept of a bonfire, but  impossible to think about it. However,  this view is contentious. For one thing,  some theorists argue that concepts are  implicated in both thought and  perception. And it has proved difficult  to say precisely what concepts are. 
Another way to distinguish thought  from perception is by their conscious  character: “what it’s like” to think about a  bonfire is very different from what it’s  like to perceive a bonfire. But here too we  run into difficulties. Although everyone  agrees that thinking about a bonfire is  subjectively different from perceiving  one, pinning down why is tricky. 
The issue is further complicated by the  fact that thoughts are often unconscious. 
3Z I NewScientist: The Collection | The Human Brain 
Sometimes an experiment is impossible. But  that doesn't necessarily stop us from doing it -  in our heads. Such thought experiments are one  of the most impressive demonstrations of the  power and scope of human thought. 
The ancient Greeks knew about thought  experiments in mathematics. Today they are  most common in physics. Galileo described the  first which dealt with the speed at which stones  of different sizes would fall when dropped. 
The most famous is Schrbdinger's cat which  demonstrates the implausibility of a certain  interpretation of quantum mechanics using a  classic reductio adabsurdum (see page 36). 
Erwin Schrodinger was later proved right in a  real-world version of the experiment. 
Einstein performed another famous one at  age 16, when he imagined himself running  alongside a beam of light. This flight of fancy, he  later said, sowed the seed for special relativity. 
A vivid imagination was also important to  Kary Mullis, who shared a Nobel prize in  chemistry for inventing a way of copying DNA. 
He did it, he said, by imagining himself "down  there with the molecules". 
Thought experiments can also help us  explore moral issues. The trolley problem, for  instance, asks whether would you act to avert an  accident that is about to kill 10 people if your  deliberate intervention would save the 10, but  intentionally kill 1 other person. 
Thought isn't always a reliable guide to reality,  however. In 1935, Einstein, Nathan Rosen and  Boris Podolsky imagined the properties of two  "entangled" particles. They used the absurdity of  the outcome to claim that quantum theory must  be incomplete. However, we've since developed  the technology to do the experiment for real, and  in this case reality turns out to be truly absurd. 
Sometimes thought leads nowhere,  as in considerations of what happens to  information absorbed by a black hole. In Z004,  Stephen Hawking conceded a bet in the face  of "proof" that information is not destroyed by  the black hole. It turns out he gave in too soon:  the question is still wide open. 
Nonetheless, "thought experiments are  incredibly useful for distilling the essential  elements of a situation", says Dave Wineland of  the National Institute for Standards and  Technology (NIST) in Boulder, Colorado. Wineland  won the Nobel prize in physics in 201Z for his  experimental work in quantum physics. Despite  our impressive array of experimental abilities,  thought experiments can still challenge and  improve our understanding of the world,  he says. Michael Brooks 
Consider when you are trying to solve a  problem, and something simply comes to  mind, or you sleep on it and find that it is  miraculously solved in the morning. So  you can’t just rely on their conscious  character to distinguish thoughts from  other mental events. 
How about thought as a mental faculty?  A useful starting point is Rene Descartes’s  description of thought as a “universal  instrument which can be used in all kinds  of situations”. What did he mean? 
Consider, again, the difference between  perceiving and thinking. In order to  perceive, say, an apple, there must be a  causal connection between you and it.  Light must be reflected from it and be  processed by your visual system. No such  connection is required to think about an  apple. You can think about one whenever  you want, whether or not it is there. This  is what allows the faculty of thought to be  used “in all kinds of situations”. 
Another feature of thought that  Descartes points us to is its scope.  Perception only provides access to a  limited range of things. Vision can tell us  that an apple is red or that it is falling, but  only a creature with the power of thought  is able to appreciate the fact that it  originated in western Asia or that it has 
/ 

 more genes than a human. We can think  about objects that are far removed from  us in space and time, about the concrete  and the abstract, about the past and the  future, and about what does and what  does not exist. The reach of human  thought may not be completely  unlimited (more on this below), but there  is no doubt that it vastly outstrips the  reach of perception. 
A final feature of the faculty of thought  is its integrative nature. It enables us to  relate one state of affairs to another and  appreciate connections between them. 
Consider a famous episode in the  history of medicine. While working at a  hospital in Vienna in the 1840s, physician  Ignaz Semmelweis noticed that the  incidence of childbed fever was much  higher in one maternity ward than  another. He also noticed that this ward  was staffed by medical students who  performed autopsies. This led him to  wonder whether the students might be  contaminating the women with  “cadaverous material”. He tested this  hypothesis by requiring the students to  wash their hands with calcium  hypochlorite - known to remove the  smell of corpses - before visiting the  maternity ward. This led to a dramatic 
“Much of the va  comes from our a  organise thoughts 
drop in deaths from childbed fever. 
Semmelweis’s discovery, which laid the  foundations for the germ theory of  disease, required two acts of integration:  not only did he make a hitherto-  unnoticed connection, he also thought of  a way of testing the resulting hypothesis. 
We make use of the problem-solving  powers of thought on a daily basis.  Whether planning a holiday, attempting  to juggle work and children, or just  trying to figure out the best way to  get from A to B, we spend much of our  lives thinking about the relationship  between events. 
Let us now turn to thought as a  mental activity. In other words, let us  consider thinking. 
Although thoughts can occur in  isolation, it is perhaps more common 
There is a certain kind  of delight to be had in  following an undirected  train of thought 
for them to come in trains. There  are two types of trains of thought.  Sometimes thoughts are related  associatively : one thought naturally  and effortlessly leads to another, like a  game of word association. For example,  thoughts of Switzerland might  trigger thoughts of skiing which might  lead to thoughts of snow which might  lead to thoughts of Christmas... and so  on. Associative thinking is familiar from  daydreams and other forms of reverie. 
Although there is a certain delight to be  had in following this kind of train of  thought, the power of thinking arguably  resides in something more systematic:  the fact that it enables us to use evidence  and logic. Indeed, the term “thinking” is  sometimes reserved for this activity (see  “Thought experiments”, left). 
Consider the chain of thought  “Socrates is a human”, “all humans are  mortal” and “Socrates is mortal”. The  components are inferentially connected,  for if the first two are true then so too is  the third. 
Much of the value of thinking comes  from our ability to organise thoughts  into coherent trains to “see” what follows  from what. In other words, much of our  interest in thinking concerns reasoning. 
The nature of thought 
Having distinguished various aspects of  thought, we can now turn our attention  to the nature of thought. What is it? 
It used to be believed that thought  required some kind of non-physical  medium - a soul or an immaterial  mind. Modern theorists typically reject  this in favour of a materialist account,  according to which thought involves  only physical processes. 
There are three main motivations for  this. The first is because it can account for  correlations between states of the brain  and states of thought. From the mild  changes that follow from drinking > 
The Human Brain I NewScientist: The Collection I 33 
 caffeine to the more radical ones that  result from brain damage, it is clear that  the state of the brain is intimately  correlated with our capacity to think. 
A second motivation is its ability to  account for the causal role of thoughts in  the world. Thoughts are both caused by  physical events and are the cause of them.  Seeing a train pull into the station might  lead you to think “time to go”, which  leads you to pick up your luggage and  board the train. 
Third, the materialist account of  thought does justice to the continuity of  nature. We assume that humans evolved  from animals that lacked thought.  Although we cannot rule out the  possibility that this involved the  emergence of some kind of non-physical  medium, it is more plausible to assume  that the evolution of thinking creatures  can be fully explained by changes in the  structure of physical systems. 
None of these reasons is decisive alone,  but taken together they provide a strong  case for the physicalist conception of  thought. So how might thoughts manifest  as physical phenomena in the brain? 
For most of human history, thought  has been essentially private, accessible  only through speech and behaviour.  There are various theories about how  thoughts arise (see “Thinking like a  computer”, right). But developments in  “brain decoding” are letting researchers  study thought more directly. 
Mental arithmetic 
Using fMRI, neuroscientists are starting  to be able to use information about a  person’s brain states to determine  what they are thinking. In one study,  volunteers were asked to choose between  two options - “add” or “subtract” - before  being presented with two numbers on  which to perform their chosen operation.  The researchers were able to tell with  70 per cent accuracy whether the subjects 
had decided to add or subtract, thereby  reading their hidden intentions. Other  researchers have had similar success  working out what people are looking at,  or even what they are dreaming about,  from their brain activity alone (for more  on this, see “The 1 in dreaming”, page 124). 
Although impressive first steps, it is  worth emphasising the limitations of  decoding studies. First, the range of  thoughts that participants are told to  entertain is artificially restricted. In the  add/subtract study, there were only two  possibilities. In the real world the range  of thoughts is not constrained like this,  and thus the task of interpreting a  person’s brain activity in everyday life  will be vastly more difficult. 
Brain decoding also requires a lot of  advance preparation, mapping  correlations between people’s thoughts  and their brain activity. Researchers  cannot read thoughts that are not already  included in their database. Brain imaging  is thus still a long way from decoding the  language of thought, let alone designing  a machine that can read people’s thoughts. 
One hotly contested question about  the nature of thought is the role that  language plays. There is a wide range of  opinion. One end of the spectrum is that  we think in language. At the other is the  view that language has no role in thought  other than to allow us to communicate  our thoughts. The truth is likely to lie  somewhere in the middle. 
One way into this debate is to consider  what kinds of thoughts non-human  animals can entertain. Researching this is  difficult, but there are at least three  domains in which evidence of animal  thought has been found: numbers, social  relations and psychological states. 
Many species have some capacity to  track mathematical properties. In one  study, rats were trained to press a lever  when they heard two tones and a  different lever when they heard four.  They were trained to do the same in  response to flashes of light. When  presented with one tone and one flash,  they pressed the first lever, indicating  that they had understood the stimulus as  “two events”. In response to two tones  and two flashes of light, they pressed the  other lever. 
A number of species can also compare  quantities quite accurately. In one  experiment, chimpanzees were given a  choice of two trays of chocolate chips. On  each tray were two piles - a 3-chip pile  and a 4-chip pile, say, or a 7-chip pile and a  2 chip pile. The chimps were thus faced  .. h the pro blem of determining which 
t ray had the most chips overall. Although 
N 
u 
'For most of history, LmUlght  h d^pee n private; accessible  oiWffaough speech 
X 
7/ 
34 1 | Ttie Human Brm , 
Ik' 
# 
 THINKING LIKE  A COMPUTER 
There are many theories that try to  explain how thought can arise from  a material object such as a brain. 
One of the most successful is the  computational theory of thought  (CTT), which envisages thinking as  being like the workings of a computer. 
CTT concerns the nature of both  thoughts and thinking. In a nutshell,  it proposes that thoughts are  sentences in a "language of thought"  and that thinking involves transitions  between these sentences governed  purely by their "formal properties"  not their meaning. 
Let's unpack that a bit. A formal  property is a property that something  has by virtue of its physical form. The  formal property of a word is its shape,  not its meaning. The English word  "monkey" and the French word "singe"  differ in their formal properties but  mean the same thing. 
What does it mean to say that  thoughts are sentences in a language  of thought? Consider the thought  "Marcel has a monkey". Just as the  sentence itself is built up out of  linguistic symbols that have  meanings, CTT holds that the thought  is built up out of "thought symbols",  each of which carries a distinct  meaning. One symbol will refer to  Marcel, another to monkeys, and a  third to the relation "having". 
CTT explains thinking by appealing  to the formal properties of these  symbols. It posits a mechanism that is  sensitive to the formal properties  (whatever they are) and implements a  set of rules about how to manipulate  these symbols without knowing what  they mean. Thought thus operates  much like an automated address  reader for letters. Although the  machine doesn't know anything about  Mr Smith or Mr Jones, it is able to  ensure that their mail gets to them  because it is sensitive to the formal  differences between Smith and Jones.  Tim Bayne 

tJiey struggled when the quantities were  very similar, they were generally good at  choosing the right tray. 
Chimps can also grasp simple  fractions. When shown half a glass of  milk, they are able to point to half an  apple and ignore three-quarters of an  apple in order to gain a treat. 
Overall, the evidence suggests that a  number of species can represent  quantities up to three in exact ways and  larger quantities in approximate terms.  These representations are thought-like in  so far as they are stimulus-independent  and systematic. 
A second domain in which there is  evidence of animal thought concerns  social rank. Some of the most intensive  research on social cognition has been  done on female baboons, whose  complex social world involves a  two-tiered hierarchy. Families are  ranked relative to each other, and  females within each family are too. 
This ranking - which is fluid - plays a  pivotal role in baboon society, and it is no  surprise that baboons have complex  representations of their social world. For  example, a baboon may be more startled  by a sequence of calls that represents a  subordinate threatening a dominant  baboon from a different family than it is  by a sequence of calls that represents an  equivalent conflict within a family, even  when the difference in overall rank order  is identical. 
There are a number of ways in which a  baboon’s understanding of its social  world has thought-like features. First,  social status is not directly obvious in the 
Unlike otheranimals, humans  are born into a social world of  other thinkers 
environment, and keeping track of it  requires the deployment of a theory  about it. Second, it appears to be  somewhat open-ended: a baboon can  represent a great number of possible  relations between members of her  troop including ones that are  unexpected. These features provide  good justification for describing the  baboon’s representation of its social  world as a form of thought. 
A third area in which thought-like  representations have been found is in the  understanding of psychological states.  Primates, at least, seem to be able to  determine what others can see - and thus,  perhaps, what they know - on the basis of  what they are looking at. They will follow  the gaze of others to locate the object of  their attention and will remove food items  from the line of sight of other animals. In  experiments, subordinate chimpanzees  will only take food items that dominant  chimps cannot see - dominant  chimpanzees typically take all the food  and punish subordinates that challenge  them - suggesting they understand the  connection between seeing and knowing. 
state of mind 
There is also evidence that primates can  monitor their own states of mind. In a  series of studies, monkeys learned to  perform a test that required them to  discriminate between two shapes. When  they answered correctly they received  food; when they got it wrong they got  nothing, and were obliged to wait a while  for the next trial, which they didn’t like to  do. The monkeys learned that by pressing  a button they could opt out of a test and  move immediately to the next. The  monkeys’ use of the opt-out suggested  that they were assessing how difficult  each test was, for they opted out only on  difficult trials. 
It seems clear that non-human species  use thought-like processes in a number of  situations. Even so, they do not come close  to matching the range and sophistication  of human thought. What accounts for the  uniqueness of human thought? The  answer appears to be related to language. 
Consider the following experiment  involving Sheba, a chimpanzee trained to  use numerals to represent items. Sheba  was offered two plates of food, one large  and one small. To obtain the larger plate,  she had to point to the smaller one. > 
The Human Brain I NewScientist: The Collection I 35 

k control our thinking,  or is it something that just  happens to us?" 
Although she understood the rule she  wasn’t able to overcome her instinct to  point towards the larger plate - until the  plates were covered and numerals  representing the number of treats were  placed on top of them. 
The use of symbols allowed Sheba to  transcend her normal abilities and do  something much smarter: disengage  her thought from perception. This  “decoupling” is a striking feature of  human thought, and may be facilitated  by (and perhaps even require) the use of  symbols, especially language. 
Another example of the transformative  power of symbols is provided by a study of  chimpanzees trained to use plastic tags to  represent sameness and difference. A pair  of cups might be associated with a red  triangle (sameness) whereas a cup and a  shoe might be associated with a blue  circle (difference). 
Once the chimps had grasped this  idea they could then - and only then -  go on to appreciate higher-order  relations of sameness and difference.  They understood that two pairs, such as  cup-cup and cup-shoe, have the relation  of difference. The researchers suggest  that the tags enabled the chimps to  perform this task because they could  transform a higher-order task into a  simpler one of determining whether  the symbols associated with each pair  were the same. 
As the philosopher Andy Clark has  remarked, “experience with external tags  and labels thus enables the brain itself...  to solve problems whose level of  complexity and abstraction would  otherwise leave us baffled.” 
Language facilitates thought in other  important ways. It is a tool that allows  us to augment our powers of thought. 
By putting thoughts into language we  are able to take a step back and subject  them to critical evaluation. There is  good reason to suppose that much  distinctively human thought involves. 
or is at least enabled by, language. 
Another distinctive feature of human  thought is that it occurs in a social  environment. We are born into a  community of thinkers, and we learn to  think by being guided by those who are  experts. Indeed, childhood is an extended  apprenticeship in thinking. We learn both  what to think and how to think. 
Perhaps most importantly of all,  cultural transmission allows the best  thoughts of one generation to be passed  on to the ones that follow. Unlike other  species, whose cognitive breakthroughs  usually have to be rediscovered anew by  each generation, we are able build on the  thoughts of our ancestors. We inherit not  just the contents of their thoughts, but  also methods for generating, evaluating  and communicating thoughts. 
What thinking involves 
Another key question that arises from  considering thought as an activity  concerns the kind of control we have over  it. Is thinking an intentional and  controlled activity, or is it largely passive?  Do we control it, or is it something that  just happens to us? 
Sometimes thought is controlled by  the application of a rule. Mathematical  and logical operations, for example, are  rule-based, and philosophers have  invented many other systematic “thinking  tools” to help them think more clearly  (see “Tools for thought”, right). But this is  an unusual kind of activity, and most  episodes of thinking involve no rule. 
Suppose that I ask you why  democracies tend not to wage war  against other democracies. (It is  often said that democracies have never  waged war on one another but that is  not true.) If you have not already  considered this question, you may  need to think about it. 
What precisely does that involve? If  your experience is anything like mine. 
TOOLS FOR  THOUGHT 
"Thinking is hard." So says Daniel Dennett at  the start of his recent book Intuition Pumps  and Other Tools for Thinking. As a philosopher,  he speaks from experience. 
But just as artisans don't have to go about  their business with their bare hands, so  thinkers don't have to work unaided. Over the  centuries, philosophers have invented a range  of handy tools to make thinking a bit easier.  Some are useful only in very specific  circumstances, such as calculus or probability  theory. Others are more broadly applicable.  Here is a selection of Dennett's favourite  thinking tools 
REDUCTIO AD ABSURDUM  Literally, reduction of an argument to  absurdity. The trick here is to take an assertion  or conjecture and show that it leads to  preposterous or contradictory conclusions.  Homeopathy's claim that water has a  "memory" of substances that were once  dissolved in it can be challenged in this way by  pointing out that tap water has had millions of  different substances dissolved in it. 
OCCAM'S RAZOR 
Don't invent a complicated explanation for  something if a simpler one will do. This is only  a rule of thumb but it has proved extremely  useful in science, such as when heliocentrism  swept away an elaborate system of epicycles  to explain the movement of the planets. (Not  to be confused with Occam's broom, which is  the intellectually dishonest trick of ignoring  facts that refute your argument in the hope  that your audience won't notice.) 
STURGEON'S LAW 
Named after sci-fi author Ted Sturgeon, who  felt that his genre was unfairly maligned by  critics. "They say '90 per cent of it is crud'," he  complained. "Well, they're right... but 90 per  cent of everything is crud." This is a useful tool  when criticising a discipline, school of thought  or art form. If you can't land a punch on the  good 10 per cent, leave it alone. 
"SURELY" AND RHETORICAL QUESTIONS  Whenever you encounter these in a text, stop  and think. The author usually wants you to  skate over them as if the claim is so obvious as  to be beyond doubt, or the answer self-  evident. The opposite is often the case. 
Graham Lawton 
36 1 NewScientist: The Collection I The Human Brain 
 you simply put the question to yourself...  and wait for something to spring to mind.  Sometimes nothing much happens; on  other occasions, your unconscious comes  up with something intelligible. Either  way, there is no rule that you can  consciously follow in order to generate  the required thoughts. 
On the whole, thinking often doesn’t  seem to extend much beyond putting  questions to yourself and waiting for  your unconscious to answer. The role of  consciousness in such cases seems to be  that of a minder whose job is to ensure  that one’s train of thought doesn’t  wander off topic. 
We are, however, surprisingly poor at  keeping our mind-wandering tendencies  in check. In one study, people were asked to  read a passage in their heads and monitor  themselves for “zoning out”. They were  interrupted at random to check whether  they were still reading the passage. It  turned out that the participants zoned  out a lot and, what’s more, were generally  not even aware that they had. 
In fact, a significant amount of  thought is undirected - that is, not  aimed at any specific goal or problem.  This kind of thought takes many forms,  ranging from simply wandering away  from a task to the spontaneous,  unbidden thoughts that pop into your  head during rest or routine chores. 
Not thinking of white  bears... not thinking of  white bears... damn 
Until recently, undirected thought was  seen as a useless and wasteful aspect of  our internal mental lives. But research  now suggests that it is a normal and even  necessary aspect of thought. Brain activity  during mind-wandering is reminiscent of  that seen when people are deliberately  engaged in creative thinking. It may be  that, paradoxically, undirected thought is  when we get our best thinking done. 
There is also evidence that attempting  to control the direction of a stream of  thought can be counterproductive. In a  famous study, psychologist Daniel  Wegner asked participants not to think  about white bears for a 5-minute period.  He found that this group reported more  thoughts about white bears than did a  second who had been instructed to think  about white bears. 
The limits of thought 
So although we have some conscious  control over the direction of our  thoughts, it is far from unlimited. And if  we have relatively little control, perhaps  we also have relatively little  responsibility for what we think. 
Nonetheless, the potential of human  thought is clearly very great. It is not  limited in the way our physical and  perceptual abilities are. We cannot see or  visit distant tracts of space and time, for  example, but we can think about them. 
Are there limits to what our minds  can grasp? The idea that certain aspects  of reality are beyond us might at first 
seem implausible. After all, there  doesn’t seem to be any aspect of the  world that we cannot think about. 
Is there any reason to take the possibility  of cognitive limits seriously? 
There is. Given that the machinery of  human thought is part of our biology,  there is every reason to suspect that it  suffers from the kinds of bugs and blind  spots that constrain other biological  systems. It is doubtful whether  chimpanzees possess the ability to think  about quantum mechanics, for example.  Perhaps that is one of the limitations of  lacking language. But if there are parts of  reality that are inaccessible to other  thinking species, why should we assume  that no part is inaccessible to us? 
It is one thing to grant that some  aspects of reality lie beyond our grasp,  but quite another to identify what they  might be. Is it possible to demarcate the  borders of human thought? 
The question might seem absurd. You  might argue that if a certain thought is  unthinkable then we can’t think about it,  let alone know that it is unthinkable. But  there is nothing paradoxical about  attempting to determine where the limits  lie. The key involves distinguishing  thinking about a thought from actually  thinking it. lust as we can know what we  don’t know - the known unknowns - so  too we might be able to think about what  we cannot think: the thinkable  unthinkables, you might say. 
Wherever the boundaries of human  thought might lie, there is no doubt that  we are very far from having reached 
em. There are thoughts - deep,  important and profound thoughts - that  no human being has yet entertained.  Thought has taken us a long way; who  knows where it will lead. ■ 
 Human Brain I NewScientist: The Collection I 37 
JODIE GRIGG5/FLICKR SELECT/GETTY 
 Most of us talk to ourselves throughout the day,  but what Is this Inner speech and how does it  shape our thoughts and decisions? Psychologist  Charles Fernyhough listens in 
Life in the  chatter box 
I T CAN happen an3HArhere. I can be driving,  walking by the river or sitting quietly in front  of a blank screen. Sometimes suddenly,  sometimes gradually and imperceptibly, 
1 become conscious of words that no one else  can hear, telling me things, guiding me,  evaluating my actions. I am doing something  perfectly ordinary - 1 am thinking - and it takes  the form of a voice in my head. 
If you ask people to reflect on their own  stream of consciousness, they often describe  experiences like this. Usually termed inner  speech, it is also referred to as the inner voice,  internal monologue or dialogue, or verbal  thought. But although philosophers have long  been interested in the relationship between  language and thought, many believed that  inner speech lay outside the realms of science.  That is now changing, with new experimental  designs for encouraging it, interfering with it  and neuroimaging it. We are beginning to  understand how the experience is created in  ^ the brain; its subjective qualities - essentially,  i what the words “sound” like; and its role in  S processes such as self-control and self-  Q awareness. The voice in our head is finally 
revealing its secrets, and it is just as  powerful as you might have imagined. 
Much of modern research has been  inspired by the long-neglected theories of  L. S. Vygotsky, a Russian psychologist whose  career unfolded in the early days of the Soviet  Union. Vygotsky only studied psychology for  about lo years before his untimely death from  tuberculosis in his late thirties - a fact that  has led some to call him “the Mozart of  psychology”. Starting with observations of  children talking to themselves while playing,  Vygotsky hypothesised that this “private  speech” develops out of social dialogue with  parents and caregivers. Over time, these  private mutterings become further  internalised to form inner speech. 
If Vygotsky was right, inner speech should  have some very special properties. Because it  develops from social interactions, it should  take on some of the qualities of a dialogue,  an exchange between different points of view.  Vygotsky also proposed that inner speech  undergoes some important transformations  as it becomes internalised, such as becoming  abbreviated or condensed relative to external 
38 1 NewScientist: The Collection | The Human Brain 
speech. For instance, when hearing a loud  metallic sound outside at night and realising  that the cat is to blame, you probably wouldn’t  say to yourself, “The cat has knocked the  dustbin over”. Instead, you might just say,  “The cat”, since that utterance contains all the  information you need to express to yourself. 
Partly because Vygotsky’s work was  suppressed by the Soviet authorities,  it was a long time before his ideas became  well known in the West, and even longer  before researchers tested whether people  actually report these qualities in their  inner speech. In the first such study,  conducted in 2011 at Durham University, UK,  my colleague Simon McCarthy-jones and I  found that 60 per cent of people report that  their inner speech has the to-and-fro quality  of a conversation. 
Eavesdropping on thoughts 
So-called “self-report” methods have  their limitations, not least that people  are being asked to comment retrospectively  on their inner experience. Another  method, offering a richer picture of people’s  thoughts during a particular time period,  was developed by psychologist Russell  Hurlburt at the University of Nevada, 
Las Vegas. It involves participants being  trained to give very detailed descriptions  of their own inner experience in response  to random cues from a beeper. Such studies  have shown that people often report a train  of thought unfolding more quickly than  circumstances ought to have allowed, and yet  not seeming rushed, which could be taken as  evidence for the compression of sentences  that Vygotsky postulated. 
Vygotsky’s theory also suggests some  possibilities about the way inner speech is  created in the brain. If it is derived from  external speech, as he proposed, both might  be expected to activate the same neural  networks. Sure enough, long after his death,  fMRl studies have linked inner speech to the  left inferior frontal gyrus, including a region  called Broca’s area, which is known to be  important for speech production. 
Quite how much our inner and outer speech  overlap remains a matter of debate. According  to one view, inner speech is just external  speech without articulation: the brain plans  an utterance, but stops short of kicking our  muscles into action. If that is the case, our  internal voice should resonate with the same  qualities of tone, timbre and accent as our  ordinary external speech. > 
The Human Brain | NewScientist: The Collection | 39 
"Dramatic difficulties can come from brain damage  that silences the inner voice, One such individual  reported a lack of self-awareness after her illness" 
There are some hints that this may be the  case. In their lab at the University of  Nottingham, UK, psychologists Ruth Filik and  Emma Barber recently asked participants to  read limericks silently in their heads. One was: 
There was a young runner from Bath, 
Who stumbled and fell on the path; 
She didn't get picked, 
As the coach was quite strict. 
So he gave the position to Kath. 
The other limerick read: 
There was an old lady from Bath, 
Who waved to her son down the path; 
He opened the gates. 
And bumped into his mates. 
Who were Gerry, and Simon, and Garth. 
Importantly, some of the participants had  northern English accents, with short vowels  (pronouncing “Bath” to rhyme with “Kath”),  while the others had the long vowels of a  southern accent (“Bath” rhyming with  “Garth”). By tracking the volunteers’ eye-  movements, the researchers showed that  reading was disrupted when the final word of  the limerick did not rhyme in that volunteer’s  accent - when a southerner read “Bath” then  “Kath”, for instance. Although this study  suggests that inner speech does indeed have  an accent - and presumably other qualities of  our spoken voice - one concern is that the  inner speech we produce when reading is not  necessarily the same thing as our everyday,  spontaneous inner speech, which means that  more naturalistic studies are needed. 
So much for the subjective qualities of inner  speech. What, if anything, does it actually do?  Vygotsky proposed that words in inner speech  function as psychological tools that transform  the task in question, just as the use of a  screwdriver transforms the task of assembling  a shed. Putting our thoughts into words gives  them a more tangible form which makes them  easier to use. It may also be that verbal  thought can allow communication between  other cognitive systems, effectively providing  a common language for the brain. 
One of Vygotsky’s most enticing predictions  was that private and inner speech give us a  way of taking control of our own behaviour. 
by using words to direct our actions. While  driving up to a roundabout in busy traffic,  for example. I’ll still tell myself, “Give way  to the right”, especially if I’ve just been  driving overseas. Knocking out the systems  responsible for inner speech should therefore  impede our performance on certain tasks that  require planning and control, offering a  powerful test of the hypothesis. 
Such experiments typically require  participants to repeat a word to themselves  out loud to suppress their verbal thoughts  while they perform a task (a technique known  as articulatory suppression). Using this set-up,  Jane Lidstone, one of my colleagues at Durham  University, looked at the performance of  children aged 7 to 10 on a planning task known  as the Tower of London, which involves  moving coloured balls around between three  sticks of differing lengths in order to match a  given pattern. Lidstone found that children  performed worse if they had to repeat a word  out loud, compared with trials in which they  instead tapped repetitively with one of their  feet. Similar findings have emerged from  studies with adults. Alexa Tullett, now at the  University of Alabama in Tuscaloosa, and  Michael Inzlicht of the University of Toronto  in Canada gave student participants a classic  test of control known as the Go/No-Go task,  which required them to press a button the  moment they saw a yellow square pop up on  the screen, but to remain still when they saw a  purple square. It is a considerable test of  impulse control, and, as predicted, the  students were less accurate during  articulatory suppression, compared with  when they were doing a spatial task. Although  experiments like these seem artificial, they  allow researchers the kind of control over  conditions that good science demands to test  something like self-control. 
Pep talks 
So we know that inner speech has a role in  regulating behaviour, but could it also have  a role in motivating it? The research on  children’s private speech (Vygotsky’s  precursor of inner speech, remember)  shows that it frequently has an emotional or  motivational flavour. Athletes often give  themselves pep talks before, during and after 
performances. In our study of the quality of  inner speech, McCarthy-Jones and I found  that two-thirds of students reported using  internal speech that either evaluated their  behaviour or served to motivate it. 
Inner speech may even help us to become  aware of who we are as individuals. Some  philosophers have proposed that awareness  of inner speech is important for  understanding our own mental processes,  an aspect of what psychologists call  metacognition. Children typically do not  become aware of their own inner speech  until around age 4, although it is uncertain  whether that shows their inability to reflect  on their own thought processes, or the fact  that inner speech is not yet fully internalised  by that age. At Mount Royal University in  Calgary, Canada, psychologist Alain Morin  has found that people who use inner speech  more often show better self-understanding.  “Inner speech allows us to verbally analyse  our emotions, motives, thoughts and  behavioural patterns,” he says. “It puts to  the forefront of consciousness what would  otherwise remain mostly subconscious.” 
 40 1 NewScientistTheCollection I The Human Brain 
While researchers are still gathering the  evidence, these results certainly suggest that  the voice in the head is important to many  cognitive processes. But what about people  who, for various reasons, don’t talk to  themselves in the usual way? As you might  expect, deaf people who communicate in  sign language often talk to themselves in  sign too. People with autism, meanwhile,  who often have problems with linguistic  communication, seem not to use inner speech  for planning, although they do use it for other  purposes such as short-term memory. A more  dramatic difficulty comes from damage to the  language areas of the brain, which can silence  some people’s inner voices. One such  individual, neuroanatomist Jill Bolte Taylor,  reported a lack of self-awareness after a stroke  that damaged her language system -  supporting Morin’s view that verbal thinking  maybe important for self-understanding. 
Lending an ear to the differences between  people might also tell us more about the dark  side of inner speech, following a growing  understanding that our internal monologue is  not always beneficial to our well-being. When 
we worry and ruminate, we often do it in  words, and our inner speech may contribute to  anxiety and depression by keeping thoughts  in the head that would be better off discarded.  Inner speech may play its biggest role,  however, in an experience that is often  associated with other forms of mental  disorder. People with certain psychiatric  diagnoses (particularly schizophrenia), and  also a small minority of people who do not  have a mental illness, report the experience of  hearing a person speak when there is no one  present. Voice-hearing, or auditory verbal  hallucination, is an enigmatic phenomenon  whose cognitive and neural bases are not yet  well understood. One prominent theory  proposes that it occurs because the individual  produces an utterance in inner speech that  they do not recognise as their own. The  result is that a bit of speech that was actually  self-generated becomes attributed to another  person: an alien voice. 
Various lines of evidence converge to  support this view. An early observation was  that people who hear voices produce very  slight activation in their articulatory muscles 
when their voices occur. Cognitive behaviour  therapy to treat voice-hearing often focuses  on blocking the phonological loop, by  articulatory suppression or listening to music,  so that the rogue inner speech cannot be  generated. But the phenomenon of voice-  hearing is undoubtedly more complicated  than this. McCarthy-Jones, now at Macquarie  University in New South Wales, Australia,  notes that '‘while inner speech appears to be  the basis of some voices, others are actual or  mutated memories of earlier life-events  (often traumatic ones)”. Many researchers,  particularly those associated with the  worldwide Hearing Voices Movement, now  believe that voices have important meanings  for the individual, and therefore that they  need to be understood rather than suppressed. 
A shower of words 
There is much more we need to learn about  inner speech’s roles in our thinking and  behaviour. Some insights may come from  people who, without any disability, don't  report any inner speech at all. For some of  these people, it may be that inner speech is  present, but that it is so condensed and  abbreviated that it no longer seems very like  language. It will also be interesting to note the  consequences when people try to suppress  their inner speech (and indeed all conscious  thought) through varieties of meditation. 
One thing we can be sure about is that inner  speech takes many forms. Some will be good  for explicit self-regulation and motivation;  others will be closer to a kind of deep thinking  with no particular sound quality. In fact,  understanding inner speech better will help  us to be clearer about what we mean by the  nebulous term “thinking”, and in this way  make progress with some long-standing  philosophical problems about how language,  cognition and consciousness work together. 
When I think about my own inner speech, 
I keep coming back to Vygotsky’s ideas about  “condensation”. Sometimes I catch myself in  the middle of a full-blown argument with  myself, debating things from different points  of view. Most of the time, though, the  experience is more fragmentary: thoughts  and feelings that are close to being put into  language, but are not yet quite the kind of  speech you would hear spoken out loud.  Vygotsky likened this transition of thought  into speech to “a cloud shedding a shower of  words”. Condensed or expanded, this rich  internal dialogue must hold clues to  understanding the distinctively creative,  flexible properties of human thought. ■ 
'According to one  view, our inner  voice should  resonate with  the same tone,  timbre and  accent as our  normal speech" 
The Human Brain I NewScientist: The Collection I 41 
 A century of clashes and discoveries has upended assumptions  and revealed fascinating paradoxes, Intelligence is definitely not  what most of us had imagined, says Linda Gottfredson 
WHAT DO 10  TESTS MEASURE? 
A century ago, British psychologist  Charles Spearman observed that  individuals who do well on one  mental test tend to do well on all  of them, no matter how different  the tests' aims, format or content. 
So, for example, your performance  on test of verbal ability predicts  your score on one of mathematical  aptitude, and vice versa. 
Spearman reasoned that all  tests must therefore tap into some  deeper, general ability and he  invented a statistical method  called factor analysis to extract this  common factor from the web of  positive correlations among tests. 
This showed that tests mostly  measure the very same thing, which  he labelled the general factor of  intelligence or "g factor". In essence,  g equates to an individual's ability to  deal with cognitive complexity. 
Spearman's discovery lay neglected  in the US until the 1970s, when  psychologist Arthur Jensen began  systematically testing competing  ideas about g. Might g be a mere  artefact of factor analysis? No, it lines  up with diverse features of the brain,  from relative size to processing speed.  Might g be a cultural artefact, just  reflecting the way people think in  western societies? No, in all human  groups - and in other species too -  most cognitive variation comes from  variation in g. 
Jensen's analyses transformed the  study of intelligence, but while the  existence of g is now generally  accepted, it is still difficult to pin  down. Like gravity, we cannot observe  it directly, so must understand it from  its effects. At the behavioural level, g  operates as an indivisible force - a  proficiency at mentally manipulating  information, which undergirds  learning, reasoning, and spotting and  solving problems in any domain. At  the physiological level, differences in  g probably reflect differences in the  brain's overall efficiency or integrity.  The genetic roots of g are even more  dispersed, probably emerging from  the joint actions of hundreds if not  thousands of genes, themselves  responding to different environments. 
Higher g is a useful tool, but not a  virtue. It is especially handy when life  tasks are complex, as they often are in  school and work. It is also broadly  protective of health and well-being,  being associated with lower rates of  health-damaging behaviour, chronic  illness, post-traumatic stress disorder,  Alzheimer's and premature death. 
Higher g helps an individual  get ahead socioeconomically,  but it has little connection with  emotional well-being or happiness.  Neither does it correlate with  conscientiousness, which is a big  factor in whether someone fulfils  their intellectual potential. 
The "three stratum  theory" of intelligence  recognises that there is a  single general cognitive  ability, g, with added  input from a range of  broad and narrow  abilities 
General 
intelligence 
factor 
Broad abilities 
Fluid intelligence  Crystalised intelligence  Processing speed  Broad retrieval ability  Broad cognitive speed  Broad visual perception  Broad auditory perception  General memory and learning 
Narrow abilities 
64 specialised aptitudes or skills that each  relates to a specific broad ability 
4Z I NewScientistTheCollection I The Human Brain 
QUANTIFYING INTELLIGENCE 
Average IQ score distribution by population 
 70 80 90 100 110 120 130 
IQ score 
The first intelligence quotient (IQ) test  was born of a desire to help the most  vulnerable. In 1904, the French  Ministry of Education commissioned  psychologist Alfred Binet to find a  practical way to identify children who  would fail elementary school without  special help. Binet assembled  30 short objective questions on tasks  such as naming an everyday object  and identifying the heavier of two  items. A child's performance on these,  he believed, would indicate whether  their learning was "retarded" relative  to their peers. His invention worked  and its success spawned massive  intelligence-testing programmes on  both sides of the Atlantic. 
Organisations turned to IQ tests to  screen large pools of applicants:  military recruits fortrainability,  college applicants for academic  potential and job applicants for  employability and promotability. The  tests were eagerly adopted at first as  a way to select talent from all social  levels, but today their use can be  considered contentious, partly  because they do not find equal  amounts of intelligence everywhere. 
Nevertheless, intelligence testing  continues because it has practical  value. Many colleges, employers and  the armed services still use paper-  and-pencil or computer-based  intelligence tests to screen large  groups of applicants. The gold 
Alfred Binet invented the IQ test  to identify those schoolchildren  most in need of help 
standard, however, is the orally  administered, one-on-one IQ test,  which requires little or no reading and  writing. These include the Stanford-  Binet and Wechsler tests, which take  between 30 and 90 minutes and  combine scores from areas such as  comprehension, vocabulary and  reasoning to give an overall IQ. These  batteries are used to diagnose, treat  or counsel children and adults who  need personal or academic assistance.  Ability testing is governed by detailed  ethical standards and professionally  administered tests must meet strict  criteria including lack of cultural bias  and periodic updating. In fact, IQ tests  are the most technically sophisticated  of all psychological tests and undergo  the most extensive quality checks  before publication. > 
 The Human Brain I NewScientist: The Collection I 43 
DIFFERENT  TYPES OF  INTELLIGENCE 
Consider the engineer's superior  spatial intelligence and the lawyer's  command of words and you have to  wonder whether there are different  types of intelligence. This question was  debated ferociously during the early  decades of the ZOth century. Charles  Spearman, on one side, defended the  omnipotence of his general factor of  intelligence, g. On the other,  psychologist Louis Thurstone argued for  seven "primary abilities", including  verbal comprehension (in which females  excel) and spatial visualisation (in which  males excel). Thurstone eventually  conceded that all his primary abilities  were suffused with the same g factor,  while Spearman came to accept that  there are multiple subsidiary abilities in  addition to g on which individuals differ. 
This one-plus-many resolution  was not widely accepted until 1993,  however. It was then that American  psychologist John B. Carroll published his  "three stratum theory" based on a  monumental reanalysis of all factor  analysis studies of intelligence (see  diagram, page 4Z). At the top is a single  universal ability, g. Below this indivisible  g are eight broad abilities, all composed  mostly of g but each also containing a  different "additive" that boosts  performance in some broad domain such  as visual perception or processing speed.  These in turn contribute to dozens of  narrower abilities, each a complex  composite of g, plus additives from the  second level, together with life  experiences and specialised aptitudes  such as spatial scanning. 
This structure makes sense of the  many differences in ability between  individuals without contradicting the  dominance of g. For example, an excellent  engineer might have exceptional  visuospatial perception together with  training to develop specialist abilities,  but above all a high standing on the g  factor. The one-plus-many idea also  exposes the implausibility of multiple-  intelligence theories eagerly adopted by  educators in the 1980s, which claimed  that by tailoring lessons to suit the  individual's specific strength - visual,  tactile or whatever - all children can be  highly intelligent in some way. 
 Intelligence tests are calibrated so that at each age, the IQ  average score is 100 and 90 per cent of individuals score between  IQ 75 and 125. The typical IQ difference between strangers is 17  points and it is 12 between full siblings. So what makes some  people smarter than others? And how can we change our score? 
OLDERANDWISER 
The brain is a physical organ and no  less subject than any other to ageing,  illness and injury. The normal  developmental trajectory is that  aptitude at learning and reasoning -  mental horsepower - increases quickly  in youth, peaks in early adulthood,  and then declines slowly thereafter  and drops precipitously before death.  The good news is that some important  abilities resist the downturn. 
Some IQ researchers distinguish  between tests of fluid intelligence  (gF) and crystallised intelligence (gC).  The first assess on-the-spot learning,  reasoning and problem solving; the  second assess the crystallised fruits  of our previous intellectual  endeavours, such as vocabulary in  one's native language and broad  cultural knowledge. During youth,  gF and gC rise in tandem, but they  follow different trajectories  thereafter. All gF abilities decline  together, perhaps because the brain's  processing speed slows down with  age. However, most people's gC  abilities remain near their personal  peak into old age because they reside  in the neural connections that gF has  laid down over a lifetime of learning  and practice. Of course, age-related  memory loss will affect an individual's  ability to recall, but exactly how this  affects intelligence is not yet known. 
This has practical implications. 
On the positive side, robust levels of  gC buffer the effects of declining gF.  Older workers are generally less able  to solve novel problems, but they can  often compensate by calling upon  their larger stores of experience,  knowledge and hard-won wisdom. 
But gC can also disguise declines in gF,  with potentially hazardous results. For  example, health problems in later life  can present new cognitive challenges,  such as complex treatments and  medication regimes, which individuals  with ample gC may appear to  understand when actually they  cannot cope. 
There are ways of slowing  or reversing losses in cognitive  function. The most effective  discovered so far is physical  exercise, which protects the brain by  protecting the body's cardiovascular  health. Mental exercise, often called  brain training, is widely promoted,  but it boosts only the particular skill  that is practised - its narrow impact  mirroring that of educational  interventions at other ages. Various  drugs are being investigated for their  value in staving off normal cognitive  decline, but for now preventive  maintenance is still the best bet -  avoid smoking, drinking to excess,  head injuries and the like. 
Overall size of the  brain, relative to the  body, correlates  with IQ 
Intelligence  requires integration  of sensory and  other information 
Learning and  experience can  increase the size of  specific brain areas 
High IQ is  associated with  faster mental  processing speed 
Volume of the  cortex, the brain's  grey matter,  correlates with IQ 
Volume of tissue  linking the brain's  hemispheres  correlates with IQ 
44 1 NewScientist: The Collection | The Human Brain 

"Intriguingly, the heritability of  intelligence is less than 30 per cent  before children start school, rising  to 80 per cent among adults" 
Intelligence is distributed across many areas of the brain  and people with the highest IQ tend to have increased volume  in a network of regions (shaded) including key language areas 
 Motor 
j/ 
ff Somatosensory 
fi T 
Planning complex  movements/  elaboration of  thought 
 L ^/Spatial r  awareness^ ^ 
Word 
formation 
 Visual 
Language, processing!  comprehension 
mar-' \r ^ 
' Behaviour, emotions,  motivation 
 NATURE AND NURTURE 
Each of us is the embodiment of our  genes and the environment working  together from conception to death. 
To understand how these two forces  interact to generate differences in  intelligence, behavioural geneticists  compare twins, adoptees and other  family members. The most compelling  research comes from identical twins  adopted into different homes -  individuals with identical genes but  different environments - and non-kin  adopted into the same home -  unrelated individuals sharing the  same environment. These and other  studies show that IQ similarity most  closely lines up with genetic similarity. 
More intriguingly, the studies also  reveal that the heritability of  intelligence - the percentage of its  variation in a particular population  that can be attributed to its variation  in genes - steadily increases with age.  Heritability is less than 30 per cent  before children start school, rising to  80 per cent among western adults. In  fact, by adolescence, separated  identical twins answer IQ tests almost  as if they were the same person and  adoptees in the same household as if  they were strangers. 
The surprising conclusion is that  most family environments are equally  effective for nurturing intelligence -  the IQ of an adult will be the same  almost regardless of where he or she  grew up, unless the environment is  particularly inhumane. 
Identical twins are a natural  laboratory in which to study how  intelligence develops 
Why do the shared environment's  power to modify IQ variation wane and  genetic influences increase as  children gain independence? Studies  on the nature of nurture offer a clue.  All children enter the world as active  shapers of their own environment.  Parents and teachers experience this  as their charges frustrate attempts to  be shaped in particular ways. And  increasing independence gives young  people ever more opportunities to  choose the cognitive complexity of  the environments they seek out. The  genetically brighter an individual, the  more cognitively demanding the tasks  and situations they tend to choose,  and the more opportunities they have  to reinforce their cognitive abilities. 
Given that an individual's ability  to exploit a given environment is  influenced by their genetic  endowment, and given that "better"  family environments tend not to  produce overall increases in IQ, it is not  surprising that attempts to raise low  IQs by enriching poor school or home  environments tend to disappoint.  Narrow abilities can be trained up but  g apparently cannot. This makes  sense if g is an overall property of the  brain. That does not mean intensive  early educational interventions lack  positive effects: among other things  they may reduce rates of teenage  pregnancy, delinquency and school  dropout. Besides, even if we cannot  boost low intelligence into the  average range, we do know how to  help all children learn more than they  currently do and achieve more with  the intelligence they have. ■ 
 The Human Brain I NewScientist: The Collection I 45 
I I 
 Worldwide, IQs have risen by up to three points per decade over  the past century. At 80, James Flynn, the man this increase is  named after, explains its implications and the effects of sex,  culture and attitude on intellectual achievement 
Our IQs have risen. Does this mean we are  getting smarter? 
Our brains have no more potential at  conception, but because we have done  different “mental exercise” throughout our  lives, our brains would look different at  autopsy, just as a weightlifter’s muscles look  different from a swimmer’s. Our ancestors  were just as good as we are at practical  intelligence, at dealing with everyday life. But  we have developed the mental skills needed to  deal with the demands of the modern world. 
Does that mean we should revise our definition  of intelligence? 
Once we understand how our minds have  changed, I leave it to you whether you want  to say we are “more intelligent”. There is no  doubt that we need a new approach to the  study of intelligence. 
If one individual is better than average on  one important cognitive skill, they tend to be  better on all of them. Society, on the other  hand, may change so as to demand  enhancement of one important skill - say,  the ability to use logic to deal with abstract  symbols - but make no extra demands on  the expansion of our everyday vocabulary.  Writing the cognitive history of the  20th century, of how our minds have  changed over time, is quite different from  measuring how much one person’s cognitive  skills are superior to another’s. 
You caused a bit of stir talking about gains in  women's IQ. 
Women have gained on men over the past  generation, to the point where they now  equal or slightly surpass men. 1 don’t think 
the advantage that women are showing is a  genetic advantage for intelligence; I suspect  it’s down to extra mental exercise. Girls are  more likely to use their mind in school than  boys are. But at university, they really are  two or three points below men, and that’s  because more marginal women, IQ-wise,  qualify for university. 
How come? 
A girl with an IQ of lOO thinks of herself as  university material and has the marks. A boy  with the same IQ hates school and doesn’t  have the marks. So you’re much more likely  to find girls with an IQ below no in university  than boys. Even so, females do better than  males at university. 
"If we didn't investigate  because it was politicaliy  incorrect, we'd never know" 
What about the infamous remarks by Larry  Summers, when he was president of Harvard  University, that women have less innate ability  than men for science and mathematics? 
This is a perfectly respectable hypothesis.  Every hypothesis should be tested. He  remarked that at the highest level of pure  mathematics, women are under-represented.  My answer is that if there is a difference, it’s  not cognitive but temperamental. 
Tm convinced from my research that women  can use logic just as well as men. It could be that  thanks to the testosterone of males and the  greater proclivity of women to be interested in  human beings, there will always be fewer  women in pure mathematics. Who knows? 
I have an open mind. When I lecture on this, 
I say Summers was wrong to think that women  are less gifted cognitively. 
It's a sensitive issue. Why tackle it? 
We need to know. And if people like me  didn’t investigate it because it was politically  incorrect, we never would know. It’s not  accidental that I’m the one who’s  overwhelmingly brought this evidence to  bear on the gender issue. A lot of other people  were too scared to go into it. 
When feminists say to me that this is a  great difficulty, I say, do you want women  to be as competitive and soulless as men? 
I mean, is your ideal human being someone  who neglects their family and kids and works  i6 hours a day to be a corporate executive? 
You can’t have it both ways. 
Do these differences in outlook exist  between cultures too? 
Yes. If you came home and told your Irish  father [Flynn is Irish American] you’d made  the football team, he’d be over the moon. If  you told that to a Jewish parent, they might  forbid you to play football. And if I came home  with a good report card, my father would give  me perfunctory praise. A Chinese parent, the  kid knows he’s over the moon. 
In my book Asian Americans, I wrote  that Chinese Americans who had come to  America before 1950 as children, or had  been born in the US, had IQs no higher  than whites - they just outperformed them  like crazy. That is, they could drop seven  points on whites and still get the SAT scores  and grades to get into Berkeley. A Chinese  American with an IQ of about 93 looked as  intelligent as a white at 100, in terms of their  educational and occupational profile. 
Like women, Chinese are more adjusted  to formal education. They don’t skip class;  they hand homework in on time; they don’t  get suspended. 
What implications does the "Flynn effect" have  for the use of the death penalty? 
In the US I’m going to be executed rather than  exonerated if I have an IQ above 70 - because  below that is where they deem “significant  limitations”, such as problems with literacy or  social skills, set in. For 10 years I have been  trying to educate judges about the Flynn effect  and the need to restandardise IQ tests every  generation or so. If I was tested in 1976 with  an IQ test from 1948, it has been inflated by  28 years of IQ gains, which means that an IQ  of 67 could be returned as one of 75. 
46 1 NewScientist: The Collection | The Human Brain 
I Have you succeeded in changing things? 
At the beginning, I faced enormous resistance.  Today, almost everyone who defends capital  offenders is aware of my work. Given how  conservative the judicial profession is, 
Tm not discouraged. I hope that before 1 die, 
1 will see more progress. 
You're now 80, but you're still working pretty  much full time? 
1 teach four-fifths of the time, two courses,  and 1 have a lot more time for my writing than  1 did when I was department head. 
1 Doesn'tthis run counter to the material in your  new book about the "dark" side of old age? 
No, that is about analytical people losing more  ground in old age. Most IQ tests divide skills  into four categories - analytical skills, verbal  skills, working memory and perceptual speed.  Disturbingly, those who are most above the  average analytically have the deepest falToff  between the age of 65 and 88. Retirement age  is what really sets things off. 
Why do people decline after they retire? 
Let's imagine that high-performance  analytical brains are like high-performance  cars: in old age, they need more servicing  than the average car. It could be that evolution  has not geared the high-performance  analytical brain to keep its tone in old age,  neuronally. Or it may be that most highly  intelligent people mainly use analysis at work,  and when they retire, they lose an analytical  exercise advantage over the average person. 
I But there's a bright spot for verbal skills : for  people who are well above the mean, verbal  facility decays slower. 
How are you holding up? 
Oddly enough, I don't really feel I have fewer  new ideas or am able to do less analysis than  I could at 20 . 1 do find my working memory  has slipped a bit. I've remained intensely  active. I still run. 
What do the next few years hold for you? 
I'm on a crusade to salvage university  education. I looked at students at a very good  US university, one of the top 10, and found  that only about 1 in 5 could do any critical  I thinking outside their major subject. 
I Universities are in a position to correct this; 
§ every department could run a course that gave  s them these key tools. And I've just finished a  I little book on climate change. This seems to be  I an issue that any educated person would want  g to have an independent opinion on. ■ 
The Human Brain I NewScientist: The Collection I 47 

There are signs that  our century-long rise  in intelligence has gone  into reverse, finds 
Bob Holmes 
 I N DENMARK, every man is liable for  military service at the age of i8. Nowadays,  only a few thousand get conscripted but  all have to be assessed, and that includes  doing an IQtest. Until recently, the same one  had been used since the 1950s. “We actually  have the same test being administered to  25,000 to 30,000 young men every year,”  says Thomas Teasdale, a psychologist at  the University of Copenhagen. 
The results are surprising. Over this time,  there has been a dramatic increase in the  average IQ of Danish men. So much so that  what would have been an average score in  the 1950s is now low enough to disqualify a  person from military service, Teasdale says. 
The same phenomenon has been observed  in many other countries. For at least a century,  each generation has been measurably brighter  than the last. But this cheerful chapter in  social history seems to be drawing to a close. 
In Denmark, the most rapid rises in IQ, of  about 3 points per decade, occurred from the  1950s to the 1980s. Scores peaked in 1998 and  have actually declined by 1.5 points since then.  Something similar seems to be happening  in a few other developed countries, too,  including the UK and Australia. 
So why have IQ scores been increasing  around the world? And more importantly,  why does this rise now seem to be coming to  an end? The most controversial explanation  is that rising IQ scores have been hiding a  decline in our genetic potential. Could this  possibly be right? Do we face a future of  gradually declining intellectual wattage? 
There’s no question that intelligence - as  measured by IQtests, at least - has risen  dramatically since the tests were first  formalised a century ago. In the US, average IQ  rose by 3 points per decade from 1932 to 1978,  much as in Denmark. In postwar fapan, it shot  up by an astonishing 7.7 points per decade, and 
 two decades later it started climbing at a  similar rate in South Korea. Everywhere  psychologists have looked, they have seen  the same thing. 
This steady rise in test scores has come  to be known as the “Flynn effect” after  lames Flynn of the University of Otago in  New Zealand, who was one of the first to  document the trend (see “The intelligent  approach to IQ”, page 46). Much has been  written about why this has been happening.  There may be a cultural element, with the  rise of television, computers and mobile  devices making us better at certain skills. 
The biggest IQ increases involve visuospatial  skills. Increasing familiarity with test formats  may also play a role. 
The general view, though, is that poor  health and poor environments once held  people back, and still do in many countries.  Wherever conditions start to improve,  though, the Flynn effect kicks in. With  improved nutrition, better education and  more stimulating childhoods, many people  around the world really have become smarter. 
We have, after all, changed in other ways:  each generation has been taller than the  previous one, probably because nutrition  has improved. So although height is thought  to have an even larger genetic component  than intelligence - taller parents tend to have  taller children - the environment matters too. 
If better nutrition and education have led to  rising IQs, the gains should be especially large  at the lower end of the range, among the  children of those with the fewest advantages  in their lives. Sure enough, that’s what testers  usually see. In Denmark, for example, test  scores of the brightest individuals have hardly  budged - the score needed for an individual  to place in the top 10 per cent of the  population is still about what it was in the  1950s. “It was the bottom end that was > 
48 1 NewScientist: The Collection | The Human Brain 
0LIVERJEFFER5 
 The Human Brain I NewScientist: The Collection I 49 
"More people are developing their  potential, but that potential may  be declining" 
moving up. The top end hardly moved at all,”  says Teasdale. 
If social improvements are behind the Flynn  effect, then as factors like education and  improved nutrition become common within  a country their intelligence-boosting effects  should taper off, country by country. ‘T Ve  been predicting for some time that we should  see signs of some of them running out,” says  Flynn. And those signs are indeed appearing. 
It seems we are seeing the beginning of the  end of the Flynn effect in developed countries. 
Similarly, the increases in height are also  tapering off. But IQ scores are not just  levelling out but appear to be declining. 
The first evidence of a small decline, in Norway,  was reported in 2004 (see chart, below). Since  then a series of studies have found similar  declines in other highly developed countries  including Australia, Denmark, the UK, Sweden,  the Netherlands and Finland. Should we be  worried? Not according to Flynn and Teasdale. 
The evidence remains sparse and sometimes A good education is 
contradictory, and could just be due to chance. one of the factors that 
help boost IQ scores 
 Underlying decline? 
Even if they are not down to chance, such  small declines could be attributable to minor  changes in social conditions such as falling  income or poorer education, which can easily  be reversed, says Flynn. But these are invented  hypotheses for a very small phenomenon, he  points out. “You’d want to be pretty certain  that phenomenon was actual before you  scratch around too hard for causes.” 
There is a more disquieting possibility,  though. A few researchers think that the Flynn  effect has masked an underlying decline in the  genetic basis of intelligence. In other words,  although more people have been developing  closer to their full potential, that potential has  been declining. 
Most demographers agree that in the  past 150 years in Western countries, the  most highly educated people have been  having fewer children than is normal in the  general population. The notion that less  educated people are outbreeding others is far  from new, as is the inference that we are  evolving to be less intelligent. It’s even the  theme of a 2006 film, Idiocracy. 
“This is a claim that has been made for over  a century now, and always with the most  horrific prediction of what might happen if  we don’t stop it,” says Bill Tucker, a historian of  psychology at Rutgers University in Camden,  New jersey. This idea led to the extensive  eugenics programme in the US, with its forced 
sterilisations, which in turn helped inspire the  “purity” policies of Nazi Germany. 
This unpleasant history, though, doesn’t  mean there is no genetic decline, some argue.  Richard Lynn of the University of Ulster, UK,  a psychologist whose work has often been  controversial, has tried to calculate the rate  of decline in our genetic potential using  measured IQ values around the world in 1950  and 2000. Ffis answer: a bit less than 1 IQ  point, worldwide, between 1950 and 2000. If  the trend continues, there would be another  1.3 point fall by 2050. Even if he is right - and  it’s a big if - that is a tiny change compared  with the Elynn effect. Would small declines  like this even matter? 
A score to settle 
The rise of average IQ scores of military conscripts  in Norway has slowed and started to reverse. 
Similar patterns are seen in a few other countries 
 Yes, argues Michael Woodley, a psychologist  at Eree University of Brussels (VUB) in Belgium.  This kind of evolution would shift the  bell curve of intelligence, he claims, and a  small shift can lead to a big drop in the  number of high scorers. For example, if  mean IQ fell from 100 points to 97, it  would almost halve the number of people  who score above IQ 135. “It’s a leverage effect,”  Woodley says. 
Would this really matter? People who score  highly in IQ tests are not always the most  successful in life. In any case, with so many  confounding factors, it is far from clear  whether the “evolving to be stupid” effect is  real. For example, it has been suggested that  caesarians allow more bigger-brained babies  to survive than in the past. 
A definitive way to settle this issue would be  to look at whether gene variants associated  with higher IQs are becoming less common.  The trouble with this idea is that so far, despite  huge effort, we have failed to find any specific  gene variant linked to significantly higher IQs  in healthy individuals. 
“Boy, a lot of investigators have spent a lot  of time looking for that stuff, with some pretty  big samples and sophisticated methodology,”  says Ronald Yeo, a psychologist at the University  of New Mexico in Albuquerque. “Of course it  doesn’t mean that there aren’t genes that are  important. It’s just there are so many of them  and they each have so little effect.” 
50 1 NewScientist: The Collection | The Human Brain 

Yet Woodley thinks his team has found clear  evidence of a decline in our genetic potential -  and he claims it is happening much faster  than Lynn’s calculation suggests. Instead of  relying on fertility estimates, Woodley looked  at a simple measure: reaction time. Quick-  witted people, it turns out, are exactly that:  smarter people tend to have quicker reaction  times, probably because they process  information more quickly. 
Back in the i88os, the polymath Francis  Gabon measured the reaction times of several  hundred people of diverse social classes in  London. A few years ago, Irwin Silverman of  Bowling Green State University in Ohio noticed  that the reaction times Gabon recorded - an  average of about 185 milliseconds between  seeing a signal and pushing a button - were  quite a bit quicker than the average of more  than 250 milliseconds in modern tests, which  began in the 1940s. 
Woodley’s team reanalysed Silverman’s  data, factoring in the known link between  reaction time and intelligence. When they  did this, they found that reaction times had  indeed slowed over the century, by an amount  corresponding to the loss of one full IQ point  per decade, or more than 13 points since the  Victorian era. 
Critics have been quick to attack Woodley’s  analysis, arguing that Gabon may not have  measured reaction times in the same way  as later investigators. If Gabon’s apparatus 
had a button with a shorter range of motion,  for instance, then he would have measured  shorter reaction times. What’s more,  Silverman points out that there is no obvious  downward trend in the post-1940 data, as  there should be if Woodley is right. 
In a detailed response published in lune  2014, Woodley maintains that today’s brains  remain slower even after accounting for all  these other explanations. But even if he’s right  about reaction times, the correlation between  IQ and reaction time is not an especially  strong one: reaction time explains only  about 10 per cent of the variation in IQ. 
“Probably every generation moans about  the new generation being less intelligent, and  every upper crust moans about the lower  classes out-breeding them,” says Kevin  Mitchell, a neurogeneticist at Trinity College  Dublin in Ireland. “The basic premise is that  IQ levels are dropping. And I don’t see any  evidence for that, which is why I find the  whole debate a bit odd.” 
Trouble ahead 
The coming decades should provide a  definitive answer. If what we are seeing in  countries like Denmark is merely the end  of the Flynn effect, IQ scores should stabilise  in developed countries. If Woodley and  his colleagues are right, we should see a  continuing decline. 
Even if we are evolving to be more stupid,  it is far from clear whether we need to worry  about it. Flynn thinks the problem may just  take care of itself, as societal improvements  such as better healthcare and more promising  employment options bring down fertility  rates in every stratum of society. 
But don’t breathe a sigh of relief just yet. 
In the longer term, there may be an even  more fundamental threat to our intelligence.  We humans mutate fast - each of us has 50  to 100 new mutations not present in our  parents, of which a handful are likely to  be harmful, says Michael Lynch, an  evolutionary geneticist at Indiana University  in Bloomington. In the past, harmful  mutations were removed as fast as they  appeared, because people unlucky enough  to inherit lots of them tended to die young,  before they had children. Now, things are  different. Fetal mortality, for example, has  declined by 99 per cent in England since the  1500s, Lynch says. 
This means that populations in developed  countries are accumulating harmful mutations.  Over tens of generations. Lynch has calculated. 
Shrinking brains 
Average volume of European female brain 
10,000 1502 
years ago millilitres 
Are humans evolving to be dumber? A few  researchers argue that this has happened  over the past century or so (see main  story), but it could have been going on for  much longer. One thing is certain: our  brains have been shrinking for at least  10,000 years. An average European  woman today, for example, has a brain  about 15 per cent smaller than that of her  counterpart at the end of the last ice age. 
It has been suggested that with the rise  of agriculture and towns, and increased  division of labour, people could survive  even if they weren't as smart and  self-sufficient as their hunter-gatherer  ancestors. But smaller doesn't necessarily  mean wimpier, saysjohn Hawks, an  anthropologist at the University of  Wisconsin in Madison. Brains are costly  to operate, so evolution is likely to favour  increased efficiency. Modern brains might  do just as much with a smaller package.  Hawks thinks. 
this will lead to a large drop in genetic fitness.  With so many genes contributing to brain  function, such a decline might well drag down  our brainpower, too. The only way to stop that  might be to tinker with our genomes. Given  our ignorance about the genetic basis of  intelligence, and the ethical complexities, that  is a long way off. 
Coming back to the short-term, though,  there is an obvious option for those concerned  about intelligence levels. “If you’re worried  about it, the answer is what the answer has  always been,” says Mitchell. “Education. If you  want to make people smarter, educate them  better. That won’t make everybody equal, but  it will lift all boats.” ■ 
The Human Brain I NewScientist: The Collection I 51 
CHAPTER FOUR 
MENTAL HEALTH 
Down with  dementia 
The dream of living to a ripe old age becomes  a nightmare if your mind disintegrates en route.  Yet there has been some much-needed good 
news about this condition. 
M y paternal grandfather died shortly  before I was born. The man my father’s  stories conjured up was physically and  mentally tough: a first world war veteran who  was boisterous with his drinking buddies and, at  home, an old-fashioned head of the household. 
But beside those tales sat his life’s sad,  unelaborated footnote; that he ended his  days demented and degraded. 
When I ask directly, my dad recalls his father  sitting silently for hours, endlessly nursing an  empty tea cup, oblivious to all. But my parents  prefer not to go into detail. My mum says:  “People just didn’t talk about dementia  40 years ago.” 
Today, though, we talk about dementia a  lot. With life expectancy continuing to rise  and the baby-boomer population bulge  standing on the cusp of old age. Western  countries face what is sometimes called a  looming tsunami of dementia. Such is the  urgency that in December 2013 London  hosted the first G8 summit on the subject,  where the world’s eight richest countries  agreed to coordinate their research efforts  against the problem. 
The epidemic will place huge strain on  healthcare systems; in the UK, the annual cost  of caring for someone with this condition is  more than the average salary. And on a personal  level, the prospect of a long life loses its appeal  if it ends this way. 
But wait a minute. All the gloomy predictions  have been based on a central assumption that  people will continue to develop dementia  at the same rate as they always have. It is a  reasonable assumption - age is the primary risk  factor for dementia - but it may well be wrong.  There is emerging evidence that the dementia  rate in developed countries has fallen. 
Liam Drew 
Since the average age of the inhabitants of  Western countries is rising, this may not be  enough to stop the total number of people  with dementia from increasing. So we still  need to plan accordingly at the societal level.  But our individual chances of succumbing  appear to have decreased. For once, this is  a good-news story about dementia. 
The search is now on to uncover what  has driven these trends, so that they can  be maintained and maybe even amplified. 
“I think this gives some basis for cautious  optimism,” says Kaare Christensen, an  epidemiologist at the University of Southern  Denmark in Odense, who led some of the  research. “There seems to be huge potential  for further progress - if we don’t destroy it.” 
How well our minds function in old age is  a major determinant of our quality of life. 
A small decline in cognitive abilities is an  almost inevitable part of ageing. For most  people this is a gentle downward turn in mental  agility, frustrating but with no great impact. 
If this fall-off is more than usual for  someone’s age, but not enough to interfere  with their day-to-day living, it is classed as  mild cognitive impairment. This is a high-risk  state for progression to dementia. 
Dementia is a general breakdown of the  intellect and personality, with disintegration  of memory, attention and emotional control.  Of all the diseases linked to ageing, for me this  is the most fearsome. It is degrading for the  person concerned and heartbreaking for those  around them. 
About two-thirds of dementia cases are  caused by Alzheimer’s disease, in which  neurons die off amid distinctive clumps of  protein. The next most common form is  vascular dementia, caused by deterioration of 
5Z I NewScientist: The Collection | The Human Brain 

the brain’s blood vessels and often involving  minor strokes. There are other, less common  subtypes, plus a growing belief that dementia  at very old ages typically involves a mix of  different forms of disease. 
What’s always been known is that the risk of  dementia rises markedly with age - seemingly  inexorably. Very few cases occur before the age  of 6o, and between 6o and 70 the condition is  still restricted to an unlucky 1 per cent or so.  After this point, though, the odds worsen  significantly: about 5 per cent of 70 to 80-year-  olds are affected, and beyond 80 the risk rises  ever more sharply (see graph, page 54). 
The logic has always seemed inescapable:  the more 8o-year-olds there are around, the  more people there will be with dementia. The  number of people with dementia globally is  often predicted to triple by 2050. 
Unequivocally good news 
But over the past few years there have been  hints that the actual numbers didn’t fit this  picture. Research suggested that dementia  was on the retreat. The studies weren’t  conclusive, though - either they were too  small or their findings statistically borderline. 
The picture has now changed. In 2013,  leading medical journal The Lancet published  two studies involving thousands of people,  which definitively challenge the orthodoxy. 
One compared two surveys of dementia  numbers in the UK, done 20 years apart. 
The first, from 1994, led to the conclusion that  there were about 650,000 people with the  condition. With the increase in average age of  the population over the intervening years, the  repeat survey - which used exactly the same  tests and definitions - should have found  nearly 900,000 people with dementia. But the  count came up over 200,000 short. Looking  at how the illness affected specific age groups,  it appeared that people were developing  dementia later in life (see chart, page 54). 
The finding came as a welcome surprise  to Carol Brayne, the epidemiologist at the  University of Cambridge who led the study. 
“It has been a very positive experience,” she  says. The editorial that The Lancet ran to  accompany the paper described the findings  as “unequivocally good news”. 
The other study looked at the health of two  groups of Danish people in their mid-90s,  born a decade apart, in 1905 and 1915. The  nonagenarians were asked to complete a  battery of physical and mental tests. While  the two groups had similar physical health,  those born in 1915 markedly outperformed > 
The Human Brain I NewScientist: The Collection I 53 
the earlier-born in cognitive tests. ‘‘They  were not stronger, but they were smarter,”  says Christensen, who led the study. “The two  papers complement each other beautifully.” 
The big question, naturally, is why things  changed. Neither study was designed to  uncover the reasons behind any trends, but  we can make educated guesses. The main  suspects are long-term trends of rising  prosperity, education, and better health;  all these things seem to be good for the brain. 
The idea that learning and thinking could  ward off the physical diseases that bring on  dementia has been controversial. “It was very  fringy in the beginning,” says Yaakov Stern,  a neuropsychologist at Columbia University  in New York, who has spent the last 25 years  investigating this idea. 
His interest was sparked in the 1980s, when  a colleague claimed that more highly educated  professionals were less likely to develop  Alzheimer’s disease. Sceptics thought there  must be other explanations - perhaps these  groups simply performed better on the  cognitive tests used to diagnose Alzheimer’s,  or maybe the low income that goes hand in  hand with lack of education was linked with  other risk factors. 
But these possibilities were ruled out  by further studies, and the notion began  to gain support that intellectual activities  create a resilience to age-related decline  across brain networks. Such “cognitive  reserve” helps the brain to keep functioning  despite mild physical deterioration, so the  theory goes, “lust because you have  pathology doesn’t mean the brain says  ‘Tm going to drop dead,’ ” says Stern. “The  brain says ‘Tm going to do the best 1 can.’ ” 
Happily for the cognitive reserve theory,  populations did become better educated over  the first half of the 20th century in many  Western countries - including the UK and  Denmark - through improved access to  education and repeated increases in the  school-leaving age. Both Brayne and  Christensen think education is probably one  part of the explanation for their findings. 
Could this trend continue? The school-  leaving age in the UK, for instance, has risen  further since the people in the British study  were at school. And in recent decades growing  numbers of people have gone on to higher  education. It remains to be seen if this will  drive further improvements or whether,  perhaps, there might be an upper limit on the  protection afforded by early-life education. 
Of course, for many of us it is too late to do  anything about our schooling. But cognitive 
Modern life, with its constant  multimedia inputs, may be much more  stimulating than it was 50 years ago" 
reserve is not just set by formal education. It  is also affected by the mental demands of our  jobs and our intellectual activities throughout  life. “Cognitive function is modifiable right  across the life course,” says Marcus Richards,  an epidemiologist with the UK Medical  Research Council’s Unit for Lifelong Health  and Ageing in London. “It’s never too late to  take control of protecting it.” 
This idea has been seized upon by firms  that produce “brain training” computer  games. There is no question that practising a  computer task makes you better at that task,  as any gamer will tell you. But it remains  uncertain whether such skills can help brain  function in general, as the adverts claim, nor  do we know how long any benefits might last. 
The first study to show that a computer  game could lead to benefits beyond the  console appeared in 2013. A game designed  to help people get better at multi-tasking  enhanced their powers of attention and  working memory for at least six months. 
But before placing any faith in such an 
Dementia curveball 
Life expectancy at birth in England and Wales 
The steady increase in life expectancy has led to  widespread predictions that there will soon be a  sharp upsurge in the number of dementia cases... 
90 
 30 
20 
10 
0 
1920 1940 1960 1980 2000 
approach, bigger and longer studies are  needed, ideally ones that also measure rates  of dementia. 
In the meantime, there are less  controversial - and arguably more enjoyable -  ways of building your cognitive reserve, like  taking up mentally taxing hobbies such as  the card game bridge, or playing a musical  instrument. A full social life may also protect  against dementia, according to several studies. 
lust existing in the modern world - with  its mobile phones and constant multimedia  inputs - may be much more intellectually  stimulating than it was 50 years ago. “Life now  is very cognitively demanding for everybody,”  says Stern. 
But mental stimulation is not the brain’s  only input - there are also its physical inputs,  in the form of oxygen, energy and nutrients,  delivered by the blood supply. Animal  research has shown that healthy blood vessels  are critical for good cognitive function in later  life, minimising the risk not just of vascular  dementia but the other forms too. “It would 
Dementia prevalence 
...but a recent study casts doubt on those  predictions, as the rate of dementia among  over-65s has fallen over the past 20 years 
40 
1989 ■ 2008 
 Age groups 65-69 70-74 75-79 80-84 85-89 >90 S 
54 1 NewScientist: The Collection | The Human Brain 
w 
 be unreasonable not to think that vascular  factors played a role,” says Brayne. 
Certainly rates of heart and vascular  disease have been falling in Western  countries since the 1970s, probably due to  a mix of factors, including better awareness  of the risks of smoking, high cholesterol and  sedentary lifestyles, and the wider use of  drugs to control blood pressure. One study  that hinted at falling dementia rates - before  the recent research in The Lancet - compared  people’s brain circulation with MRl scans,  and found that later-born people had  healthier blood vessels. 
As the advice in the UK’s National Dementia  Strategy puts it: “What’s good for the heart is  good for the brain.” The take-home messages  are not new: don’t smoke, try to stay in  shape, and keep an eye on your blood pressure  and cholesterol levels. But the recent evidence  is providing more incentive than ever to  pay heed. 
What’s more, the benefits of exercise  have not only been shown in observational  studies - where people who happened to be  more active had less dementia - but also in  randomised trials, the best kind of evidence. In  other words, people asked to do more exercise  had less intellectual decline as they aged. 
There is, however, another very important  factor affecting the health of our blood vessels,  and that is what we eat. At the start of the  20th century, malnutrition was widespread  in the UK - almost half the men called up to  serve in the first world war were found not fit 
to serve for this reason. People suffered from  a lack of vitamins and other micronutrients,  as well as a general shortage of calories. 
Diets improved markedly over the following  decades, thanks to rising prosperity levels and  public health measures such as free school  meals. Thankfully child malnutrition is now  rare in the UK. But could we use diet to  improve our brain health still further? 
Fish appeal 
The most promising nutrients to target  would be the antioxidant vitamins C and E, the  B vitamins and folate, and omega-3 fatty acids,  abundant in fish. But while observational  studies show that eating too little of these  substances heightens the risk of dementia,  randomised trials of adding extra to the diet,  in the form of supplements, haven’t shown  benefits. Such trials have limitations, though,  says Richards; few last longer than a couple of  years, while “people are accumulating these  dietary exposures over decades”. 
Still, at the moment most researchers are  reluctant to recommend anything other than  the standard heart-healthy nutritional advice.  That is a Mediterranean diet, rich in fruit and  vegetables, with plenty of fish and not too  much red meat or high-calorie junk food. 
For what gives most concern is dietary  excess rather than deficiency. Unlike people  born in the first half of the 20th century, later  generations have famously got themselves  overweight. And today the West is suffering 
unprecedented levels of diabetes, which also  predisposes people to dementia, according  to recent research. Some even talk of  Alzheimer’s being a form of “brain diabetes”. 
As no one knows the relative contributions  of all the possible factors that could explain  why dementia rates have fallen - diet,  education, health - it is impossible to  confidently predict future disease rates. Yet it  is likely that rising obesity and diabetes will  affect future trends and that, says Richards, is  something dementia researchers are watching  with “nervous anticipation”. 
For the most pessimistic, the upsurge  in these twin risk factors means we should  not say that dementia rates are falling, merely  that they fell between the two observed  generations. For these reasons, and also  simply because people are living longer,  healthcare systems must be ready. “We still  need a society which is adapted to cope with  a lot of old people, and a lot of old people with  some cognitive impairment,” says Brayne.  “That message doesn’t go away because our  paper shows an age-specific reduction.” 
Which may explain why the recent studies  in The Lancet didn’t get much attention at the  G8 dementia press conference I attended at  the launch of the summit in London: they  might dilute the message that more research  funding is needed as a matter of priority.  Dementia research is certainly neglected  compared with other conditions,  in relation to the number of people they affect,  and the promise of greater funding that  emerged at the summit is essential. 
Yet the new studies suggest that researching  preventative measures could be a sound  investment. For while we may not get to  choose when, where and to whom we’re born,  we do have some control over how we live. 
My granddad, born in 1898, had barely any  schooling, fought in the first world war and  endured the austerity of the second. Born in  1977, I’ve had over two decades of education  and a pretty comfortable life. On the other  hand, while he dug graves for a living, I spend  my work day mainly sitting down. 
Still, since researching this article, I have  felt more hopeful about my odds of enjoying  a healthy and independent old age. I like the  thought that it’s within my power to improve  those odds. Lately, when possible. I’ve even  been walking instead of taking tube trains, and  putting a bit more effort into eating enough  fish and vegetables. 
It’s one thing if my body suffers the  consequences of an unhealthy lifestyle -  quite another if my mind does too. ■ 
The Human Brain I NewScientist: The Collection I 55 
 56 1 NewScientist: The Collection | The Human Brain 

There are gentler ways  of helping people with  schizophrenia to reclaim  their lives than fighting  their delusions with drugs,  says Clare Wilson 
I WAS trembling all the time. I couldn’t shave. 
I couldn’t wash. I was filthy,” says Peter  Bullimore. “I had become the archetypal  schizophrenic. People would write on my  windows: ‘Schizo out’ and I had one member  of the public slash my face.” 
Today, that period of Bullimore’s life is long  behind him. He runs a mental health training  consultancy in Sheffield, UK, and travels  the world giving lectures on the subject. 
You might think that Bullimore’s  turnaround is thanks to a wonder drug that  has brought his schizophrenia under control.  On the contrary: it was the side effects of his  medication that had brought him so low.  Instead, he opted for a seemingly radical  course of action - he was slowly weaned off his  medications and started a new type of therapy. 
Bullimore’s experience may be an extreme  case, but we have long known that the drugs  used to treat schizophrenia are very far from  ideal. The downsides have always been seen  as a necessary price to pay for relief from the  condition’s devastating symptoms, but now  that idea is being called into question. Not  only are the side effects of these drugs worse  than we thought; the benefits are also smaller.  Although people need to be taken off their  drugs slowly and carefully to avoid a relapse,  it looks as though outcomes are better in the  long run if medication is kept to a minimum. 
Now, there is growing interest in less  damaging ways of helping people with the  condition - including talking therapies  and even forms of brain training. “People  are starting to think differently about  schizophrenia,” says Max Birchwood, a  psychologist at the University of Warwick in  the UK. “Attitudes are definitely changing.”  Since it was first described by European  psychiatrists in the late 19th century,  schizophrenia has often been seen as the  most fearsome of all mental illnesses. Those  affected usually start behaving oddly in their  teens or 20s: hearing voices or seeing things  that aren’t there, often coupled with paranoid  delusions, such as that members of their family  want to kill them. These periods of psychosis  may come and go unpredictably over the  years, and they can be life-wrecking; 1 in 10  people with schizophrenia commits suicide. 
Bullimore was 29 when it first hit. 
Ostensibly his life was on track: he ran a  manufacturing business and was married  with three children. But during a period of  stress and overwork, things started to go  badly wrong. He became convinced that cars  were following him, and heard voices calling  him a pervert. He saw the horror-film 
character Freddy Krueger looking back at him  from mirrors. “It was a very frightening time,”  he says. 
After a particularly terrifying hallucination  one night, the next day, Bullimore smashed  his business partner over the head with a  telephone, then went home and curled up in  a chair. “1 stopped there for three weeks,” he  says. “All the voices were really, really bad.” 
The causes of schizophrenia are  frustratingly mysterious. A long-standing  theory is that the strange symptoms stem  from a person’s inability to distinguish  between their own thought processes and  inputs from the outside world. The  imagined voices often say things the person  could plausibly be thinking themselves,  for instance (see “Life in the chatter box”,  page 38). But that doesn’t so neatly explain  the hallucinations and delusions, nor the  memory and concentration difficulties that  often come with schizophrenia. 
Many genes that raise the risk of  schizophrenia have been discovered, most  of which seem to affect brain development or  functioning - suggesting that the condition  arises when something goes wrong with the  brain’s wiring as it develops and matures  during adolescence. The prevailing theory  is that the problems lie in neural networks  that use the brain chemical dopamine, in  part because drugs such as LSD and  amphetamines, which can cause symptoms of  psychosis, are known to raise dopamine levels. 
Until the 1950s, there was little that doctors  could do for someone like Bullimore, other  than lock them up in an asylum and sedate  them with strong tranquillisers called  barbiturates. But then a new class of drugs  was developed that proved helpful in treating  people in the grip of acute psychosis. These > 
"I saw Freddy  Krueger  looking back  at me from  mirrors" 
The Human Brain | NewScientist: The Collection! 57 
"Therapists can  now accompany  patients into  their private hell  using virtual  reality" 
antipsychotics, as they became known, could  calm people who were distressed or shouting,  without knocking them out like tranquillisers  did. The drugs were found to block dopamine  signalling, bolstering the theory that  overactivity of these pathways caused  schizophrenia. 
As wider use of antipsychotics allowed  people with schizophrenia to live in the  community rather than a psychiatric hospital,  they are often credited with bringing an end  to the often inhumane asylums. But right  from the start these drugs were known to  have unpleasant side effects. 
Mental fog 
The most obvious effects were physical: the  slowing down and stiffening of movements.  After a few weeks on the drugs, some people  start to get strange tics and spasms of their  face muscles. But the biggest complaints are  about the way the drugs affect a person’s  thoughts. Antipsychotics seem to slow down  people’s thinking, worsening the memory  and concentration problems caused by the  condition itself. ‘‘My head was clouded and  1 couldn’t think,” remembers Bullimore. A  recent study has confirmed suspicions that  long-term use actually shrinks the brain. 
They can also make people feel both  unhappy and highly agitated, a potentially  lethal combination, says psychiatrist David  Healy, head of the North Wales Department of  Psychological Medicine, Bangor, UK. His study  of historical records from a Welsh mental  hospital showed that lOO years ago people 
wJ th schizophrenia were no more likely to  kill themselves than the general population.  ThEs suggests it is modern drugs that cause  sc hliEophrenia’s high suicide rate, he says.  “They can produce some of the most  uncomfortable experiences a human can have.” 
Yet the potential side effects were seen as  the necessary cost of controlling a dangerous  Illness. Scores of trials had shown that after  a person’s initial psychotic breakdown had  been brought under control, if they stopped  taking their medication they were at higher  risk of relapse. 
Those studies were short, though, typically  lasting from months to a year, with the longest  being two years. Now for the first time there  has been long-term follow-up of a randomised  trial comparing people who reduced their use  of antipsychotics with those who continued  their treatment. The findings have sent shock  waves through the world of psychiatry. 
In this Dutch study, while the people  assigned to the dose-reduction group initially  had a higher relapse rate, after two to three  years, the people who stayed on their drugs  had “caught up”, and after seven years  differences between the two groups were  statistically insignificant (see graph, right). 
More importantly, those in the dose-  reduction group had more than double the  chance of achieving what psychiatrists call  “functional recovery” - 40 versus 18 per cent. 
In other words, even though they  might have occasional symptoms, they could  hold down jobs and look after themselves.  “That’s what’s meaningful to the patient,”  says Lex Wunderink, a psychiatrist at  Friesland Mental Health Services in  Leeuwarden, the Netherlands, who led  the study. Wunderink speculates that this  ability to function independently is being  hampered by the dopamine-suppressing  effects of antipsychotics. 
As the case against the drugs mounts,  some are beginning to question whether the  dopamine theory itself is right. After all,  there has never been strong evidence that  people with schizophrenia have overactive  dopamine signalling, says Ioanna Moncrieff,  a psychiatrist who has written a polemic  against antipsychotics called The Bitterest  Pills. Along with others, Moncrieff believes  antipsychotics may simply be another  version of the tranquillisers used back in  the 1950s. “If someone’s preoccupied by  their psychotic symptoms, if you can  dampen down their thinking, they lose  interest in their delusions,” she says. 
Today, there are rival theories about the  causes of psychosis. Some cases may be 
caused by an autoimmune reaction to certain  proteins on the surface of brain cells. Other  research implicates different brain chemicals,  including glutamate and serotonin. Several  compounds that boost glutamate signalling  in the brain have reached early clinical trials,  although it is too soon to say if they will pass  the larger trials needed to prove their worth. 
In the meantime, the problems with  antipsychotics are leading to growing interest  in a range of alternatives to medication. The  most promising are talking therapies like  cognitive behavioural therapy (CBT), which  aims to train people in new ways of thinking.  CBT is often used for depression and anxiety  to combat negative thought patterns, but  psychiatrists have been sceptical about its  usefulness for schizophrenia. “People say  CBT can’t possibly work - schizophrenia is an  intrinsic brain disorder,” says Birchwood, who  helped pioneer CBT. “How can talking therapy  change anything to do with the brain?” Yet  many studies have shown it to be useful. 
There are at least two possible explanations.  For starters, people are more likely to descend  into psychosis if they are stressed and  unhappy - as Bullimore did. “It’s a very stress-  sensitive disorder,” says Birchwood. Many of  the talking therapies help people cope better 
What makes a recovery? 
People with schizophrenia are often forced to take  antipsychotic drugs for the rest of their lives 
Short-term studies had suggested that drugs reduce  the risk of relapse - but a recent paper indicates that  the longer-term picture is different 
The longest previous  study lasted 2 years 
 Time from start of study (years) 
Life without drugs 
What's more, people weaned off the drugs have a  greater chance of a "functional recovery" - the ability  to hold down a job and look after themselves, even if  they have occasional symptoms 
Minimal or no drugs  With drugs 
 58 1 NewScientist: The Collection | The Human Brain 
SOURCE: JAMA 
 with everyday problems, such as family  arguments, reducing the stress that could  trigger a breakdown. 
Another benefit of talking therapies is that,  while unable to eliminate the voices and  hallucinations, they do help people feel less  disturbed by them. One goal of CBT is to help  people realise the voices don’t have any power  over them. “That enables them to disengage  from the voices,” says Birchwood. 
Rather than CBT, Peter Bullimore received  help from informal group therapy that  explored the psychological origins of his  troubles. Bullimore was sexually abused from  the age of 5. “The voices would repeat what the  abuser had said,” he says. “This was an area of  my life 1 hadn’t dealt with.” 
And it maybe possible to enhance the  power of talking therapy with a new  computer-based technique designed  specifically to combat aggressive voices,  which looks promising from a small pilot  study. Patients were helped to make a  computer avatar that “embodies” the voice  in their head. Sitting in another room, the  therapist then had their speech digitally  altered so they could be the voice of the avatar,  speaking to the patient through the computer  monitor. “We accompany the patient into  their own private hell,” says lulian Leff, a  psychiatrist at the Institute of Psychiatry in  London, who designed this approach. 
Over several sessions, the patient was  encouraged to stand up to the avatar, while  the therapist made it become less aggressive  in response. The approach helped 15 out of 16  people in the study, who found that it reduced  the frequency and intensity of the voices.  Three people even reported that they stopped  hearing the voices altogether. “What they  learn to do with the avatar they can then do  outside the sessions,” says Leff. 
A different approach is to target the  memory and concentration problems that  plague people with schizophrenia. Once  done with pen and paper, there are now  several “brain training” computer programs  in trials that are marketed specifically for this  condition. Typically they comprise a range of  tasks designed to improve people’s mental  skills in a variety of ways, particularly  memory, attention and logical reasoning. 
At the least, this should help people stay  in work or education - but the benefits may  be even greater. Some think the cognitive  problems could lie behind the psychosis,  perhaps because they lead people to mix up  external sensations with their own thoughts.  Carefully targeted brain training programs  could reverse the core symptoms of 
psychosis if the illness is caught early enough,  says Sophia Vinogradov at the San Francisco  Medical Center. She says a small study done by  her group has shown that brain training for  people in the early stages of schizophrenia  reduced psychotic symptoms. 
The turning tide 
It is much too soon to say whether brain  training can indeed reverse psychosis, but  talking therapies have certainly been shown  to reduce relapses. NICE, the agency that  produces clinical guidelines for the UK  National Health Service, recommends that  talking therapies should be offered to all  those with schizophrenia, in addition to  antipsychotic drugs. The British Psychological  Society has now also launched a report calling  for greater access to talking treatments.  Unfortunately, it is cheaper and easier to just  dole out the tablets. “Most [health] trusts have  not invested sufficiently in training to deliver  these services,” says Birchwood. 
And many doctors think the evidence  favours the continued use of antipsychotics.  “Nobody would say that antipsychotics are  perfect, but they are effective in preventing  relapse,” says David Taylor, head of pharmacy  at the Maudsley Hospital in London, the UK’s  largest psychiatric teaching hospital. While  the seven-year Dutch study suggests that  people do better in the long-term without  medication, that needs replicating before it  changes practice. “It is something that needs  more investigation,” he says. In Taylor’s  experience, people can avoid some of the 
worst side effects by switching medicines.  “They can usually find a drug which is  reasonably well tolerated,” he says. 
And when it comes to people in the throes  of a severe psychotic breakdown, Taylor says  antipsychotics are the only option. “Acute  psychosis is not a pleasant condition. It’s  extremely frightening and debilitating,” he  says. “The more rapidly those symptoms can  be relieved the better.” 
Indeed, most of those who favour  alternative treatments agree drugs are  unavoidable at such times. But subjecting  people to a lifetime of compulsory  antipsychotics seems to be on the way out. 
The tide is already turning in some parts of  the world, with Finland minimising drug use  and New York experimenting with such a  policy. The Finnish scheme’s success is gaining  worldwide attention, and it seems likely that  other countries will follow their lead. 
There are also efforts to give people who  hear voices practical support to continue with  their lives, such as sheltered accommodation  or supported employment. “It is possible for  people to have ongoing symptoms and yet  hold down a job,” says Birchwood. 
It’s a transformation that would be  welcomed by Bullimore. These days he still  hears voices, although now they are quieter  and are usually friendly, or at least neutral. 
He sometimes hears his dead mother giving  guidance, for instance, and another voice  helped him write a book. “That was my  creative side,” he says. “My relationship with  my voices has changed. It has woken me up to  a new world.” ■ 
The Human Brain I NewScientist: The Collection I 59 
 u ..collaboration is  the cornerstone oF  our strategy 
 'Brain cancer research is getting  really collaborative. It's a cross  and multidisciplinary team that's  getting together to solve  this complex problem.* j / 
Jim 
- Michelle Stewart, Head of Research  Cure Brain Cancer Foundation /VjTi 


mt n 
Cure Brain Cancer 
FOUNDATION 
Many minds, one purpose 
Find out more about us  curebraincancerorg.au/research 
Depression that resists every treatment is on the rise, but luckily  the key to a cure may already be in our hands, says Samantha Murphy 

O NE OF Vanessa Price’s first chronic 
cases involved a woman we’ll call Paula.  Paula came to the London Psychiatry  Centre, where Price is a registered nurse, after  two years of unrelenting depression. First she  stopped seeing her friends. Then she stopped  getting out of bed. Finally, she began cutting  herself. Sessions with a psychiatrist didn’t  help, neither did medication. In fact, they  made it worse. Paula had joined the ranks of  people diagnosed with treatment-resistant  depression. 
The steady rise in this diagnosis over the  past two decades reflects a little-known trend.  The effectiveness of some antidepressant  drugs has been overstated, so much so that  some pharmaceutical companies have  stopped researching them altogether. 
The stubborn nature of these cases of  depression has, however, spurred research  into new and sometimes unorthodox  treatments. Surprising and impressive  results suggest that we have fundamentally  misunderstood the disorder. 
In fact, the new research has opened the door  to thinking about depression not as a single  condition but as a continuum of illnesses,  all with different underlying neurological  mechanisms, which may hold clues to lasting  relief. This promise has sparked a renaissance  in drug development not seen since the 1950s. 
Depression is an illness whose brutality is  matched only by its perverseness. Estimates  vary, but it is likely that close to one in six of us  can expect to struggle with it at some point in  our lives. The symptoms are cruel - including  insomnia, hopelessness, loss of interest in life,  chronic exhaustion and even an increased risk  of ailments such as heart disease. Depression 
also leads people to cut themselves off from  others, a tendency exacerbated further by the  continuing stigma surrounding the condition,  thought to deter over half of depressed people  from seeking treatment. Untreated, depression  can lead to suicide; the World Health  Organization estimates there is one suicide  every 40 seconds. These factors all contribute  to the WHO’s assessment of depression as the  leading cause of disability in the world. 
What causes people to become depressed?  The dominant theory is that depression  results from a chemical imbalance in the  brain, with the neurotransmitter serotonin as  the prime suspect. Many trials have linked  depression to low levels of serotonin,  something that was thought to disrupt the  brain’s ability to pass messages across  synapses, the tiny gaps between neurons. 
Mysterious decline 
The theory was that a boost in serotonin should  return neural signalling and mood to normal  levels. The first drug based on the serotonin  hypothesis - fluoxetine, better known as  Prozac - was launched in the late 1980s, and  nearly all subsequent antidepressants have  operated on the same general principle: keep  levels of serotonin high by preventing the  brain from reabsorbing and recycling it. 
Although such drugs remain the go-to tools  for lifting depression, however, they seem to  be getting less effective (see '‘False dawn”,  page 62). Clinical trials in the 1980s and  1990s indicated that these drugs would help  80 to 90 per cent of depressed people go into  remission. But studies in the 2000s showed  that standard antidepressants work only in 
60 to 70 per cent of people, a decline that was  underscored in 2006 when the National  Institute of Mental Health (NIMH)  in Bethesda, Maryland published the results  of a massive, nationwide clinical trial. Unlike  many pharmaceutical trials - which often  screen out certain participants - this was  the first to measure the effectiveness of  antidepressants in a population representative  of the real world. The results were disquieting:  few of the 2876 participants fully recovered  without switching to or in many cases adding  other medications. 
What can explain this apparent decline in  the potency of antidepressants? Perhaps the  drugs themselves were never quite as effective  as claimed. To approve a given antidepressant,  the US Food and Drug Administration only  requires two large-scale studies to verify that  the drug is superior to a placebo. However,  pharmaceutical companies are under no  obligation to supply the FDA with every study  they have conducted; only the positive ones. 
When David Mischoulon, director of  psychiatry research at Massachusetts General  Hospital in Boston, sifted through previously  unpublished data from pharmaceutical trials,  he says he found many more negative results  than positive ones: a high percentage of  studies showed that the drugs were only  slightly better than the placebo. “Now we  think it’s more in the neighbourhood of 50 per  cent of people who may respond to a given  antidepressant,” Mischoulon says. So the rise  of treatment-resistant depression might be a  reflection of the time it has taken doctors to  see that reality reflected in their clinics. 
The next question then is why - could the  drugs’ failure be down to a problem in our > 
The Human Brain I NewScientist: The Collection I 61 
"After 15 sessions of magnetic  stimulation, getting out of bed began  to seem like a good idea to Paula" 
understanding of the underlying mechanism?  After all, untreatable depression wasn’t the  only inconsistency to cast doubt on the  serotonin hypothesis. A 2007 study, for  example, showed that serotonin levels in  the brains of depressed people not receiving  treatment were double those in volunteers  who were not depressed. 
In the wake of this confusion, several  pharmaceutical companies decided to stop  their work on mood disorders altogether.  GlaxoSmithKline - the company that makes  the well-known antidepressants Paxil and  Wellbutrin - announced in 2010 that it would  halt research into depression. 
Without new drugs to help the growing  number of people whose depression seemed  incurable, clinicians found themselves in  a bind. ‘‘We got used to telling our patients  to hang in there,” says Carlos Zarate, a  neurobiologist who directs research on mood  disorders at the NIMH. While they waited for a  drug to start working, doctors relied on intense  and frequent therapy to ensure depressed  people didn’t lose their jobs or attempt  suicide. That strategy wasn’t always effective.  “I felt like a failure,” Paula says. After nothing  worked, she took an overdose of sleeping pills.  It wasn’t that she wanted to die, she says; she  simply didn’t care if she lived or not. 
Last resort 
Desperate to help their charges, some  frustrated clinicians began to look for new  therapies. Their investigations were all over  the map: electrical and magnetic brain  stimulation, and a veterinary tranquilliser  known as ketamine. But they worked. 
After drug treatment and behavioural  therapy failed, what saved Paula was a  groundbreaking therapy called repetitive  transcranial magnetic stimulation (rTMS). 
It was the stuff of movies. Paula would put a  cap on her head and sit under a big machine  for about 20 minutes while a brief electric  current passed through a small coil positioned  a few inches above her left temple, creating  a fleeting high-intensity magnetic pulse. 
After 15 sessions, Paula stopped wanting  to hurt herself. Getting out of bed began to  seem like a good idea. When her friends  dragged her to a concert, she was surprised  to find herself enjoying it. “That would have  been unthinkable before,” she says. 
Price was surprised. “I have to be honest, 
I was dubious,” she says. “But I am absolutely  stunned by the results.” Price and her team  have now treated 99 people there using rTMS, 
of which 64 per cent made a full recovery. 
Price’s experience is reflected in a growing  body of research over the past few years, which  finds that rTMS seems particularly effective  against treatment-resistant depression. In one  study, it benefited 12 out of a group of 28  people for whom nothing else had worked.  And a review of 18 studies found that people  with treatment-resistant depression who were  given rTMS were five times more likely to go  into remission than those receiving a sham  treatment. 
At the moment, rTMS treatment is not  cheap. In the UK, the procedure is not available  on the National Health Service, so the people  treated at Price’s clinic have to shell out  around £8000. Australia’s Medical Services  Advisory Committee have decided there is  insufficient evidence that rTMS works and so  have declined to fund such treatments. 
In the US, some clinicians have turned to a 
False dawn 
New antidepressants that boost serotonin levels  looked promising when launched in the 1980s.  But since then, depression appears to have  become more common and more stubborn 
Percentage of US  population being  treated for depression 
 Number of SSRI  antidepressants  on market 
Percentage of people  with depression whose  condition does not  respond to treatment 
 1990 2006 
Mentions of  "Treatment-resistant  depression" in medical  literature 
 more affordable option that shows similar  promise: cranial electrical stimulation. It  simply involves delivering a tiny current with  two electrodes strapped to the head using a  sweatband. Unlike rTMS equipment, which is  bulky, this device is roughly the size of a deck  of cards and is available with a prescription. 
Stephen Xenakis, a doctor who is also a  retired general and a former adviser to the US  Department of Defense, uses the device not  only on his patients, but also on himself. He  asks his patients to use it for 20 minutes at a  time, twice a day. “Sometimes this can help in  ways that the medications don’t,” he says. “The  thing I’ve seen it help most with is insomnia  and anxiety”, conditions which both fuel, and  are fuelled by, treatment-resistant depression. 
But the most promising option in terms of  convenience could be the drug ketamine. As  early as 2000 a study of eight people with  long-standing, untreatable depression  suggested that a single dose of ketamine, given  intravenously, would almost immediately  lift symptoms. 
Several studies have replicated the results.  In a clinical trial involving 72 participants,  researchers from the Icahn School of Medicine  at Mount Sinai in New York found that people  who’d failed to respond to any other  treatments experienced relief from suicidal  thoughts when given ketamine intravenously  for 40 minutes. Zarate says that a growing  body of research suggests the drug could work  for 60 per cent of patients. “Some people go  into remission within a day,” he says, and can  remain free from depression for up to 10 days. 
But what mechanisms might explain the  success of a seemingly unrelated group of  treatments where traditional ones had failed?  When researchers began to piece together the  results, the link they found was glutamate. 
Glutamate is the most dominant  stimulatory neurotransmitter in the brain,  playing a key role in learning, motivation,  memory and plasticity. Some researchers  think that levels of glutamate, like serotonin,  are too low in the depressed person’s brain. 
But that’s where the similarity ends. Rather  than simply aiding in the transport of  messages between neurons, glutamate may be  a factor in helping the brain’s neurons repair  themselves. This would dovetail with a theory  of depression that has gained a significant  following in recent years: that depression  causes some dendrites - message-relaying  “fingers” at the ends of neurons - to shrivel.  The synapses become like broken bridges,  with messages unable to cross between the  affected neurons. Among other evidence to 
62 1 NewScientist: The Collection I The Human Brain 
 support this theory is the finding that each  successive episode of depression seems to  leave people more vulnerable to a subsequent  episode (see graph, right). 
The ketamine trials were the first clue that  glutamate might help. Ketamine sets off a  complex chain reaction. First, it blocks the  specific receptors that glutamate binds to,  releasing a tide of the chemical into synapses.  That leads to an increase in a protein called  brain-derived neurotrophic factor which,  animal studies show, causes the dendrites to  sprout new spines, helping them to receive  messages from neighbouring neurons. 
When Ronald Duman of Yale University  injected rats with ketamine, he saw a burst  of glutamate in rodents’ prefrontal cortex -  along with a fast increase in the formation of  new synapses. Other studies show that rTMS  also raises glutamate levels to cause similar  structural effects. 
Instead of enabling a broken brain to pass on  messages in spite of damage, then, glutamate  may be teaching a depressed brain how to  rebuild itself. The feeling, Zarate says, is that in  some cases, depression maybe better explained  as a disorder of neuron structure than being  due to a chemical imbalance. But that doesn’t  necessarily mean serotonin is out of the picture. 
‘T don’t think we were wrong,” says  Mischoulon. “1 think we didn’t have the whole 
High risk 
The risk of relapse increases substantially with every  episode of depression, a possible clue to an  underlying physical mechanism 
I 
After first episode 

 After second episode  After third+ episode 
0 20 40 60 80 100 | 
Percentage of people who relapse within S 
five years of recovering from depression 9 , 
Story.” The Diagnostic and Statistical Manual  of Mental Disorders, the bible of psychiatry  in the US, already subdivides depression  into categories, including postnatal and  bipolar, but it considers their underlying  neurophysiological mechanisms to be the  same. The new research could change that.  “We’re now thinking that there are probably a  wide continuum of illnesses lumped together  under the heading of depression,” he says, with  either glutamate or serotonin as the culprit. 
New beginnings 
If SO, how will individuals know which type of  depression they have? One way to find out  would be to see which drugs are effective. “If  you don’t get a response from ketamine the  first day, you probably never will,” says Zarate.  Work to develop a diagnostic test is already  under way. “We’re trying to identify certain  factors in the blood associated with certain  subtypes of depression,” says Mischoulon.  Brain scans are another possibility: these can  already show whether a person will respond  better to talk therapy or medication. 
All this research is still very much in its  infancy, but well before biomarker tests arrive,  there should be a raft of new medications that  exploit glutamate to combat depression. At  least five companies have been working on  ketamine derivatives. One example is GLYX-13,  which is showing promise in clinical trials.  Several pharmaceutical companies are also  developing pills and intravenous drugs, the  first of which should be with us within a  couple of years. Zarate says some  pharmaceutical companies are even focusing  on glutamate drugs for first line use rather  than as a last-resort treatment for depression. 
One tantalising possibility remains. If  glutamate affects neuroplasticity, could that  lead to lasting structural changes in the brain?  George Aghajanian at Yale, whose seminal  work inspired all the ketamine investigations,  says that in people predisposed to recurring  depression, ketamine may help neurons  permanently maintain new and thicker  connections. In recent work on rats, he found  that the drug, when combined with other  compounds, “leads to long-term structural  repairs in the brain”, he says. But whether the  same is true in humans will require much  further study. 
Whatever the future holds, glutamate - and  the new possibilities it has raised - has at least  enabled us to start thinking about depression  in a different way. That is rare in the troubled  waters of psychiatry. ■ 
The Human Brain I NewScientist: The Collection I 63 



 Why do bliss and ecstasy sometimes  accompany epileptic seizures? The  answer might shed light on religious  awakenings, joy and the sense of self,  says Anil Ananthaswamy 
 64 1 NewScientist: The Collection | The Human Brain 
I T WAS one of the most profound experiences  of Fyodor Dostoevsky’s life. “A happiness  unthinkable in the normal state and  unimaginable for anyone who hasn’t  experienced it. . . 1 am then in perfect harmony  with myself and the entire universe,” he told  his friend, Russian philosopher Nikolai  Strakhov. What lay behind such feelings? 
The description might suggest a religious  awakening - but Dostoevsky was instead  describing the moments before a full-blown  epileptic seizure. 
Those sensations seem to have informed  the character of Prince Myshkin in  Dostoevsky’s novel. The Idiot. ‘T would give  my whole life for this one instant,” the prince  says of the brief moment at the start of his  epileptic fit - a moment “overflowing with  unbounded joy and rapture, ecstatic devotion,  and completest life”. 
For a long time, the novelist was thought  to be exercising his artistic licence and  exaggerating this “ecstatic aura”, rather than  accurately representing a real phenomenon.  Most epileptic attacks are terrifying, after all,  and many people with epilepsy would give  a lot not to experience another. But as more  and more people with the condition have  come forward reporting the same feelings,  there has been a renewed interest in this  “Dostoevsky syndrome” - and neuroscientists  are now on the hunt for the cause. 
Besides explaining those feelings of bliss  experienced by Dostoevsky and other people  with “ecstatic epilepsy”, their investigations  could also open a window on self-awareness  more generally. The question is, are there safe  ways we could all be transported to similar  states of being? 
Epileptic seizures are broadly divided  into two groups: generalised and focal. In  generalised seizures, electrical discharges  overwhelm the outer layer of the brain, the  cortex, and often lead to loss of consciousness.  Ecstatic seizures seem to be of the second kind.  In focal or partial epilepsy, the electrical storm  is confined to a small region of the brain and  the person usually remains conscious. This  type of seizure can turn into a generalised one  if the errant electrical signals spread. 
Despite Dostoevsky’s famous accounts,  records of ecstatic feelings among other  people with epilepsy have been scarce -  perhaps because this kind of seizure is rare,  but also because people are reluctant to  divulge such personal feelings. “I think that  they are probably underestimated,” says  Eabienne Picard, a neurologist at the  University Ffospital in Geneva, Switzerland. 
“Because the emotions are so strong and  strange, maybe they feel embarrassed to speak  about them; maybe they think the doctor will  find them mad.” 
Picard’s interest in the subject was piqued  when she came across Dostoevsky’s writings  while making the film Art & Epilepsy. She soon  realised that some of her patients were having  very similar experiences. “When they really  explained their feelings, it was incredible,”  says Picard. “It was very close to Dostoevsky’s  descriptions.” 
Unbelievable harmony 
As Picard cajoled her patients to speak up  about their ecstatic seizures, she found that  their sensations could be characterised using  three broad categories of feelings. The first  was heightened self-awareness. For example,  a 53-year-old female teacher told Picard:  “During the seizure it is as if I were very, very  conscious, more aware, and the sensations,  everything seems bigger, overwhelming me.”  The second was a sense of physical well-being.  A 37-year-old man described it as “a sensation  of velvet, as if I were sheltered from anything  negative”. The third was intense positive  emotions, best articulated by a 64-year-old  woman: “The immense joy that fills me is  above physical sensations. It is a feeling of  total presence, an absolute integration of  myself, a feeling of unbelievable harmony of  my whole body and myself with life, with the  world, with the All’,” she said. 
When I met another one of Picard’s patients,  a 41-year-old Spanish architect, she talked of  that same connectedness. “You are just feeling  energy and all your senses,” she said. “You take  in everything that is around, you get a fusion.” 
As Picard began looking for the neurological  origin of the disorder, such descriptions  pointed her towards the insula - a region of  the cortex that is of growing interest to  scientists studying consciousness. It is buried  inside the fissure dividing the frontal and  parietal lobes from the temporal lobe, and its  main function seems to be to integrate  “interoceptive” signals from inside the body,  such as the heartbeat, with “exteroceptive”  signals such as the sensation of touch. 
There is also evidence that the processing of  these signals gets progressively more  sophisticated looking from the back of the  insula to the front. The portion of the insula  closest to the back of the head deals with  objective properties, such as body  temperature, and the front portion, or  anterior insula, produces subjective feelings > 
The Human Brain I NewScientist: The Collection I 65 
 Dostoevsky described  his seizures as  "unthinkable happiness' 
 "BRAIN STIMULATION TRIGGERED A PLEASANT  FLOATING SENSATION AND A 'SWEET SHIVER'  IN THE PATIENT'S ARMS" 
of body states and emotions, both good and  bad. In other words, the anterior insula is  responsible for how we feel about our body  and ourselves, helping to create a conscious  feeling of “being”. This led Bud Craig at the  Barrow Neurological Institute in Phoenix,  Arizona, to argue that this part of the brain is  the key to “the ultimate representation of all  of one’s feelings - that is, the sentient self”. 
Mapping ecstasy 
The altered self-awareness that Picard’s  patients experienced would certainly  implicate the anterior insula in ecstatic  epilepsy - but more direct evidence was  difficult to come by. Over the past few years,  Craig, Picard and their colleagues have  managed to find a few people with ecstatic  epilepsy who agreed to have their brains  imaged during seizures. The researchers  injected the patients with “nuclear tracers”,  which accumulate in different parts of the  body and are detected using a device called  a gamma camera. Areas with higher blood  flow absorb more of the tracer, and the scans  revealed increased blood flow - which is  assumed to reflect higher neuronal activity -  at or near the anterior insula during seizures. 
But such imaging studies cannot be  conclusive because they cannot pinpoint  the hyperactive regions precisely. It takes  about 30 seconds for active brain regions  to absorb the tracers, but seizures usually  spread rapidly to many different regions,  making it difficult to locate their origins  with certainty. 
More concrete proof didn’t come until  March 2013. 1 was visiting Picard in her office  in Geneva at the time, when she received an  email from Fabrice Bartolomei, a neurologist  at Timone Hospital in Marseille, France.  Bartolomei’s surgical team had implanted  electrodes inside the brain of a young woman  suffering from epilepsy with episodes of  ecstatic seizures. Bartolomei’s message read,  “We have explored the patient... The  stimulations in the anterior insula trigger a  pleasant sensation of floating and chills.”  Picard shot off a quick reply: “I’m so happy!” 
Bartolomei’s patient was a 23-year-old  woman. She started having seizures when  she was 15, and stopped going to school as  a result. She also had a difficult personality  with aggressive, sociopathic tendencies. 
Even so, before her seizures rendered her  unconscious, they always began with  moments of ecstasy, much like Dostoevsky’s. 
Because drugs were not effective in treating  the woman’s epilepsy, she gave Bartolomei the  go ahead to insert electrodes into her brain to  find the focus of the seizures and possibly  surgically remove the tissue that was setting  off the attacks. Bartolomei’s measurements  suggested that the seizures began in the  temporal lobe but spread to the anterior  insula in less than a second - supporting the  idea that hyperactivity in this region was  triggering the blissful feelings that preceded  the generalised seizure. 
Next, Bartolomei used the electrodes to  stimulate the young woman’s brain in specific  places. The technique allows surgeons to  double-check that they have found the cause  of the seizure, and helps prevent them  damaging or cutting away any key brain  tissue. It is also the best way to determine the  function of different brain regions. Much of  what we have learned about the brain has  come from people who have undergone this  kind of exploration while conscious. 
Unfortunately, the procedure can be  uncomfortable, which caused Bartolomei’s  patient to become aggressive. But when the  electrode in the anterior insula was activated,  her feelings changed. “I feel really well with a  very pleasant funny sensation of floating and  a sweet shiver in my arms,” she said. These  sensations were identical to the ecstatic aura  that usually accompanied her epilepsy, she  said. Based on these tests, Bartolomei  suggested surgery, but the woman opted  against it. The experiences nevertheless gave  Picard some much needed evidence of the 
66 1 NewScientist: The Collection I The Human Brain 
anterior insula’s role in ecstatic seizures. 
More studies will be needed to confirm the  effect, but Anil Seth, a neuroscientist at the  University of Sussex in Brighton, UK, is  impressed by these findings. ‘‘The fact that the  direct electrical stimulation of the insula does  elicit these kinds of feelings is pretty  compelling,” he says. He studies people with  depersonalisation and derealisation disorders,  which are associated with a dysfunctional or  underactive insula, and they describe the  world as being drained of sensory and  perceptual reality. In a way, a hyperactive  insula during ecstatic seizures produces the  opposite effect, he says. 
Investigating how abnormal activity in the  anterior insula leads to disorders like ecstatic  epilepsy might also help scientists establish  how this region creates our normal experience  of self-awareness. Picard’s patients reported  feelings of certainty - the sense that all is right  with the world - which would seem to fit with  a theory that the anterior insula is involved in  predicting the way the body is going to feel in  the next instant. Those predictions are then  compared with actual sensations, generating a  “prediction error” signal that might help to  determine how we react to a changing  environment. If the prediction error is small,  we feel good, if it is large we feel anxious. It is  possible that the electrical storm in the  anterior insula may be disrupting the  comparator mechanism, causing there to be  no prediction error. As a result, the person is  left feeling as if nothing is wrong with the  world, that everything makes sense. 
"it is uncanny how 
FEELINGS OF SERENITY,  HEIGHTENED AWARENESS  AND A SLOWING OF  TIME ALSO UNDERPIN J  MYSTICAL experiences! 
A 

 Besides the sense of expanded awareness  and certainty, people like Dostoevsky have  also recorded the strange sense that time is  slowing down during their seizures. This  might reflect the way the insula samples our  senses. Craig argues that the anterior insula  usually combines interoceptive,  exteroceptive and emotional states to create  a discrete “global emotional moment” every  125 milliseconds or so - dividing our feelings  into separate frames, like a film reel. He  posits that a hyperactive anterior insula may  generate these global emotional moments  faster and faster, leading to a sense that time  is slowing. 
Underfire 
It is uncanny how these feelings of serenity,  heightened awareness and a slowing of  time also underpin apparent religious  experiences. Have mystics over the ages  been having ecstatic seizures? Picard’s  patients could see why some might attribute  religious meaning to their seizures. “Some  of my patients told me that although they  are agnostic, they could understand that  after such a seizure you can have faith,  belief, because it has some spiritual  meaning,” she says. 
Needless to say, our understanding of this  crucial brain region and its role in ecstatic  epilepsy is still in its early stages. Neuro-  imaging studies sometimes come under fire  for oversimplifying complex brain  mechanisms hy pinning them to single 


 regions (for more on this, see “Hidden depths”,  page 24). Some might argue that the recent  work on the insula is no different. Most  experiences, after all, are the result of complex  networks of activity. 
It is important to recognise, for instance,  that the insula is responsible for bad feelings  as well as good, with studies showing that it is  often highly active during feelings of anxiety.  So it will be crucial to understand exactly what  sort of activity contributes to each feeling. 
That may depend on what’s going on  elsewhere in the brain, but a better  understanding may also come with  more detailed maps of the insula itself. 
There is some evidence that the left side is  more relevant for the positive feelings in  question, whereas the activation of the  right-hand side may be more closely linked  to negative feelings. Tellingly, some people  experiencing ecstatic epilepsy report  alternating pleasant and unpleasant  sensations - so scanning them during a  seizure might help researchers to elucidate  the basis of such emotions in more detail.  Researchers could then work out how the  different parts of the insula interact with  each other and function within broader brain  networks to produce everyday experiences. 
We could also gain insights into the  insula’s role by other means. Craig and  Picard think that feelings evoked by drugs  like amphetamine, ecstasy and cocaine  may share many similarities with ecstatic  epilepsy. These chemicals usually trigger  a flush of neurotransmitters through the  brain, and there is evidence that, following  drug use, levels of dopamine in the anterior  insula are unusually high relative to other  regions. The neurotransmitter serotonin  may be similarly implicated in the case of  ayahuasca, a psychedelic brew long  associated with shamanistic rituals in  the Amazon. Again, nuclear imaging results  show increased blood flow in the anterior  insula about 100 minutes after consumption. 
Fortunately, there may be safer ways to  come close to the same feelings. Meditators  often experience the time-slowing,  heightened self-awareness and feelings of  profound well-being that come with  Dostoevsky syndrome. In 2007, Richard  Davidson of the University of Wisconsin in  Madison and his colleagues studied 15 expert  and 15 novice meditators. They found that the  deeper the meditative state, the greater the  activity in the anterior insula. 
If that does reflect the same “unbounded  joy and rapture” that Dostoevsky’s Prince  Myshkin reported, it certainly doesn’t come  easily: the experienced meditators had logged  more than 10,000 hours of practice to see  these effects. You may not need to give your  “whole life for this one instant”, as Prince  Myshkin put it - but it may not be far off. S 
TheHumanBrainI NewScientistTheCollectionI 67 
The mind-altering effects of gut bacteria are finally being  understood. This knowledge offers a new way to improve our  mental health, say John Cryan and Timothy Dinan 
WE HAVE all experienced the influence of gut  bacteria on our emotions. Just think how you  felt the last time you had a stomach bug. Now  it is becoming clear that certain gut bacteria can  positively influence our mood and behaviour.  The way they achieve this is gradually being  uncovered, raising the possibility of unlocking  new ways to treat neurobehavioural disorders  such as depression and obsessive-compulsive  disorder (OCD). 
We acquire our intestinal microbes  immediately after birth, and live in an  important symbiotic relationship with them.  There are far more bacteria in your gut than  cells in your body, and their weight roughly  equals that of your brain. These bacteria have  a vast array of genes, capable of producing  hundreds if not thousands of chemicals,  many of which influence your brain. In fact,  bacteria produce some of the same molecules  as those used in brain signalling, such  as dopamine, serotonin and gamma-  aminobutyric acid (GABA). Furthermore,  the brain is predominantly made of fats,  and many of these fats are also produced by  the metabolic activity of bacteria. 
In the absence of gut bacteria, brain  structure and function are altered. Studies of  mice reared in a germ-free environment, with  no exposure to bacteria, show that such mice  have alterations in memory, emotional state  and behaviour. They show autistic patterns of  behaviour, spending as much time focusing  on inanimate objects as on other mice. This  behavioural change is driven by alterations in  the underlying brain chemistry. For example,  dramatic changes in serotonin transmission  are seen, together with changes in key  molecules such as brain-derived neurotrophic  factor, which plays a fundamental role in  forming new synapses. 
These findings give weight to the notion  of probiotics - bacteria with a health benefit.  Probiotics were first proposed by Russian 
biologist Elie Metchnikoff who, in the early  1900s, observed that people living in a region  of Bulgaria who consumed fermented food  tended to live longer. However, it now seems  that certain bacteria - dubbed psychobiotics -  might have a mental-health benefit, too. 
Although the field of psychobiotics is in  its infancy, there are already promising signs.  For instance, researchers from the California  Institute of Technology in Pasadena recently  showed that when the bacterium Bacteroides  fragilis was given early in life, it corrected  some of the behavioural and gastrointestinal  deficits in a mouse model of autism. And  previous reports indicate thdit Bifidobacterium  infantis is effective in an animal model of  depression. 
How exactly do gut bacteria influence the  brain? The mechanisms are becoming clear.  The bdiCterium Lactobacillus rhamnosus,  which is used in dairy products, has potent  anti-anxiety effects in animals, and works by  changing the expression of GABA receptors in  the brain. These changes are mediated by the  vagus nerve, which connects the brain and  gut. When this nerve is severed, no effect on  anxiety or on GABA receptors is seen following  psychobiotic treatment withl. rhamnosus. 
L. rhamnosus has also been shown to  alleviate OCD-like behaviours in mice. 
Microbiota with personality 
The transplantation of faecal  microbes (FMT) from a healthy  individual into a recipient has  emerged as an effective  treatment for life-threatening  Clostridium difficile infection. 
The success of this approach has  focused attention on FMT to  treat gastrointestinal, immune  and metabolic disorders, but 
Whatgoeson in ourgut may have profound  effectsonwhatgoesoninourmind 
Interestingly, this bacterium not only alters  GABA receptors in the brain but has been  shown to synthesise and release GABA. Other  evidence supports the view that gut bacteria  may influence the brain by routes other than  the vagus nerve - by altering the immune  system and via the manufacture of short-  chain fatty acids, for example. 
Just as certain genes render bacteria  pathogenic, it is likely that clusters of genes  within gut bacteria provide mental health  benefits. However, the essential genes  for effective psychobiotics have yet to be  established. It may be that, in the future,  the ideal psychobiotic will be a genetically  modified organism containing genes from  several different bacteria. 
In the meantime, cocktails of bacteria are  likely to be more effective than single strains  in producing health benefits. For example,  a 2011 study showed that a combination of  Lactobacillus helveticus dind Bifidobacterium  longum reduced anxiety and depressive  symptoms in healthy volunteers. A 2013  neuroimaging study showed that a fermented  milk product containing four different  probiotic bacteria was associated with the 
anxious - and vice versa. 
If such effects can be  translated to humans they  have marked implications for  development of microbial-based  therapies for mental disorders. 
It also means that would-be FMT  donors may need to be screened  for mental health issues as well  as infectious disease. 
could FMT be useful in treating  neuropsychiatric conditions too? 
Intriguingly, a 2011 study by  researchers in Canada showed  that anxious mice have different  microbiota compared with  normal mice, and that  transplantation of their  microbiota into the normal  mice makes the normal mice 
68 1 NewScientist: The Collection | The Human Brain 
reduced response of a brain network involved  in the processing of emotion and sensation.  And certain strains of bacteria can reduce the  symptoms of irritable bowel syndrome,  a common stress-related disorder of the brain-  gut axis. This is probably achieved through a  reduction in levels of the ‘‘stress hormone”  cortisol and of inflammatory molecules  produced by the immune system. 
These findings are promising, but we are still  a long way from the development of clinically  proven psychobiotics and it remains to be seen  whether they are capable of acting like - or  perhaps even replacing - antidepressants. At a  time when prescriptions for antidepressants  have reached record levels, effective natural  alternatives with fewer side effects would be  welcome. We have now completed a study of  the gut microbiota in people with severe  depression and are analysing the results. If we  find consistent alterations, this will provide a 
"You have more gut bacteria  than bodily cells, and they  are as heavy as your brain" 
Strong rationale for targeting depression  with a suitable psychobiotic. We have also  recently completed a placebo-controlled study  of Lactobacillus brevis in treating anxiety in  healthy volunteers. 
We must, however, sound a note of caution.  Despite marketing claims to the contrary,  most putative probiotics have no psychobiotic  activity. Until recently, lax regulation in both  the US and the European Union allowed  manufacturers to make outlandish claims  without supporting data. This situation is  changing and will protect consumers from  fraudulent marketing, but the reality is that  only a small percentage of bacteria tested  have positive neurobehavioural effects. 
Some bacteria fail to survive storage in  the health food store or are eliminated  by acidity in the stomach. Even if they do  survive gut transit, they may be devoid  of health benefits. 
In the 20th century, the major focus of  microbiological research was on finding  ways to kill microbes via antibiotics. This  century the focus has changed somewhat,  with a recognition of the health benefits  of bacteria, not just from an immunity  perspective but from a mental health one.  Today, in richer nations, the impact of stress  on health is perhaps as great as the threat  from harmful bacteria. Psychobiotics have  enormous potential. ■ 
The Human Brain | NewScientist: The Collection! 69 
KOHEI HARA/TAXI JAPAN/GETTY 
CHAPTER FIVE 
AGES AND SEXES OE THE BRAIN 
 We remember next to nothing from the time before  we go to school. Why is that asks Kirsten Weir 
 70 1 NewScientist: The Collection | The Human Brain 
 W HEN my younger sister was born, I was  almost 6. 1 woke up early the day after  Christmas and asked my teenage sister  where our parents were. “They’re at the hospital  having the baby,” she said. “Go back to bed.” 
1 remember that conversation clearly, but  the actual arrival of my baby sister? Nothing. 
I don’t recall visiting her in the hospital and  holding her tiny pink hand for the first time,  or my mother bringing her home and tucking  her into her crib in the room next to mine.  There is nothing unusual in the failings  Memories of our first of my early memory. In fact, “childhood 
few years remain sparse amnesia”, as the phenomenon is known, is  throughout life universal. Most people remember nothing 
from before the age of 2 or 3, and memories  from the next few years are sketchy at best. 
This is puzzling, because in other ways  children are phenomenal learners. In our first  couple of years we pick up many complex,  lifelong skills, like the ability to walk, talk and  recognise people’s faces. Yet memories of  specific events in our childhood are lost to us  in adult life. It’s as if someone has torn the first  few pages from our autobiography. 
So what causes childhood amnesia? 
The question has troubled psychologists  for more than a century, but at last we are  starting to see some plausible answers. The  new findings explain why some of us can  remember more of our childhood than  others, and even raise the question of whether  it might be possible to unlock those earliest  memories. 
The first serious study of the problem, by  the French psychologists V. and C. Henri, was  in 1898. The pair found that when adults were  asked about their earliest autobiographical  memories, the average age at which these  events occurred was just over 3 years. These  findings have been confirmed by numerous  later studies, which point to an average age  of between 3 and 3.5 years for the very first  memories. Even then, we still have notably  poor recall for the following 3 years or so, at  which point things start to become clearer.  There is a lot of variability, however: some  people seem to remember events before age 2,  while others recall nothing before 6 or even 8. 
Attempts to explain the phenomenon came  in fits and starts in the decades after the  Henris published their work. Sigmund Freud  put his mind to the problem in a 1905 essay,  concluding that we repress childhood  memories because they are full of sexual and  aggressive impulses too shameful for us to  face. That idea eventually fell by the wayside,  to be replaced by the view that young children  just can’t form explicit memories of events. 
The picture changed again in the 1980s,  with the first studies of children themselves,  rather than investigations of adults’ childhood  recollections. This revealed that children  as young as 2 or 3 can indeed recall  autobiographical events, but that these  memories fade away. The question therefore  became: What causes them to disappear? 
There appears to be no simple answer.  “We’ve come to this view that there are a  number of factors that coalesce to allow us  to retain our memories,” says Harlene Hayne  at the University of Otago in Dunedin, New  Zealand, who studies how memory abilities  change during childhood and adolescence. 
One of those factors may be the brain’s  anatomy. Two major structures are involved in  the creation and storage of autobiographical  memories: the prefrontal cortex and the  hippocampus. The hippocampus is thought  to be where details of an experience are  cemented into long-term memory. 
Broken bridge 
It’s here that the problem seems to lie. 
“We used to think the hippocampus and the  surrounding cortices were well developed  early on,” says Patricia Bauer, who studies the  development of memory during childhood at  Emory University in Atlanta. But in the past  15 years or so, it has become clear that one  small area of this region, called the dentate  gyrus, does not fully mature until age 4 or 5.  This area acts as a kind of bridge that allows  signals from the surrounding structures to  reach the rest of the hippocampus, so until the  dentate gyrus is up to speed, early experiences  may never get locked into long-term storage,  Bauer says. “If the route isn’t sufficiently  mature to allow the information to get in,  it’s not going to effectively consolidate.” 
Hayne agrees that the brain continues to  mature over a long period of development,  and that this is an important step in  establishing long-term memories. Yet children  can still remember some events before this  region is fully developed, so it can’t be the  be-all and end-all of childhood amnesia. 
What’s more, there are puzzling cross-  cultural differences in the age of earliest  memories. In one cross-cultural study, for  example, researchers found the average age  of first memories in people of European  descent hovered around 3.5 years, compared  with 4.8 years for east Asians and 2.7 years for  Maori people in New Zealand.“Those  differences cannot be explained by brain  maturation alone,” she says. Clearly, there > 
The Human Brain I NewScientist: The Collection I 71 
"Is it coincidental that autobiographical  memories emerge at the point a child is  able to give a story of their experiences?" 
must be more pieces to the puzzle. 
Mark Howe at City University London in the  UK thinks he has come across one of the other  important factors. “The thing that brings  childhood amnesia to an end,” he suggests, 
“is the advent of what we call a cognitive self.”  That’s our sense of our own uniqueness -  the understanding that the entity “me” is  different from “you”. This ability emerges  at around i8 to 24 months of age, just before  autobiographical memory begins to surface.  Could it be the answer? 
Over the past 10 years, Howe has explored  this idea through a series of experiments. 
In one of his recent studies, for example, he  tested whether toddlers could recognise  themselves in a mirror, a well-accepted sign  they have developed a sense of self. Next he  showed them a stuffed lion, which he then  tucked into one of several drawers in a set of  cabinets. Weeks later, he brought each child  back to the lab and asked if he or she  remembered where the lion was napping.  “The children who had a cognitive self at the  time of the lion event were able to remember  weeks later,” he says, “whereas children who 
TRUE OR FALSE? 
You have heard that adorable anecdote  from your childhood a million times. 
You can see the scene clearly in your  head. But is your recollection real, or  have you concocted a false memory  around an oft-told family tale? "That  question is the bane of memory  researchers " says Patricia Bauer. In fact  says Martin Conway, you can't wholly  trust any of your memories. "They  always contain missing information,  and I think they always contain  misremembered details as well." 
Unfortunately, it looks as if we are  particularly susceptible to creating  false memories relating to events  during the period of childhood  amnesia. When Harlene Hayne and  her colleagues primed subjects to  "recall" a childhood event that never  actually happened, they were much  more likely to create the false memory  if they were told it happened at age Z,  rather than age 10. That could have  important bearings for court cases  that rely on early memories, such as  those investigating allegations of  childhood abuse. 
didn’t have a cognitive self did very poorly.” 
Howe believes our sense of self helps us  to organise our memories, making them  easier to recall. “They become more  memorable and stay with you for longer  periods,” he says. Yet that can’t be the  whole story either, since memories continue  to be sparse long after the point at which a  toddler can recognise his or her reflection.  “The cognitive self is a necessary - although  maybe not a sufficient - condition for  autobiographical memory,” Howe concedes. 
The magic shrinking machine 
Some other factor is therefore needed to  explain why memories continue to be sparse  well beyond the point at which our cognitive  self appears. For Hayne, the extra ingredient  is the development of language skills. To  investigate, she asked a group of 2 to 4-year-  olds to play with a toy called the “magic  shrinking machine”. The kids had to place  an object in the machine and perform a series  of actions before an identical but smaller  version of the object popped out. Hayne also  recorded the words the kids could speak and  understand at the time they played the game. 
Then, six months to a year later, she  brought the children back and asked them  about the magic shrinking event. They  could remember the game and re-enact  aspects of it, but in no instance did they  use a word to describe the machine that  had not been part of their vocabulary when  they first played with it - even though their  vocabularies had grown by leaps and bounds  in the meantime. “Their ability to describe it  was really locked relative to their language at  the time of the event,” she says. 
Further evidence came in 2010 when Martin  Conway and Catriona Morrison both then at  the University of Leeds, UK, published a study  that again suggested the contents of our first  memories depend on our first words. They  asked adults to describe and date their earliest  memories associated with words like “ball”  or “Christmas”. It turned out that the earliest  memories around each cue word dated to  several months after the average age at which  the word is acquired. “You have to have a word  in your vocabulary before you’re able to set  down memories for that concept,” Morrison  concludes (for more on this, see “Memory: The  ultimate guide”, page 104). 
Perhaps a sense of self provides a  structure around which to organise  memories, and language then provides a  further kind of memory scaffold. 
Recognising your  reflection is a sign that  you have a sense of self 
 anchoring the details in a format we can  call up years later. Morrison suggests that  this may be because language allows a child  to construct a narrative, which might help  them to consolidate their memories. 
A 2-year-old can identify a dog, for example,  but it takes until about 4 before the child can  flesh out a story about their new pet. “Is it  coincidental that autobiographical memory  emerges at the same stage at which a child is  able to give you a narrative account of an  experience?” Morrison asks. 
Hayne and her colleagues have explored  the importance of narrative by recording  conversations between mothers and their  children at various points between the  child’s second and fourth birthdays,  noting whether each conversation included  “elaborations” (richly detailed descriptions)  or merely “repetitions” (which focus on  just one or two aspects of the event). Ten  years later, the team contacted the children  and asked them about their early memories.  This revealed that those whose mothers  had many more elaborations than  repetitions within their conversations  had distinctly earlier memories than  children of mothers who had a lower  elaboration-to-repetition ratio. In other  words, the way you talk to your kids when  they are young might shape what they will  remember years down the road. 
7Z I NewScientist: The Collection | The Human Brain 
MEMOIRS OF AN ELEPHANT 
 If our autobiographical memories  only emerge once we develop  language and a sense of self, does that  mean humans are alone in reminiscing  about the past? Some animals such as  chimps, elephants and bottlenose  dolphins pass the mirror self-  recognition test (see main story),  indicating they have some capacity for  self-awareness. And they can  definitely store long-term information. 
But according to Catriona Morrison,  animal memories are thought to be  conditioned responses to stimuli rather  than conscious (or self-conscious)  reflection. Without language and a  more sophisticated sense of self, it's  unlikely our non-human cousins have  autobiographical memories, Morrison  says. Harlene Hayne agrees. "Most  experts believe that autobiographical  memory is unique to humans," she says. 
This could also explain those puzzling  differences between cultures. Compared  with east Asian parents, European and North  American parents tend to discuss the past  more often with more elaborate storytelling.  As a result, their children have more early  memories. The Maori storytelling culture is  even richer, with detailed oral histories and a  strong focus on the past, leading to even earlier  memories. When it comes to autobiographical  memory, ‘‘early family memory sharing is  important”, says Qi Wang at Cornell University  in Ithaca, New York, who studies the  interaction of cognitive and social development. 
Mental time travel 
This may seem to confirm that language  skills are the key to retaining childhood  memories - but in fact the issue is not that  clear-cut. Talking about the past doesn’t just  help children develop narrative skills, it also  fosters development of a sense of self. “In  North American culture, people are crazy  about memoirs and reality TV. It’s all about life  stories,” Wang says, so parent-child  conversations in this culture tend to focus on  a child’s own experiences and feelings. Among  east Asians, by contrast, “the past is the way  for us to learn to do better in the future”. Asian  parents tend to use past events as teaching  tools, and do not dwell on the child’s feelings 
or role in the event. As a result, children in  these different cultures have different  understandings of their personal identities. 
It now looks as if language and self-  perception go hand in hand, and both are  necessary for autobiographical memory to  flourish. The findings could have a bearing  on our wider understanding of the mind. For  example, our capacity to plumb the depths of  our past appears to be intimately linked to our  ability to imagine the future. Given the ways  different cultures reflect on their past, you  might also expect differences in this “future  time travel” - and that’s exactly what Wang  has found. The work might even shed light on  the quality of other animals’ memories (see  “Memoirs of an elephant”, above). 
One big question remains, however: is it  ever possible to reclaim memories from that  period of our early childhood that is hidden  from us? It is clear that very young children  remember a lot in the short term. As many a  parent has witnessed, toddlers can accurately  describe a trip to the zoo that happened weeks  earlier. But such early recollections are fragile  and may never become locked into permanent  storage. “The likelihood is those early  memories are simply not there,” Bauer says. 
Hayne’s subsequent work supports the idea  that those early memories aren’t cemented for  later retrieval, even if the reminders come  soon after the event. She found that the 
amount of information a 20-year-old  remembers about the birth of his 15-year-old  brother is virtually identical to the amount of  information a 5-year-old remembers about his  brother’s birth just a month earlier. “If you  plot adult next to child data, they are virtually  identical,” she says. She concludes that these  memories aren’t simply forgotten as a person  ages. “The memory never got in there in the  first place,” she says. 
Others harbour hopes of being able to  recover these early memories, however. 
“I think they are retained but not accessible,”  Conway says. In his view, memories are  “snapshots” of sensory experiences. As you  mature, you develop language, a sense of self  and other conceptual knowledge that helps  you to frame those sensory snapshots and  access them. If he is right, our buried  memories could be excavated - if we could  only find the right cues. 
That’s a line of reasoning that Morrison  also follows. Reaching beyond traditional  memory cues of words and images, she is  exploring the use of smells, flavours and  music for calling up ancient memories. If she  and her colleagues can identify the proper  tools, perhaps one day I will be able to unearth  the memory of meeting my sister for the first  time. “One of the things we understand as  memory researchers,” Morrison says, “is  there’s a lot more in there than we realise.” ■ 
The Human Brain I NewScientist: The Collection I 73 

Studies of psychoactive stimulants and consciousness can  shine a light on how we viewed the wodd as an infant  finds Anil Ananthaswamy 
W HAT is it like to be a bat? Philosophers  of consciousness love toying with  that question. We’re fascinated by the  possibility of minds so unlike our own. But  there’s a deep mystery far closer to home.  Never mind bats - we barely even know what  it’s like to be a baby. 
We’ve all been there, but none of us  remember. As we develop into fully self-aware  beings, our subjective experience of the world  shifts dramatically. Once we leave infanthood  behind, that early window on the world - and  what it’s like to look through it - is closed to us. 
But research is prising open the shutters. 
As we learn more about how drugs can alter  our consciousness, we’re learning more about  how our brain states relate to subjective  experiences. And that’s giving tantalising  glimpses into our infancy. 
For those who want to get inside a baby’s  head, Alison Gopnik, a psychologist at the  University of California, Berkeley, has a few  suggestions: go to Paris, fall in love, smoke  four packs of Gauloises cigarettes and down  four double espressos. '‘Which is a fantastic  state to be in, but it does mean you wake up  at 3 o’clock in the morning crying,” she told  a room of philosophers and neuroscientists  at the Toward a Science of Consciousness  meeting in Tucson, Arizona, in April 2014. 
And if that wasn’t enough, Gopnik adds  another ingredient to the list: psychedelic  drugs. Because a baby’s world might be vivid  beyond adult imagination. 
To get a handle on the infant state of mind,  we first need to know what goes on in the 
brains of adults - then see how it differs in  babies. Fortunately, consciousness seems to  have a telltale signature. A team led by Stanislas  Dehaene of the French National Institute of  Health and Medical Research in Gif- sur- Yvette  has found that adult conscious perception of  stimuli involves a two-stage process. The first  stage involves unconscious processing of, say,  an image. If we look long or hard enough, then  after about 300 milliseconds, the second stage  kicks in, and a network of brain regions starts  reverberating. The activity correlates with  conscious perception: people are able to 
"A baby's world might  be vivid beyond adult  Imagination" 
report on what they have seen. It is only when  this network of frontal and parietal brain  regions, dubbed the global neuronal workspace,  becomes active that we have conscious access  to information about what we have perceived. 
Dehaene and his colleagues recently teamed  up with Sid Kouider of the Ecole Normale  Superieure in Paris, France, to look for a  similar signature in babies who were between  5 and 15 months old. In the first study of its  kind, the team spotted clear signs of conscious  perception. But there was one important  difference. In babies from 12 to 15 months old,  the second stage of reverberating neural  activity began about 750 milliseconds after  the onset of stimulus, rather than after 
300 milliseconds. And in 5-month-olds, the  lag was even greater. Their brains responded  after 900 milliseconds. “Babies have the same  mechanism, but are just slower,” says Kouider. 
So, babies are aware of their environment,  but, compared with adults, there’s a lag. 
The slower reaction could be down to the  prefrontal cortex, a hub for brain activity that  the studies looked at. “It allows the sharing  and transmission of information throughout  different regions of the brain,” says Kouider.  And it is one of the last brain regions to  mature, becoming fully developed only in late  adolescence. Another slowing factor might be  down to the connections between distant brain  regions. In infants, the long-distance axons  that carry signals in the brain don’t yet have  a fully formed coating of insulation called a  myelin sheath. This means signals travel more  slowly along the axons than they do in adults. 
But there’s more to the story. Kouider and  Dehaene are investigating something called  access consciousness - being aware enough  of a stimulus to reflect on it and talk about it.  Access consciousness is widely studied  because researchers typically depend on  subjects being able to monitor and report  their experience. But some think access  consciousness is just one extreme of a  spectrum. Is there middle ground between  being fully aware and fully unaware? Gopnik  thinks so. And that is where babies find  themselves, she says. 
Philosopher Ned Block of New York  University has a term for this middle ground. 
He calls it phenomenal consciousness - what > 
74 1 NewScientist: The Collection | The Human Brain 
 The Human Brain | Newbcientist: Ttie Collection | 75 

it's like to have a subjective experience such as  seeing, hearing, tasting, smelling or touching  something. Take vision. For Block, when we  observe a complex scene, we are conscious of  a lot more than we can put into words. 
Of course, subjective experience is a  slippery fish to study. But Block points to a  new experiment that backs up his ideas. Zohar  Bronfman of Tel Aviv University in Israel and  his colleagues devised a test to unpick these  layers of awareness. They showed subjects  grids with letters in varying ranges of colours.  At the start of the test, the researchers  highlighted one row of letters before  displaying the entire grid for 300 milliseconds.  The participants were told their task was to  recall a letter from the row that had been 
"When we pay attention,  we regress a little part of  our brain to childhood" 
highlighted, so they paid attention to that  row more than others. But having recalled a  letter from the row, they were then asked to  estimate the diversity of colours either in that  row or in one of the others. 
Bronfman found that people were just as  good at estimating colour diversity for the  rows that had not been the focus of attention  as they were for the ones that had. For some,  this is clear evidence that there's more to  conscious awareness than access  consciousness - which would only account  for the ability to recall individual letters. 
Working with Tim Sweeny at the University  of Denver in Colorado and his colleagues,  Gopnik carried out a similar test with infants.  They found that infants, like adults, are able to  make judgements about a collection of objects  without focusing on any particular one. The  team showed cartoon images of two trees, each  with oranges of varying sizes, to children aged  4 and 5. The children then played a game in  which they had to help a hungry monkey pick  the tree with the largest oranges. They chose  correctly more often when comparing groups  of oranges than when comparing individuals. 
So young children are good at making  judgements about groups. But they are less  good at focusing attention on particulars. If  adult awareness is like a spotlight that lets us  pay selective attention to things, an infant's  awareness is like a lantern, shedding diffuse  light on everything around, says Gopnik. That  may let them perceive many things at once. 
The upshot for Gopnik is that, instead of 
paying attention to individual things,  a baby is probably picking up patterns in  the bombardment of stimuli. And because  they are less able to control their attention,  babies are drawn to things that are rich in  information. For an adult, an infant's play  area can be a cacophony of colour and sound. 
A baby, however, is in its element. 
This inability to control attention probably  also means that babies are bad at shutting  things out. Take a deafening pneumatic drill  that has been hammering away outside your  window all morning. Block notes that adults  can tune out. If you're focusing on something  else, for example, you may suddenly notice  the drill only at midday. A baby, though, is  likely to find it hard to shut out the noise to  begin with. As an infant, the world may be  bright and brash, with no dimmer switch. 
Which brings us to the bustle of Paris, being  in love, and buzzing on coffee and cigarettes.  The differences between our adult experiences  of the world and those of our lost early years  are down to changes in the brain. But even as  adults, our brains remain relatively plastic. 
As our attention shifts, our pliable brains shift  with it, so perhaps there are ways to roll back  the years, at least temporarily. 
Michael Merzenich at the University of  California in San Francisco and his colleagues  have shown that when rats are trained to pay  attention either to the frequency or the  intensity of sounds, their brains rearrange  themselves. When the rats were paying  attention to frequency, relevant neurons were 
Psilocybin disrupts hubs in the brain, rewinding  them to when the ego was yet to emerge 

 recruited to the task - but no changes were  seen in nearby neurons involved with  processing intensity. And vice versa. 
It turns out that coffee and cigarettes  can drive similar changes. The activation  of certain parts of the brain for focused  attention is managed by the neurotransmitter  acetylcholine, which is mimicked by  nicotine. At the same time, inhibitory  neurotransmitters should work to stop other  areas from joining the party. Unless you are  drinking coffee, that is, because caffeine is  thought to keep the effects of such killjoy  neurotransmitters at bay, keeping your brain  alert to anything and everything. 
By smoking and drinking coffee you nudge  your brain into a state where you're paying  lots of attention - but in a wide-eyed,  indiscriminate way. Being in love and  travelling to new places seem to have a similar  effect, says Gopnik. Under these influences, we  get a more pliable, plastic brain. And that's a  fair approximation of what's happening with  babies, whose immature brains are more  plastic overall. Being a baby is like paying  attention with most of our brain. ‘As adults,  when we pay attention, we are regressing a  little part of our brain to its childhood state,"  says Gopnik. “We are taking a little part of us  and turning that into a 2-year-old again." 
Gopnik has another analogy to help us  get inside the head of our infant self. Think  what it's like to be totally immersed in an  engrossing movie. “You are not in control,  your consciousness is not planning, your self 
76 1 NewScientist: The Collection I The Human Brain 

Adults are better than young children at  focusing attention and shutting out distractions 
seems to disappear - that’s part of what’s great  about being absorbed in a movie,” she says.  “Yet the events in the movie are very, very  vivid in your awareness.” Being a baby might  be like being sucked into a really good movie. 
It gets stranger. In infants, this expansive,  along-for-the-ride experience of the world  may go beyond perception. Kouider is not  convinced by Block’s and Gopnik’s ideas about  phenomenal consciousness - for him there is  little to hold onto once you let go of access  consciousness. But he does think that infants  have a very different sense of self. 
In fact, we may start life without  recognisable self-awareness at all. Instead,  a baby’s sense of self is mixed up with its  awareness of other people. That means  babies may feel their own emotions and  the emotions of others, without being able  to tell them apart. “When the baby is having  an experience, it is probably richer and  much more intense, emotionally and  subjectively,” says Kouider. 
Magical childhood 
How does a baby feel the emotions of others?  Probably through imitation. Smile at a baby  and it smiles back. The very act of smiling is  thought to induce happiness, so by imitating  us, the baby feels emotions associated with  those actions. The same might happen for  other actions like waving or clapping.  According to Kouider, we all had to figure out  the boundary between ourselves and other  people through social interactions. A baby  learns to distinguish its own emotions from  those of others by realising that it can control  its own emotional state and behaviour but not  that of its parents, for example. 
The idea that infants might be  experiencing an unbounded sense of self  finds support from an unlikely source:  magic mushrooms. And this also gives us  another way to mimic infant consciousness.  Robin Carhart-Harris of Imperial College  London and his colleagues have been  studying the effects of psilocybin - the active  ingredient in psychedelic mushrooms - on  states of consciousness. They looked at the  network that connects regions in the  prefrontal cortex, the cingulate cortex and the  temporal lobes, among others. 
 Babies' brains show  the hallmarks of  adult awareness,  but there's a lag 
"The psychedelic state  offers a window into what  infant consciousness is like" 
Previous studies have shown that this  “default mode network” is active when we are  resting and when we are thinking about  ourselves, and suppressed when we  concentrate on a task. Carhart-Harris’s team  showed that psilocybin deactivates hubs in the  brain like the posterior cingulate cortex and  medial prefrontal cortex, as well as reducing  long-range connectivity between brain  regions. These hubs are like conductors of an  orchestra, says Carhart-Harris. Bring on  psilocybin and the conductors leave the room. 
The resulting dissonance has a striking  effect, disrupting our self-awareness. “It was  quite difficult at times to know where I ended  and where I melted into everything around  me,” said one of the volunteers in Carhart-  Harris’s study. These findings fit neatly with  research that shows that the parts of the brain  responsible for self-awareness are  underdeveloped in infants. According to  Carhart-Harris, psilocybin seems to rewind  parts of the brain to when they were less  organised and the ego was yet to emerge. 
“One of the reasons why the psychedelic  state is so interesting is that it offers a window  into what infantile consciousness is like,” he  says. “It’s the brain and mind moving back to  an earlier stage, essentially, where our style of  cognition is less constrained, less analytical,  and more influenced by imagination and  wishes, but also fears.” Psilocybin also makes  us emotionally volatile. Carhart-Harris is often  struck by the child-like behaviour of his  subjects. “One of the really notable things that  you see with psychedelics is that people start  to giggle,” he says. “People behave in a very  silly, immature way. It’s quite endearing. 
They seem quite vulnerable.” 
Carhart-Harris’s work on psychedelics has  prompted Gopnik to rethink what it’s like to  be a baby. Being strung out on coffee and  cigarettes may not be quite enough to explain  just how bizarre infant consciousness might  be. “It may be even weirder than that,” she  says. It might be like being on LSD, an even  more powerful psychedelic than psilocybin. 
Gopnik now alerts audiences to the dangers  of revisiting their past. “LSD is dangerous,  nicotine is very dangerous and nothing is  more dangerous than falling in love,” she says.  “So tea with toddlers is really the safest way to  expand your consciousness.” ■ 
The Human Brain I NewScientist: The Collection I 77 

If you believe fading brainpower is an inevitable part of growing  older, think again, say Michael Ramscar and Harald Baayen 
IT IS one of life’s eternal mysteries: why does  it get ever more difficult to recall the name  of the person you were just introduced to?  Surely it is a no-brainer that our cognitive  powers fade as we grow older? Research  seems to back this up: as we age, our scores  in tests of cognitive ability decline. 
Is this picture really correct? When we  applied the techniques we use to study  language learning to this evidence, we came  to a different conclusion. In fact, counter-  intuitively, many of these lower scores reflect  cognitive improvement. 
To illustrate the point, let’s look at a test  often used to measure our ability to learn and  recall new information, called paired associate  learning (PAL). In this test, people learn word  pairs. Some are easy, baby-cries; others harder,  obey-eagle. People perform worse on this task  as they get older, supporting the conclusion  that learning ability declines with age. 
We think PAL tests paint a misleading  picture of our cognitive abilities because  they do not take into account prior knowledge  of the words being tested, which grows with  age and experience. To explain why this  matters, we need to take a close look at the  learning process. 
The Russian physiologist Ivan Pavlov is  famous for conditioning dogs to salivate at the  sound of a bell. This led to a view of learning  called associationism: if a cue is present, and  an outcome follows, animals learn to associate  them. Although humans can learn this way,  the word ‘‘associate” is misleading. Our brains  actually learn by making and testing  predictions about the world. These are used  to determine cues that are unreliable, which  our brains then ignore and hence eliminate. 
It turns out that a dog associates a bell with  food only because it has learned to ignore all  other cues available to it. 
We can apply this understanding of the  role of elimination in the learning process to  the PAL test. Results not only show that we  find this test harder as we grow older but also  that harder word-pairs become more difficult  to learn. Why? An obvious answer is that  words such as baby and cries often appear  together in everyday language. This is what  makes these pairs easy to remember.  Meanwhile, learning nonsense pairs of words  such as obey-eagle is hard because experience  teaches us that obey is uninformative about  eagle in English. This suggests a reason why  older adults find PAL learning harder: they  have greater experience of how words do and  don’t occur together. 
In the past, this suggestion would have been  impossible to test. There was simply no way of  measuring how differences in experience  might play out in learning on something like a  PAL task. However, computational models  enable us to estimate the connections between  words based on their patterns of occurring  together in billions of words of English text  and speech. We used these techniques to  assess the way that PAL words should behave  in English. We found that as adults grow older,  whether they find PAL pairs easier or harder  reflects how difficult the information  structure of English says they ought to be. 
Traditional interpretation of PAL results  assumes that all participants have equal  knowledge of the words being tested. This is  clearly wrong. Once we correct for the effects  that increased experience can be expected to  have on subsequent learning, any evidence of 
F-* 
Older and wiser:  we get better at  ignoring what we don't  need to know 
cognitive decline disappears. What we find  instead is evidence that older people have a  superior knowledge of how the English  language works. In a similar vein, it is well  known that as we age we get slower at  discriminating real words from non-words in  tests. What is less well known is that age also  makes us more accurate at this task.  Interestingly, people who speak two languages  respond more slowly than monolingual  people on similar tests, yet this is not taken as  evidence that bilingualism leads to cognitive  deficits. Rather, bilingual people’s slower  responses are thought to reflect the time it  takes to search their larger “mental dictionaries”. 
78 1 NewScientist: The Collection | The Human Brain 
 The problem of understanding the effects of  prior learning on performance are unlikely to  be unique to PAL and word-recognition tasks.  Other psychometric tests of cognitive ability  (intelligence or short-term memory, for  example) also assume that the participant’s  prior knowledge of items being tested is  irrelevant. What our research shows is that  increased knowledge brings costs as well as  benefits. Learning increases the amount of  information that our brains have to process,  which inevitably affects test performance. 
Contrary to popular belief, neuronal loss  does not play a significant role in age-related  changes in brain structure. Rather, consistent 
with our findings, most of the changes that  occur as healthy brains age are difficult to  distinguish from those that occur as we learn.  Thus, understanding the costs and benefits of  learning is critical if we are to establish the  facts of cognitive ageing. For example,  memory experiments show that, as we age,  we “encode” less contextual information,  such as what we were wearing when we  learned a new fact. This makes the fact harder  to recall, and is seen as a sign of cognitive  decline. Yet everything we know about the way  our brains learn indicates that people must  inevitably become insensitive to many  background details as life experience grows. 
recognising American-English names harder  over time, independent of the fact that people  also learn more names as their experience  grows with age. In a computer simulation, we  found that simply processing the information  required to recognise a name ought to take  today’s 70-year-olds half-a-second longer than  when they were 20. 
The processes involved in forming  I memories and recalling names highlight  how the way we learn interacts with the  I environment throughout our lifetimes, and  I shows how difficult it is to separate changes  “ caused by learning from those of decline. 
This is important. We are not arguing that  the functionality of our brains stays the same  as we grow older, or that cognitive decline  never happens, even in healthy ageing. What  we do know is the changes in performance  seen on tests such as the PAL task are not  evidence of cognitive or physiological decline  in ageing brains. Instead, they are evidence  of continued learning and increased  knowledge. This point is critical when it comes  to older people’s beliefs about their cognitive  abilities. People who believe their abilities  can improve with work have been shown to  learn far better than those who believe  abilities are fixed. It is sobering to think of the  damage that the pervasive myth of cognitive  decline must be inflicting. ■ 
This is simply because detuning our attention  to irrelevant information is integral to the  process we call “learning”. 
This observation hints at a way to  overcome age-related problems with memory  recall. As we age, varying the contexts of our  lives more can help counteract the way our  minds have evolved to continually tune out  irrelevant information. This also means that,  when retirement leads older people to spend  most of their time in highly familiar  environments, they will find it difficult to  absorb the “context” that separates one  memory from another. As a result, memories  will become confused, even without declines  in underlying brainpower. 
Our research sheds similar light on another  problem associated with old age: the inability  to recall people’s names. It turns out that  names, at least in the US, have become more  complex at an almost exponential rate since  the 1880s. This has made the task of 
"Contrary to popular belief,  neuronal loss does not play  a significant roie as we age" 
TheHumanBrain I NewScientist: The Collection I 79 
u ..miracle  patients 
‘We don’t see a lot oF long term  survivors and these intrigue me.  It’s our job to study the miracle  patients and extend that  success to, not just a Few, but  the vast majority oF patients. 
I think that is possible  within 10 years.’ 
A/ProF Kerrie McDonald  Chair oF Cure Brain Cancer  Neuro-oncology Group, UNSW 

Find out more about us  curGbraincancGr.org.au/rGsearch 
Many minds, one purpose 
I 
[NK 
BRAINS,  BLUE  BRAINS,  PURPLE  PEOPLE 
Some differences  between men's and  women's brains may be  ^here to make us act the  saMe. Kayt Sukel reports  on a'startling idea 
S EVERAL years ago, the car I was driving  was rear-ended by another at a stop sign.  No one was hurt, but my passenger  and I had to wait around to give a statement  to the local police. Later on 1 asked my  companion if he had noticed that the  officer addressed most of the questions to  him, even though 1 was the one who had  been driving. “I think he was just afraid you  were going to do the typical female thing  and fall apart,” he replied. 
The notion that men can face adversity  with stoicism while women are more  likely to respond with histrionics is just  one example of the gender stereotypes  that permeate our culture. If my friend  was right, they even persist among those  who should be taking particular care to  treat people equally. 
Perhaps such prejudice is justified, though.  After all, in recent years evidence has turned  up of numerous differences between men’s  and women’s brains, whether at the level of  synapses, signalling chemicals, or gross  anatomy. Brains come in hues of either pink  or blue, as one researcher puts it. 
But could we be overlooking an important  caveat? A new theory that has sprung from  research on prairie voles says that at least  some of those disparities evolved not to create  differences in behaviour or ability, but to  prevent them. They are there to compensate  for the genetic or hormonal differences that  are necessary to create two sexes with different  sets of genitals and reproductive behaviours. 
If that sounds paradoxical, imagine  comparing a chunky mountain bike with a  lightweight road bike. To compensate for the 
mountain bike’s greater resistance, you have  to pedal harder to reach the same speed; one  difference makes you introduce another to  achieve the same output. In brain terms, while  certain circuits may be shaded pink or blue,  that would not stop the output, or behaviour,  being a uniform purple. 
Of course this '‘compensation theory” will  not explain away all brain differences between  the sexes, but it could account for some. The  idea is still in its infancy and so far has largely  been overlooked. If it is right, though, our  innate abilities may not be so different after all. 
“Compensation is a concept that most  people haven’t thought about and it’s  important,” says Larry Cahill, a neuroscientist  who researches human sex differences at the  University of California, Irvine. “This is  something we need to be paying attention to.” > 
The Human Brain I NewScientist: The Collection I 81 
For most of history, men’s and women’s  different roles in life were assumed to be  mainly innate and unalterable. This was  challenged in the west with the rise of  feminism in the second half of the last century.  Perhaps the different behaviours of boys and  girls arose because of cultural norms: parents  praising boys for romping and smashing toy  cars, for instance, while expecting girls to be  more reserved and play with their dolls. 
Around the same time, though, new light  was being shed on the biology of gender. In  the womb, we all start out more or less female,  until sometime between six and 12 weeks of  pregnancy. Then, in male fetuses, a gene on  the Y chromosome causes certain cells to  make testosterone, which leads to the  development of the penis and testicles. Female  fetuses do not have this '‘testosterone bath”  and so develop female reproductive organs. 
But the sex hormones’ influence is not  limited to our gonads: they also play a key role  in the brain’s development, influencing the  architecture of various neural circuits. As well  as establishing these anatomical differences,  the sex hormones presumably affect our  behaviour as adults too, as their receptors have  been found in many brain regions. 
Understanding the ways in which male  and female brains differ has become a hot  topic in neuroscience, particularly in the  past decade with the growth of brain  scanning as a research tool. For instance,  one of the most famous findings is that  men seem to have a larger region of the brain  thought to be involved in spatial reasoning,  such as that used in a task like mentally  rotating three-dimensional figures: the  left-hand-side inferior parietal lobule,  located just over the ear. Women, on the  other hand, appear to have larger areas of  the brain associated with language. 
A common critique of this sort of work is  that there is only a small average difference  between the sexes, with more variability  within each sex than between men and women  as a whole. The results tell us about population  averages, not individuals, in other words. 
Even so, any such findings tend to be seized on  by the media. UK newspapers are fascinated  by neuroscience, according to an analysis of  their coverage of this topic over the 10 year  period between 2000 and 2010. In a detailed  breakdown of the stories by subject area,  sex differences came eighth out of 
"The women had the same  feelings of stress as the  men but their brains were  acting differently" 
41 neuroscience categories. As with any  science stories in the media, findings tend to  be exaggerated. “They want to take these  results and try to spread males and females  way apart on function and ability,” says Cahill. 
It is certainly true that while society has  become more equal in many respects, men  still outnumber women in mathematics,  engineering and many areas of science. While  young girls do as well in these subjects as their  male classmates, they start lagging behind as  they grow up and enter further education. 
For instance, women make up about 20 per  cent of computer science students in the US,  and the same fraction of engineering students.  Is it down to innate brain differences or  cultural conditioning that they miss out on  these well-paying sectors so crucial to today’s  technology-oriented society? 
Research into brain sex differences has  also fuelled calls to educate boys and girls 
SPACED OUT 
Men tend to do better than women on tests involving spatial reasoning. 
Is the shape on the right a rotated version of the one on the left? 
 separately in same-sex classes or schools,  particularly in the US. It is argued that  teaching methods need to be tailored to  those differently hued brains. 
With this sort of research having such  significant implications, it is important to  be aware of possible flaws. The compensation  theory first caught people’s attention in  2004, with the publication of a review entitled  “Sex differences in adult and developing  brains: compensation, compensation,  compensation”. 
The author was Geert de Vries, who studies  hormones and brain signalling systems in  rodents at the University of Massachusetts in  Amherst. In the 1980s he stumbled across a  big sex difference in the brains of prairie voles,  small rodents found in the US Midwest. 
Unlike most mammals, prairie voles are  monogamous and the males are devoted  fathers. They spend just as much time as the  females licking their pups and toting them  around. Yet compared with the females,  males have many more receptors in the brain  for vasopressin, a brain signalling molecule  that has been linked to parental care. 
De Vries recalls: “When we linked this  sexually [different] system to a behaviour that  is spectacularly similar in males and females, 
I thought, ‘Wait a moment, why are the sex  differences opposite from the things they are  doing? Could the differences be there so they  can act the same?”’ 
The more de Vries considered the idea,  the more it made sense to him. The female  voles’ maternal devotion was demonstrably  triggered by the hormonal changes of  pregnancy. The males’ vasopressin circuits  seemed to be compensating for the lack  of pregnancy hormones. And if that kind  of compensation was going on in prairie  voles, could something similar also be  happening elsewhere? 
De Vries realised that the most likely  candidates for compensatory circuits were  those that are influenced by sex hormones  or the sex chromosomes. Poring through the  research literature, he found several possible  compensatory mechanisms in other animals,  including rats, mice and zebra finches. 
While de Vries had outlined the  compensation theory before, his 2004 review  succeeded in bringing the idea to wider notice.  One convert is Margaret McCarthy, a sex  differences researcher at the University of  Maryland School of Medicine in Baltimore.  “Many of the sex differences we see in the  brain are there to help males and females  develop their different reproductive 
8Z I NewScientist: The Collection | The Human Brain 
J0Y5ALE/FLICKR/GETTY 
 strategies,” she says. '‘But those differences  also carry with them some constraints. Males  have high testosterone; females have cycles  of various hormones. And those hormones  come with costs with regards to behaviours  outside reproduction.” 
To date, the evidence for compensation  in people seems thin on the ground. But  could it be going unnoticed because of the  assumption that a difference in the brain  always means a difference in performance? 
In a 2006 review of sex difference research,  Cahill cited several brain-scanning studies  that had turned up differences in men and  women that were not accompanied by  differences in their performance. 
While the mechanisms involved are  unknown, Cahill thinks these could represent  compensation in action, although they had  not been noted as such by those who did the  research. 
Equal but different 
For instance, in one study men and women  were asked to name everyday objects in photos  that were flashed up at a challenging pace.  According to the PET scanner, men showed  more activation in certain brain regions  thought to be responsible for visual  recognition, although they scored about the  same as the women. The authors speculated  that the men might have needed to work  harder to get the same result because of  women’s superior language abilities. 
Cahill himself may have found evidence  of compensatory circuits at work, involving  the amygdalae, a pair of almond-shaped  structures deep within the brain thought to  be involved in the processing and memory of  emotional reactions. CahilTs group showed 
that even when the brain is at rest, amygdala  activity is different in men and women. 
That made neuroscientists sit up and  take notice, because most imaging studies  require resting activity levels to be subtracted  from levels seen during experimental tasks  in order to reveal changes caused by the task.  Given these findings, important results may  be going unseen because at the moment  men and women’s results tend not to be  analysed separately. 
Cahill thinks the difference in amygdala  activity could be a compensatory mechanism  to make up for differences in testosterone  levels. “There are instances where everyone  agrees that there is no sex difference on the  behavioural level. But that doesn’t mean there  isn’t a sex difference in the brain,” he says. 
“It remains possible that the equal behaviour  was achieved in different ways. 
1 can’t help but think of compensation when  1 remember the car accident. I don’t think my  behaviour was any different from that of my  male friend. We were both a bit rattled, of  course, but more impatient to finish the  paperwork and be on our way. But were  our brains behaving any differently? Recent  work from jill Goldstein’s lab at Harvard  Medical School in Boston suggests they  may have been. While she did not go looking  for a compensation effect, she believes de  Vries’s theory could explain her results. 
Goldstein’s team did fMRI scans on  12 women and 12 men as they viewed a variety  of photos, some of which were designed to  be shocking (think car accidents and  dismembered bodies). The women did the test  twice: once at the beginning of their menstrual  cycle, when oestrogen levels would have been  low and then again just before ovulation,  when they would have been peaking. 
When viewing the gruesome photos the  women reported similar subjective feelings  of stress as the men, irrespective of the stage  in their menstrual cycle. But when their  oestrogen was high, the women had less  activity than men in several different brain  regions involved in the stress response.  Goldstein thinks this was to damp down a  more sensitive stress response that otherwise  would have been triggered by the surging  oestrogen. “They had the same subjective  feelings of stress but their brains were acting  slightly differently to get to that state,”  she says. 
While the compensation theory has not yet  gained much traction among neuroscientists,  it is getting harder to ignore as the number of  possible human examples accumulates. Even  where compensatory brain differences have  no net effect on behaviour or ability, they  could still help explain why certain medical  conditions are more common in one sex than  the other. Women, for instance, are more  vulnerable to mental illnesses like anxiety and  depression, while men have a higher incidence  of developmental disorders like autism. 
Goldstein’s work on stress is a case in point.  “We need to understand how these circuits  develop differently in the healthy male and  female brain,” she says. “Only then can we  understand how these circuits are disrupted  in psychiatric disorders.” 
Funders are also starting to take the issue  seriously. In 2014, the US National Institutes of  Health issued new policies requiring that sex  differences be addressed in future biomedical  research programmes funded by the agency. 
No one is saying the compensation theory  can explain away all the observable brain  differences between men and women. Many  of them do in fact correspond to differences  in performance. But some do not. 
That suggests we should be more careful  about how we interpret brain data from now  on, according to Lise Eliot, a neuroscientist at  Rosalind Franklin University in Chicago,  who coined a phrase with the title of her 2010  book on sex differences. Pink Brain, Blue Brain. 
“The more we learn, the more we realise  that sex differences don’t translate very well  into that Mars-Venus pop culture everyone  seems to want to project,” she says.  “Neuroscientists, the media, parents- we  all need to be careful about how this data is  interpreted and what conclusions we draw  from it.” ■ 
saA luopsanb luejBeip 0; ja/v\su\/ 
The Human Brain I NewScientist: The Collection I 83 
CHAPTER SIX 
CONSCIOUSNESS 
 We are closer than we think to solving one of science's hardest  puzzles, says Christof Koch - understanding how feelings of  love and ennui, the taste of an apple or sight of alpenglow on a  distant peak relate to the physical brain. What is the secret? 
A QUICK glance at the thousands of books that  purport to explain consciousness makes the  real understanding of it look like a Herculean  task. There is, after all, a profound explanatory  gap between neural activity of any sort and  subjective feelings. The first belongs to the  realm of physics, to space and time, energy  and mass, the second to experience. And while  experiences are ephemeral, they are the very  stuff of life. The only way we know about the  world, about space and time, about energy  and mass, about anything in fact is by seeing,  hearing and smelling, by lusting and hating,  by remembering and imagining. 
That these two realms are closely  related is revealed by the effects of a stroke, a  strong blow to the head, or by a neurosurgeon  stimulating electrically some part of a person’s  brain and evoking a childhood memory. 
Yet consciousness does not appear in the  equations of physics, nor in chemistry’s  periodic table, nor in the A-T-G-C molecular  chatter of our genes. Somehow it emerges  from the nervous system. 
I have spent 25 years - the first 16 years  working with my mentor, colleague and  friend Francis Crick - linking specific aspects  of consciousness to the mammalian brain. 
We popularised the idea of the neuronal  correlates of consciousness (NCC): the minimal  neuronal mechanisms - the synapses,  neurons and brain regions - that are jointly  sufficient for any one conscious percept. 
Since then, much progress has been  made. We now know that some sectors of 
the cerebral cortex making up the bulk of  the brain (for its size the most complex  organ in the universe) have a privileged  relationship to consciousness, that not all  of its many regions participate equally in  generating the content of a conscious  experience. Micro-electrodes and magnetic  scanners have also shown us that the  neocortex can be active without necessarily  giving rise to a conscious experience. This is  the domain of the non-conscious. 
Yet Crick and I looked deeper. Why did  a particular NCC give rise to one specific  conscious experience? Why should particular  vibrations of highly organised matter trigger  conscious feelings? It seems as magical as  rubbing a lamp and having a genie emerge. 
What is needed is a fundamental account  of how activity in any system can give rise to  consciousness. We therefore turned to the  ideas of Giulio Tononi at the University of  Wisconsin-Madison. He advocates a  sophisticated information theory account of  consciousness, called integrated information.  The theory introduces a precise measure,  called phi, which captures the extent of  consciousness. Expressed in bits, phi  quantifies the extent to which any system  of interacting parts is both differentiated  and integrated when that system enters a  particular state. 
This is the heart of phenomenal  experience: anyone conscious experience  is both highly differentiated from any other  one but also unitary, holistic. The larger the 
 We know the world by seeing and hearing,  lusting and hating, remembering and imagining 
phi, the richer the conscious experience of  that system. Furthermore, the theory assigns  any state of any network of causally  interacting parts (these neurons are firing,  those ones are quiet) to a shape in a high-  dimensional space. The shape (think of it as  a crystal in a fantastically high-dimensional  space) accounts for the peculiar feel of any  one conscious experience. If the network  switches into a different state - you fantasise  about sex rather than listen to a droning  speaker- the crystalline shape changes as well. 
This crystal is the system viewed from  within. It is the voice in the head, the light 
84 1 NewScientist: The Collection | The Human Brain 

inside the skull. It is everything you will ever  know of the world. It is your only reality. It is  the quiddity of experience. The dream of the  lotus-eater, the mindfulness of the meditating  monk, the agony of the cancer patient, all feel  as they do because of the shape of the distinct  crystals in a space of a trillion dimensions. 
Integrated information makes specific  predictions about which brain circuits are  involved in consciousness and which ones are  peripheral players, even though they might  contain many more neurons. The theory  should let doctors build a consciousness  meter to measure the extent to which severely  brain-injured patients are in a vegetative state,  and which ones are partially conscious but  unable to signal their pain and discomfort. 
I am now pursuing a different tack at the  Allen Institute for Brain Science in Seattle,  a few hours north of Pasadena by plane. In  2012, we embarked on an ambitious lo-year  project involving hundreds of scientists and  technologists. Philanthropist Paul G. Allen,  who founded the institute in 2003, has  pledged $300 million for the first four years  of the project. Our goal is to understand how  information is encoded, transformed and  represented in the mouse and the human  cerebral neocortex and its satellites. 
The neocortex is a layered structure: the  human neocortex is about twice as thick  that of the mouse, and has about 1000 times  the surface area. It is a highly versatile,  computational tissue that excels at processing 
sensory information, making and storing  associations, and planning and producing  complex motor patterns. The neocortex is  partitioned into multiple areas, made up of  smaller columns with reasonably similar cell  types and architectures across species and  brain regions. 
The institute plans to build a series of  brain “observatories” to identify, record and  intervene in the cortical networks underlying  visually guided behaviours in the mouse,  including visual perception, decision-making,  and even murine consciousness. The fast-  developing technology of optogenetics will  allow us to control defined events in defined  neurons at defined times in mouse brains. That  is, we will move from correlation to causation.  Building these observatories is a large-scale 
"Why should vibrations of  organised matter trigger  conscious feelings?" 
effort to synthesise anatomical, physiological  and theoretical knowledge into a model of the  cerebral cortex, which we think has the  potential to revolutionise our understanding  of the mammalian brain. The fruits of this  cerebroscope will be freely available. 
Throughout my quest to understand  consciousness, I never lost my sense  ^ of living in a magical universe. I do believe  I some deep and elemental organising principle  s created the universe and set it in motion for a  I purpose I cannot comprehend. I grew up  8 calling this god - but a god much closer to  § Baruch de Spinoza’s god than the god of  i Michelangelo’s paintings. 
§ A pioneering generation of stars had to  5 die in spectacular supernovae to seed space  with the heavier elements needed for the  rise of self-repIicating bags of chemicals, on  a rocky planet orbiting a young star at just  the right distance. The competitive pressures  of natural selection made possible the  accession of creatures with nervous systems.  As the complexity of these systems grew to  staggering proportions, some of the creatures  evolved the ability to reflect on themselves, to  contemplate their beautiful but cruel world. 
While the rise of sentient life was inevitable,  it does not mean Earth had to bear life or that  bipedal, big-brained primates had to walk the  African grasslands. But I do believe the laws  of physics overwhelmingly favoured the  emergence of consciousness, and that those  laws will lead us to a more or less complete  knowledge of it. ■ 
The Human Brain I NewScientist: The Collection I 85 
 JONATHAN BURTON 
 86 1 NewScientist: The Collection | The Human Brain 

 Could a special type  of brain cell give us and  other smart animals  our emotions, empathy  and sense of self?  Caroline Williams  investigates 
T he origin of consciousness has to  be one of the biggest mysteries of  all time, occupying philosophers and  scientists for generations. So it is strange  to think that a little-known neuroscientist  called Constantin von Econo mo might  have unearthed an important clue nearly  90 years ago. 
When he peered down the lens of his  microscope in 1926, von Economo saw a  handful of brain cells that were long, spindly  and much larger than those around them. 
In fact, they looked so out of place that at first  he thought they were a sign of some kind of  disease. But the more brains he looked at, the  more of these peculiar cells he found - and  always in the same two small areas that  evolved to process smells and flavours. 
Von Economo briefly pondered what these  “rod and corkscrew cells”, as he called them,  might be doing, but without the technology  to delve much deeper he soon moved on to  more promising lines of enquiry. 
Little more was said about these neurons  until nearly 80 years later when Esther  Nimchinsky and Patrick Hof, then both at  Mount Sinai University in New York, also  stumbled across clusters of these strange- 

looking neurons. Now, after more than a  decade of functional imaging and post-  mortem studies, we are beginning to piece  together their story. Certain lines of evidence  hint that they may help build the rich inner  life we call consciousness, including emotions,  our sense of self, empathy and our ability to  navigate social relationships. 
Many other big-brained, social animals also  seem to share these cells, in the same spots as  the human brain. A greater understanding of  the way these paths converged could therefore  tell us much about the evolution of the mind. 
Admittedly, to the untrained eye these  giant brain cells, now known as von Economo  neurons (VENs), don’t look particularly  exciting. But to a neuroscientist they stand  out like a sore thumb. For one thing, VENs  are at least 50 per cent, and sometimes up to  200 per cent, larger than typical human  neurons. And while most neurons have a  pyramid-shaped body with a finely branched  tree of connections called dendrites at each  end of the cell, VENs have a longer, spindly  cell body with a single projection at each end  with very few branches (see diagram, page 89).  Perhaps they escaped attention for so long  because they are so rare, making up just 1 per  cent of the neurons in the two small areas of  the human brain: the anterior cingulate  cortex (ACC) and the fronto-insular (El) cortex. 
Their location in those regions suggests  that VENs may be a central part of our  mental machinery, since the ACC and FI  are heavily involved in many of the more  advanced aspects of our inner lives. Both  areas kick into action when we see socially  relevant cues, be it a frowning face, a grimace  of pain or simply the voice of someone we  love. When a mother hears a baby crying,  both regions respond strongly. They also  light up when we experience emotions such  as love, lust, anger and grief. For john Allman,  a neuroanatomist at the California Institute  of Technology in Pasadena, this adds up to  a kind of “social monitoring network” that  keeps track of social cues and allows us to  alter our behaviour accordingly. 
The two brain areas also seem to play a  key role in the “salience” network, which  keeps a subconscious tally of what is going  on around us and directs our attention to  the most pressing events, as well as  monitoring sensations from the body to  detect any changes. 
What’s more, both regions are active when  a person recognises their reflection in the  mirror, suggesting that these parts of the > 
The Human Brain I NewScientist: The Collection I 87 
"Von Economo neurons  might be a vital adaptation  in large brains for keeping  track of social situations" 
brain underlie our sense of self - a key  component of consciousness. ‘‘It is the  sense of self at every possible level - so the  sense of identity, this is me, and the sense of  identity of others and how you understand  others. That goes to the concept of empathy  and theory of mind,” says Hof. 
To Bud Craig, a neuroanatomist at Barrow  Neurological Institute in Phoenix, Arizona,  it all amounts to a continually updated sense  of “how 1 feel now” : the ACC and FI take inputs  from the body and tie them together with  social cues, thoughts and emotions to quickly  and efficiently alter our behaviour. 
This constantly shifting picture of how  we feel may contribute to the way we  perceive the passage of time. When  something emotionally important is  happening, Craig proposes, there is more  to process, and because of this time seems  to speed up. Conversely, when less is going  on we update our view of the world less  frequently, so time seems to pass more slowly. 
VENs are probably important in all this,  though we can only infer their role through  circumstantial evidence. That’s because  locating these cells, and then measuring  their activity in a living brain hasn’t yet been  possible. But their unusual appearance is a  signal that they probably aren’t just sitting 
Nice ice: sharing food was, perhaps, the  setting in which empathy evolved 
there doing nothing. “They stand out  anatomically,” says Allman, “And a general  proposition is that anything that’s so  distinctive looking must have a distinct  function.” 
Fast thinking 
In the brain, big usually means fast, so Allman  suggests that VENs could be acting as a fast  relay system - a kind of social superhighway -  which allows the gist of the situation to move  quickly through the brain, enabling us to  react intuitively on the hop, a crucial survival  skill in a social species like ours. “That’s what  all of civilisation is based on: our ability to  communicate socially, efficiently,” says Craig. 
A particularly distressing form of dementia  that can strike people as early as their 30s  supports this idea. People who develop  fronto-temporal dementia lose large numbers  of VENs in the ACC and FI early in the disease,  when the main symptom is a complete loss  of social awareness, empathy and self-control.  “They don’t have normal empathic responses  to situations that would normally make you  disgusted or sad,” says Hof. “You can show  them horrible pictures of an accident and  they just don’t blink. They will say ‘oh, yes,  it’s an accident’.” 
Post-mortem examinations of the brains  of people with autism also bolster the idea  that VENs lie at the heart of our emotions and 
empathy. According to one recent study,  people with autism may fall into two  groups: some have too few VENs, perhaps  meaning that they don’t have the necessary  wiring to process social cues, while others  have far too many. The latter group would  seem to fit with one recent theory of autism,  which proposes that the symptoms may  arise from an over-wiring of the brain. 
Perhaps having too many VENs makes  emotional systems fire too intensely, causing  people with autism to feel overwhelmed, as  many say they do. 
Another recent study found that people  with schizophrenia who committed suicide  had significantly more VENs in their ACC than  schizophrenics who died of other causes. The  researchers suggest that the overabundance  of VENs might create an overactive emotional  system that leaves them prone to negative  self-assessment and feelings of guilt and  hopelessness. 
VENs in other animals provide some clues,  too. When these neurons were first identified,  there was the glimmer of hope that we might  have found one of the key evolutionary  changes, unique to humankind, that could  explain our social intelligence. But the earliest  studies put paid to that kind of thinking, when  VENs turned up in chimpanzees and gorillas.  In recent years, they have also been found  in elephants and some whales and dolphins. 
Like us, many of these species live in big  social groups and show signs of the same kind  of advanced behaviour associated with VENs  in people. Elephants, for instance, display  something that looks a lot like empathy:  they work together to help injured, lost or  trapped elephants, for example. They even  seem to show signs of grief at elephant  “graveyards”. 
What’s more, many of these species can  recognise themselves in the mirror, which is  usually taken as a rudimentary measure of  consciousness. When researchers daub paint  on an elephant’s face, for instance, it will  notice the mark in the mirror and try to feel  the spot with its trunk. This has led Allman  and others to speculate that von Economo  neurons might be a vital adaptation in large  brains for keeping track of social situations -  and that the sense of self may be a  consequence of this ability. 
Yet VENs also crop up in other animals  including manatees, hippos and giraffes - not  renowned for their busy social lives. The cells  have also been spotted in macaques, which  don’t reliably pass the mirror test, although 
 88 1 NewScientist: The Collection | The Human Brain 
Judgement cells 
Von Economo neurons may play an important role in our sense of self 
they are social animals. Although this  seems to put a major spanner in the works  for those who claim that the cells are crucial  for advanced cognition, it could also be that  these creatures are showing the precursors  of the finely tuned cells found in highly  social species. ‘‘1 think that there are  homologues of VENs in all mammals,”  says Allman. “That’s not to say they’re  shaped the same way but they are located  in an analogous bit of cortex and they are  expressing the same genes.” 
It would make sense, after all, that whales  and primates might both have recycled,  and refined, older machinery present in a  common ancestor rather than independently  evolving the same mechanism. Much more  research is needed, however, to work out the  anatomical differences and the functions of  these cells in the different animals. 
That work might even help us  understand how these neurons evolved  in the first place. Allman already has some  ideas about where they came from. Our  VENs reside in a region of the brain that  evolved for olfaction, which integrates taste  and smell, so he suggests that many of the  traits now associated with the El and the  ACC evolved from the simple act of deciding  whether food is good to eat or likely to make  you ill. When reaching that decision, he says,  the quicker the “gut” reaction kicks in the  better. And if you can detect this process  in others, so much the better. 
“One of the important functions that  seems to reside in the El has to do with  empathy,” he says. “My take on this is  that empathy arose in the context of shared  food - it’s very important to observe if  members of your social group are becoming  ill as a result of eating something.” The basic  feeding circuity, including the rudimentary  VENs, may then have been co-opted by some  species to work in other situations that involve  a decision, like working out if a person is  trustworthy or to be avoided. “So when we  have a feeling, whether it be about a foodstuff  or situation or another person, 1 think that  engages the circuitry in the fronto-insular  cortex and the VENs are one of the outputs of  that circuitry,” says Allman. 
"Far from being the  pinnacle of brain evolution,  consciousness might have  been a big, happy accident" 
VON ECONOMO  NEURONS 
Allows the high-speed  connections necessary  for rapid emotional and  intuitivejudgements 
These cells are found  in just two small areas  of the brain 
( 
 OTHER TYPES OF NEURONS 
MOTOR 
Send signals to parts  ofthe body, eg muscle,  to direct movement 
 ! PYRAMIDAL  I Involved in many areas  j of cognition -such as  j object recognition  ! within the visual cortex 

ANTERIOR CINGULATE  CORTEX 
FRONTO-INSULAR 
CORTEX 
 SENSORY 
Transmit signals from  the rest of the body to  the brain 
 I INTER 
= Bridge connections  I between other neurons 
 Allman’s genetics work suggests he may be  on to something. His team found that VENs in  one part ofthe El are expressing the genes for  hormones that regulate appetite. There are  also a lot of studies showing links between  smell and taste and the feelings of strong  emotions. Our physical reaction to something  we find morally disgusting, for example,  is more or less identical to our reaction to  a bitter taste, suggesting they may share  common brain wiring. 
Other work has shown that judging a  morally questionable act, such as theft, while  smelling something disgusting leads to  harsher moral judgements. What’s more,  Allman points out that our language is loaded  with analogies - we might find an experience  “delicious”, say, or a person “nauseating”. This  is no accident, he says. 
Red herring 
However, it is only in highly social animals  that VENs live exclusively in the scent and  taste regions. In the others, like giraffes and  hippos, VENs seem to be sprinkled all over  the brain. Allman, however, points out that  these findings may be a red herring, because  without understanding the genes they  express, or their function, we can’t even be  sure how closely these cells relate to human  VENs. They may even be a different kind of  cell that just looks similar. 
Based on the evidence so far, however, Hof  thinks that the ancestral VENs would have 
been more widespread, as seen in the hippo  brain, and that over the course of evolution  they then migrated to the ACC and FI in some  animals, but not others - though he admits to  having no idea why that might be. He suspects  the pressures that shaped the primate brain  may have been very different to those that  drove the evolution of whales and dolphins. 
Craig has hit on one possibility that would  seem to fit all of these big-brained animals. He  points out that the bigger the brain, the more  energy it takes to run, so it is crucial that it  operates as efficiently as possible. A system  that continually monitors the environment  and the people or animals in it would  therefore be an asset, allowing you to adapt  quickly to a situation to save as much energy  as possible. “Evolution produced an energy  calculation system that incorporated not just  the sensory inputs from the body but the  sensory inputs from the brain,” Craig says. 
And the fact that we are constantly updating  this picture of “how I feel now” has an  interesting and very useful by-product: we  have a concept that there is an “I” to do the  feeling. “Evolution produced a very efficient  moment-by-moment calculation of energy  utilisation and that had an epiphenomenon,  a by-product that provided a subjective  representation of my feelings.” 
If he’s right - and there is a long way to go  before we can be sure - it raises a very humbling  possibility: that far from being the pinnacle  of brain evolution, consciousness might have  been a big, and very successful accident. — 
The Human Brain I NewScientist: The Collection I 89 
 90 1 NewScientist: The Collection I The Human Brain 
To understand consciousness, we need  to work out how anaesthesia makes it fade  away, Linda Geddes journeys.., 
Into the void 
I WALK into the operating theatre feeling  vulnerable in a draughty gown and  surgical stockings. Two anaesthetists  in green scrubs tell me to stash my  belongings under the trolley and lie down.  ‘‘Can we get you something to drink from the  bar?” they joke, as one deftly slides a needle  into my left hand. 
1 smile weakly and ask for a gin and tonic.  None appears, of course, but I begin to feel  light-headed, as if 1 really had just knocked  back a stiff drink. 1 glance at the clock, which  reads lo.io am, and notice my hand is feeling  cold. Then, nothing. 
1 have had two operations under general  anaesthetic this year. On both occasions I  awoke with no memory of what had passed  between the feeling of mild wooziness and  waking up in a different room. Both times  1 was told that the anaesthetic would make  me feel drowsy, I would go to sleep, and when  I woke up it would all be over. 
What they didn’t tell me was how the drugs  would send me into the realms of oblivion.  They couldn’t. The truth is, no one knows. 
The development of general anaesthesia  has transformed surgery from a horrific  ordeal into a gentle slumber. It is one of the  commonest medical procedures in the world,  yet we still don’t know how the drugs work.  Perhaps this isn’t surprising: we still don’t  understand consciousness, so how can we  comprehend its disappearance? 
That is starting to change, however, with  the development of new techniques for  imaging the brain or recording its electrical  activity during anaesthesia. “In the past  five years there has been an explosion of  studies, both in terms of consciousness,  but also how anaesthetics might interrupt  consciousness and what they teach us about  it,” says George Mashour, an anaesthetist at  the University of Michigan in Ann Arbor.  “We’re at the dawn of a golden era.” 
Consciousness has long been one of the  great mysteries of life, the universe and  everything. It is something experienced by  every one of us, yet we cannot even agree on  how to define it. How does the small sac of  jelly that is our brain take raw data about the  world and transform it into the wondrous  sensation of being alive? Even our increasingly  sophisticated technology for peering  inside the brain has, disappointingly, failed  to reveal a structure that could be the seat  of consciousness. 
Altered consciousness doesn’t only happen  under a general anaesthetic of course - it  occurs whenever we drop off to sleep, or if  we are unlucky enough to be whacked on the  head. But anaesthetics do allow neuroscientists  to manipulate our consciousness safely,  reversibly and with exquisite precision. 
It was a Japanese surgeon who performed  the first known surgery under anaesthetic, in  1804, using a mixture of potent herbs. In the  West, the first operation under general  anaesthetic took place at Massachusetts  General Hospital in 1846. A flask of sulphuric  ether was held close to the patient’s face until  he fell unconscious. 
Since then a slew of chemicals have been  co-opted to serve as anaesthetics, some  inhaled, like ether, and some injected. The  people who gained expertise in administering  these agents developed their own medical  speciality. Although long overshadowed by the  surgeons who patch you up, the humble “gas  man” does just as important a job, holding you  in the twilight between life and death. 
Consciousness may often be thought of as  an all-or-nothing quality - either you’re awake  or you’re not - but as I experienced, there are  different levels of anaesthesia (see diagram,  page 92). “The process of going into and out of  general anaesthesia isn’t like flipping a light  switch,” says Mashour. “It’s more akin to a  dimmer switch.” ^ 
The Human Brain I NewScientist: The Collection I 91 
You are feeling sleepy 
Losing consciousness under anaesthesia is not so much flipping a light switch  as turning down a dimmer switch 
STAGES OF ANAESTHESIA  LIGHT-HEADEDNESS 
BROADLY EQUIVALENT TO  BEING DRUNK 
 A typical subject first experiences a state  similar to drunkenness, which they may or  may not be able to recall later, before falling  unconscious, which is usually defined as  failing to move in response to commands. 
As they progress deeper into the twilight  zone, they now fail to respond to even the  penetration of a scalpel - which is the point  of the exercise, after all - and at the deepest  levels may need artificial help with breathing. 
These days anaesthesia is usually started off  with injection of a drug called propofol, which  gives a rapid and smooth transition to  unconsciousness, as happened with me. 
(This is also what Michael jackson was  allegedly using as a sleeping aid, with such  unfortunate consequences.) Unless the  operation is only meant to take a few minutes,  an inhaled anaesthetic, such as isoflurane, is  then usually added to give better minute-by-  minute control of the depth of anaesthesia. 
Lock and key 
So what do we know about how anaesthetics  work? Since they were first discovered, one of  the big mysteries has been how the members  of such a diverse group of chemicals can all  result in the loss of consciousness. Other  drugs work by binding to receptor molecules  in the body, usually proteins, in a way that  relies on the drug and receptor fitting snugly  together like a key in a lock. Yet the long list of  anaesthetic agents ranges from large complex  molecules such as barbiturates or steroids, to  the inert gas xenon, which exists as mere  atoms. How could they all fit the same lock? 
For a long time, there was great interest in  the fact that the potency of anaesthetics  correlates strikingly with how well they  dissolve in olive oil. The popular “lipid  theory” said that instead of binding to  specific protein receptors, the anaesthetic  physically disrupted the fatty membranes of  nerve cells, causing them to malfunction. 
In the 1980s, though, experiments in test  tubes showed that anaesthetics could bind to  proteins in the absence of cell membranes.  Since then, protein receptors have been found  for many anaesthetics. Propofol, for instance,  binds to receptors on nerve cells that normally  respond to a chemical messenger called GABA.  Presumably the solubility of anaesthetics in  oil affects how easily they reach the receptors  bound in the fatty membrane. 
But that solves only a small part of the  mystery. We still don’t know how this binding  affects nerve cells, and which neural networks  they feed into. “If you look at the brain under 
both xenon and propofol anaesthesia, there  are striking similarities,” says Nick Franks of  Imperial College London, who overturned the  lipid theory in the 1980s. “They must be  triggering some common neuronal change  and that’s the big mystery.” 
Many anaesthetics are thought to work by  making it harder for neurons to fire, but this  can have different effects on brain function,  depending on which neurons are being  blocked. So brain-imaging techniques such  as functional MRI scanning, which tracks  changes in blood flow to different areas of the  brain, are being used to see which regions of  the brain are affected by anaesthetics. Such  studies have revealed several areas that are  deactivated by most anaesthetics.  Unfortunately, so many regions have been  implicated it is hard to know which, if any, are  the root cause of loss of consciousness. 
But is it even realistic to expect to find a  discrete site or sites acting as the mind’s “light  switch”? One intriguing study conducted on a  woman who had electrodes implanted in her  brain because she had epilepsy found that  stimulating just one area - the claustrum -  caused the woman to lose consciousness. She 
regained consciousness as soon as the  electrical stimulation stopped. But even if  this experiment is repeated in others, such a  consciousness-switch is still likely to be one  piece in a larger network of brain activity. 
A leading theory of consciousness that has  gained ground in the past decade states that  consciousness is more widely distributed. In  this “global workspace” theory, incoming  sensory information is first processed locally  in separate brain regions without us being  aware of it. We only become conscious of the  experience if these signals are broadcast to a  network of neurons spread through the brain,  which then start firing in synchrony. 
The idea has recently gained support from  recordings of the brain’s electrical activity  using electroencephalograph (EEG) sensors on  the scalp, as people are given anaesthesia. This  has shown that as consciousness fades there is  a loss of synchrony between different areas of  the cortex - the outermost layer of the brain  important in attention, awareness, thought  and memory. 
This process has also been visualised using  fMRI scans. Steven Laureys, who leads the  Coma Science Group at the University of Liege 
9Z I NewScientist: The Collection I The Human Brain 

in Wallonia, Belgium, looked at what happens  during propofol anaesthesia when patients  descend from wakefulness, through mild  sedation, to the point at which they fail to  respond to commands. He found that while  small ‘‘islands” of the cortex lit up in response  to external stimuli when people were  unconscious, there was no spread of activity  to other areas, as there was during  wakefulness or mild sedation. 
A team led by Andreas Engel at the  University Medical Center in Hamburg,  Germany, have been investigating this process  in still more detail by watching the transition  to unconsciousness in slow motion. Normally  it takes about lo seconds to fall asleep after a  propofol injection. Engel has slowed it down  to many minutes by starting with just a small  dose, then increasing it in seven stages. At  each stage he gives a mild electric shock to  the volunteer’s wrist and takes EEG readings. 
We know that upon entering the brain,  sensory stimuli first activate a region called  the primary sensory cortex, which runs like  a headband from ear to ear. Then further  networks are activated, including frontal  regions involved in controlling behaviour,  and temporal regions towards the base of the  brain that are important for memory storage. 
Engel found that at the deepest levels of  anaesthesia, the primary sensory cortex was  the only region to respond to the electric  shock. “Long-distance communication seems  to be blocked, so the brain cannot build the  global workspace,” says Engel. “It’s like the  message is reaching the mailbox, but no one  is picking it up.” 
Other recent research also suggests that 
sensory signals reach the cortex but fail to be  sent out to the rest of the brain more widely. 
What could be causing the blockage? Engel  has EEG data suggesting that propofol  interferes with communication between the  primary sensory cortex and other brain  regions by causing abnormally strong  synchrony between them. “It’s not just  shutting things down. The communication  has changed,” he says. “If too many neurons 
Laureys, for example, has seen a similar  breakdown in communication between  different cortical areas in people in a coma.  “Anaesthesia is a pharmacologically induced  coma,” he says. “That same breakdown in  global neuronal workspace is occurring.”  Many believe that studying anaesthesia  will shed light on disorders of consciousness  such as coma. “Anaesthesia studies are  probably the best tools we have for 
'Is it realistic to expect to find a discrete site in  the brain acting as the mind's light switch? Not  according to the leading theory of consciousness " 
fire in a strongly synchronised rhythm, there  is no room for exchange of specific messages.” 
The communication between the different  regions of the cortex is not just one way;  there is both forward and backward signalling  between the different areas. EEG studies on  anaesthetised animals suggest it is the  backwards signal between these areas that is  lost when they are knocked out. 
Mashour’s group recently published  EEG work showing that this is important in  people too. Both propofol and the inhaled  anaesthetic sevoflurane inhibited the  transmission of feedback signals from the  frontal cortex in anaesthetised surgical  patients. The backwards signals recovered at  the same time as consciousness returned. 
Similar findings are coming in from studies  of people in a coma or persistent vegetative  state (PVS), who may open their eyes in a sleep-  wake cycle, although remain unresponsive. 
 Studying anaesthesia  might shed light  on conditions  such as coma 
understanding consciousness in health and  disease,” says Adrian Owen of the University  of Western Ontario in London, Canada. 
Owen and others have previously shown that  people in a PVS respond to speech with electrical  activity in their brain. More recently he did the  same experiment in people progressively  anaesthetised with propofol. Even when  heavily sedated, their brains responded to  speech. But closer inspection revealed that  those parts of the brain that decode the  meaning of speech had indeed switched off,  prompting a rethink of what was happening in  people with PVS. “For years we had been  looking at vegetative and coma patients whose  brains were responding to speech and getting  terribly seduced by these images, thinking  that they were conscious,” says Owen. “This  told us that they are not conscious.” 
As for my own journey back from the void,  the first 1 remember is a different clock telling  me that it is 10.45 ^ncn. Thirty-five minutes  have elapsed since my last memory - time that  1 can’t remember, and probably never will. 
“Welcome back,” says a nurse sitting by  my bed. I drift in and out of awareness for a  further undefined period, then another nurse  wheels me back to the ward, and offers me a  cup of tea. As the shroud of darkness begins  to lift, I contemplate what has just happened.  While I have been asleep, a team of people  have rolled me over, cut me open, and  rummaged about inside my body - and I don’t  remember any of it. For a brief period of time  “1” had simply ceased to be. 
My experience leaves me with a renewed  sense of awe for what anaesthetists do as a  matter of routine. Without truly knowing how,  they guide hundreds of millions of people a  year as close to the brink of nothingness as it is  possible to go without dying. Then they bring  them safely back home again. ■ 
The Human Brain I NewScientist: The Collection I 93 
 ...there is now  real hope 
'Brain cancers are the deadliest  cancers, and weVe barely seen  an improvement in 3 decades.  There is now, For the First time,  real hope oF extending the liFe  oF patients with new therapeutic  approaches; this is wonderFul  news For patients.* 
Professor Inder M. Verma, Ph.D,  Cure Brain Cancer Foundation  Scientific Advisory Committee,  Professor, Laboratory of Genetics  The Salk Institute for Biological  Sciences 
Cure Brain Cancer 
FOUNDATION 
Many minds, one purpose 
Find out more about us  curebraincancer.org.au/research 
 Consciousness is just another state of matter, like a solid,  liquid or gas, says physicist Max Tegmark 
WHY are you conscious right now? 
Specifically, why are you having a subjective  experience of reading these words, seeing  colours and hearing sounds, while the  inanimate objects around you presumably  aren’t having any subjective experience at all? 
Different people mean different things by  '‘consciousness”, including awareness of  environment or self. 1 am asking the more  basic question of why you experience  anything at all, which is the essence of what  philosopher David Chalmers has coined  “the hard problem” of consciousness. 
A traditional answer to this problem is  dualism - that living entities differ from  inanimate ones because they contain some  non-physical element such as an “anima” or  “soul”. Support for dualism among scientists  has gradually dwindled. To understand why,  consider that your body is made up of about  quarks and electrons, which as far as we  can tell move according to simple physical  laws. Imagine a future technology able to  track all of your particles : if they were found  to obey the laws of physics exactly, then your  purported soul is having no effect on your  particles, so your conscious mind and its  ability to control your movements would  have nothing to do with a soul. 
If your particles were instead found not  to obey the known laws of physics because  they were being pushed around by your soul,  then we could treat the soul as just another  physical entity able to exert forces on particles,  and study what physical laws it obeys. 
Let us therefore explore the other  option, known as physicalism: that  consciousness is a process that can occur  in certain physical systems. This begs a  fascinating question: why are some physical  entities conscious, while others are not? 
If we consider the most general state of  matter that experiences consciousness - let’s  call it “perceptronium” - then what special 
properties does it have that we could in  principle measure in a lab? What are these  physical correlates of consciousness? Parts of  your brain clearly have these properties right  now, as well as while you were dreaming last  night, but not while you were in deep sleep. 
Imagine all the food you have eaten in your  life and consider that you are simply some of  that food, rearranged. This shows that your  consciousness isn’t simply due to the atoms  you ate, but depends on the complex patterns  into which these atoms are arranged. If you  can also imagine conscious entities, say aliens  or future superintelligent robots, made out  of different types of atoms then this suggests  that consciousness is an “emergent  phenomenon” whose complex behaviour 
"Just as there are many types  of liquid, there are many  types of consciousness" 
emerges from many simple interactions. 
In a similar spirit, generations of physicists  and chemists have studied what happens  when you group together vast numbers of  atoms, finding that their collective behaviour  depends on the patterns in which they are  arranged. For instance, the key difference  between a solid, a liquid and a gas lies not in  the types of atoms, but in their arrangement.  Boiling or freezing a liquid simply rearranges  its atoms. 
My hope is that we will ultimately be  able to understand perceptronium as yet  another state of matter. Just as there are  many types of liquids, there are many types  of consciousness. However, this should not  preclude us from identifying, quantifying,  modelling and understanding the  characteristic properties shared by all liquid  forms of matter, or all conscious forms of  matter. Take waves, for example, which are 
substrate-independent in the sense that  they can occur in all liquids, regardless of  their atomic composition. Like consciousness,  waves are emergent phenomena in the sense  that they take on a life of their own: a wave  can traverse a lake while the individual water  molecules merely bob up and down, and the  motion of the wave can be described by a  mathematical equation that doesn’t care  what the wave is made of. 
Something analogous happens in  computing. Alan Turing famously proved  that all sufficiently advanced computers  can simulate one another, so a video-game  character in her virtual world would have no  way of knowing whether her computational  substrate (“computronium”) was a Mac or a  PC, or what types of atoms the hardware was  made of. All that would matter is abstract  information processing. If this created  character were complex enough to be  conscious, like in the film The Matrix, then  what properties would this information  processing need to have? 
I have long contended that consciousness is  the way information feels when processed  in certain complex ways. The neuroscientist  Giulio Tononi has made this idea more  specific and useful, making the compelling  argument that for an information processing  system to be conscious, its information must  be integrated into a unified whole. In other  words, it must be impossible to decompose  the system into nearly independent parts -  otherwise these parts would feel like two  separate conscious entities. Tononi and his  collaborators have incorporated this idea into  an elaborate mathematical formalism known  as integrated information theory (IIT). 
IIT has generated significant interest in  the neuroscience community, because it  offers answers to many intriguing questions.  For example, why do some information  processing systems in our brains appear > 
The Human Brain I NewScientist: The Collection I 95 
to be unconscious? Based on extensive  research correlating brain measurements  with subjectively reported experience,  neuroscientist Christof Koch and others have  concluded that the cerebellum - a brain area  whose roles include motor control - is not  conscious, but is an unconscious information  processor that helps other parts of the brain  with certain computational tasks. 
The IIT explanation for this is that  the cerebellum is mainly a collection of  ‘‘feed-forward” neural networks in which  information flows like water down a river,  and each neuron affects mostly those  downstream. If there is no feedback, there is  no integration and hence no consciousness.  The same would apply to Google’s recent  feed-forward artificial neural network that  processed millions of YouTube video frames  to determine whether they contained cats. 
In contrast, the brain systems linked to  consciousness are strongly integrated,  with all parts able to affect one another. 
IIT thus offers an answer to the question  of whether a superintelligent computer  would be conscious: it depends. A part of  its information processing system that is  highly integrated will indeed be conscious.  However, IIT research has shown that for  many integrated systems, one can design a  functionally equivalent feed-forward system  that will be unconscious. This means that 
"Consciousness is the way  information feels when  processed in certain ways" 
so-called “p-zombies” can, in principle, exist:  systems that behave like a human and pass  the Turing test for machine intelligence, yet  lack any conscious experience whatsoever.  Many current “deep learning” AI systems are  of this p-zombie type. Fortunately, integrated  systems such as those in our brains typically  require far fewer computational resources  than their feed-forward “zombie” equivalents,  which may explain why evolution has  favoured them and made us conscious. 
Another question answered by IIT is why  we are unconscious during seizures, sedation  and deep sleep, but not REM sleep. Although  our neurons remain alive and well during  sedation and deep sleep, their interactions are  weakened in a way that reduces integration  and hence consciousness. During a seizure,  the interactions instead get so strong that  vast numbers of neurons start imitating one  another, losing their ability to contribute 
independent information, which is another  key requirement for consciousness according  to IIT. This is analogous to a computer hard  drive where the bits that encode information  are forced to be either all zeros or all ones,  resulting in the drive storing only a single  bit of information. Tononi, together with  Adenauer Casali, Marcello Massimini and  other collaborators, recently validated these  ideas with lab experiments. They defined  a “consciousness index” that they could  measure by using an EEG to monitor the  electrical activity in people’s brains after  magnetic stimulation, and used it to  successfully predict whether they were  conscious. 
Detection devices 
Awake and dreaming people had comparably  high consciousness indices, whereas those  anaesthetised or in deep sleep had much lower  values. The index even successfully identified  as conscious two patients with locked-in  syndrome, who were aware and awake but  prevented by paralysis from speaking or  moving. This illustrates the promise of this  technique for helping doctors determine  whether unresponsive patients are conscious. 
Despite these successes, IIT leaves many  questions unanswered. If it is to extend our  consciousness-detection ability to animals,  computers and arbitrary physical systems,  then we need to ground its principles in  fundamental physics. IIT takes information  measured in bits as a starting point. But when  I view a brain or computer through my  physicist’s eyes, as myriad moving particles,  then what physical properties of the system  should be interpreted as logical bits of  information? I interpret as a “bit” both the  position of certain electrons in my computer’s  RAM memory (determining whether the  micro-capacitor is charged) and the position  of certain sodium ions in your brain  (determining whether a neuron is firing),  but on the basis of what principle? Surely  there should be some way of identifying  consciousness from the particle motions  alone, even without this information  interpretation? If so, what aspects of the  behaviour of particles correspond to  conscious integrated information? 
The problem of identifying consciousness  in an arbitrary collection of moving particles is  similar to the simpler problem of identifying  objects in such a system. For instance, when  you drink iced water, you perceive an ice cube  in your glass as a separate object because its 
 parts are more strongly connected to one  another than to their environment. In other  words, the ice cube is both fairly integrated  and fairly independent of the liquid in the glass.  The same can be said about the ice cube’s  constituents, from water molecules all the way  down to atoms, protons, neutrons, electrons  and quarks. Zooming out, you similarly  perceive the macroscopic world as a dynamic  hierarchy of objects that are strongly  integrated and relatively independent, all the  way up to planets, solar systems and galaxies. 
This grouping of particles into objects  reflects how they are stuck together, which can  be quantified by the amount of energy needed  to pull them apart. But we can also reinterpret 
96 1 NewScientist: The Collection I The Human Brain 
 Why do we experience things  in the ways that we do? 
this in terms of information: if you know the  position of one of the atoms in the piston of an  engine, then this gives you information about  the whereabouts of all the other atoms in the  piston, because they all move together as  a single object. A key difference between  inanimate and conscious objects is that for  the latter, too much integration is a bad thing:  the piston atoms act much like neurons  during a seizure, slavishly tracking one  another so that very few bits of independent  information exist in this system. A conscious  system must thus strike a balance between too  little integration (such as a liquid with atoms  moving fairly independently) and too much  integration (such as a solid). This suggests  that consciousness is maximised near a phase  transition between less- and more-ordered  states; indeed, humans lose consciousness  unless key physical parameters of our brain  are kept within a narrow range of values. 
An elegant balance between information  and integration can be achieved using error-  correcting codes: methods for storing bits  of information that know about each other,  so that all information can be recovered  from a fraction of the bits. These are widely  used in telecommunications, as well as in the  ubiquitous QR codes from whose characteristic  pattern of black and white squares your  smartphone can read a web address. As  error correction has proven so useful in our  technology, it would be interesting to search  for error-correcting codes in the brain, in case  evolution has independently discovered their  utility - and perhaps made us conscious as a  side effect. 
We know that our brains have some ability  to correct errors, because you can recall the  correct lyrics for a song you know from a  slightly incorrect fragment of it. john  Hopfield, a biophysicist renowned for his  eponymous neural network model of the  brain, proved that his model has precisely  this error-correcting property. However, if the  hundred billion neurons in our brain do form  a Hopfield network, calculations show that it  could only support about 37 bits of integrated  information - the equivalent of a few words  of text. This raises the question of why the  information content of our conscious  experience seems to be significantly larger  than 37 bits. The plot thickens when we view  our brain’s moving particles as a quantum-  mechanical system. As I showed in a recent  paper, the maximum amount of integrated  information then drops from 37 bits to  about 0.25 bits, and making the system  larger doesn’t help. 
This problem can be circumvented by  adding another principle to the list that a  physical system must obey in order to be  conscious. So far I have outlined three: the  information principle (it must have substantial  information storage capacity), the  independence principle (it must have  substantial independence from the rest of  the world) and the integration principle  (it cannot consist of nearly independent  parts). The aforementioned 0.25 bit problem  can be bypassed if we also add the dynamics  principle - that a conscious system must have  substantial information-processing capacity, 
'Glaring problems in physics  come from our confusion  about consciousness" 
and it is this processing rather than the static  information that must be integrated. For  example, two separate computers or brains  can’t form a single consciousness. 
These principles are intended as  necessary but not sufficient conditions for  consciousness, much like low compressibility  is a necessary but not sufficient condition  for being a liquid. As I explore in my book  Our Mathematical Universe, this leads to  promising prospects for grounding  consciousness and IIT in fundamental physics,  although much work remains and the jury is  still out on whether it will succeed. 
If it does succeed, this will be important  not only for neuroscience and psychology,  but also for fundamental physics, where  many of our most glaring problems reflect our  confusion about how to treat consciousness. 
In Einstein’s theory of general relativity, we  model the “observer” as a fictitious  disembodied massless entity having no effect  whatsoever on that which is observed. In  contrast, the textbook interpretation of  quantum mechanics states that the observer  does affect the observed. Yet after a century of  spirited debate, there is still no consensus on  how exactly to think of the quantum observer.  Some recent papers have argued that the  observer is the key to understanding other  fundamental physics mysteries, such as why  our universe appears so orderly, why time  seems to have a preferred forward direction,  and even why time appears to flow at all. 
If we can figure out how to identify  conscious observers in any physical  system and calculate how they will perceive  their world, then this might answer these  vexing questions. ■ 
TheHumanBrain I NewScientist: The Collection I 97 
 CHAPTER SEVEN 
EXTENDED MIND 
 Our minds extend way out into the material  world around us, argues cognitive archaeologist 
Lambros Malafouris 
WHERE should we look for the mind? This  might sound like an odd question: surely,  thinking takes place inside people’s heads.  Nowadays, we even have sophisticated  neuroimaging techniques to prove it. 
As deeply intuitive as this assumption about  the boundaries of the mind may be, I think  it is quite mistaken. 
I see no compelling reason why the study  of the mind should stop at the skin or the  skull. Quite the contrary. There is an  abundance of evidence, ranging from earliest  prehistory to the present, to testify that  things, as well as neurons, participate in  human cognitive life. From the viewpoint  of archaeology, it is clear that stone objects,  body ornaments, engravings, clay tokens and  writing systems play an active role in human  evolution and the making of the human mind.  Consequently, I suggest that what is outside  the head may not necessarily be outside the  mind. In fact, I doubt if notions like “inside”  and “outside” make any useful sense in the  study of human cognition. 
It is easy to see how the mind and the brain  became equated. Most of what we know about  the human mind has been uncovered through  isolating people from the material culture  they are usually surrounded by in order to  study them. This makes good sense if you are  a neuroscientist, because of the constraints  imposed by using a brain-scanning machine.  But as a result, it often goes unnoticed that  much of our thinking takes place outside our  heads. Naturally, I do not mean to question the  neural basis of cognition, but to point out that  mind is more than a brain. 
Instead, it would be more productive  to explore the hypothesis that human  intelligence “spreads out” beyond the skin  into culture and the material world. I am a  cognitive archaeologist, trying to understand 
the way ancient people thought by studying  the archaeological evidence they left behind.  And this is exactly where the challenge for  this field lies: at the realm of engagement  with the material world. Meeting this  challenge demands reconnecting the brain  with the body and beyond, breaking with  reductionistic “internalist” explanations that  separate the mental realm from the realm of  the material world. This is where a theory iVe  developed - material engagement theory  (MET) - comes in. 
At its heart, MET aims to explore the  different ways in which things become  cognitive extensions or are incorporated by  the human body, such as when one makes  numbers and symbols out of clay, or uses a  stone to strike another, forming a tool. It also  investigates how those ways might have  changed since earliest prehistory, and what  those changes mean for the ways we think.  This approach gives a new understanding of  what minds are, and what they are made of,  by changing what we know about what things  do for the mind. 
Think of a blind person with a stick. Where  does this person’s self begin? This famous  example is one of my favourites. The unity  of the blind man and the stick offers a way  to conceptualise minds and things as  continuous, but it also provides an analogy  for the profound plasticity of the human  mind: using a stick, the blind man turns  touch into sight, but the stick has its own  interesting active role. Tactile sensation is  somehow projected onto the point of contact  between the tip of the stick and the outside  environment. As a result, the brain treats  the stick as part of the body. This is not  simply a matter of expanding “peripersonal  space” - that is, the space surrounding the  body. Neither is it simply a matter of 

A blind person's  walking stick is an  extension of their  mind -a "cognitive  prosthesis" 
substituting vision for touch. The stick does  more than that. It becomes an interface of a  peculiarly transformative sort - what might  be called a brain- artefact interface, or a  “cognitive prosthesis”. 
It is especially in the latter sense that  the example of the blind man’s stick  encapsulates the spirit of MET. It reminds  us of something that many people forget;  namely, that it is in the nature of human  intelligence to remain amenable to drastic,  deep reorganisation by incorporating new  technological innovations. 
Let me explain. My approach sees the  human mind as an unfinished project, in a  permanent state of ongoing evolution and  neural reuse. It is important to keep in mind  that, whatever actual form the “stick” might  have taken in the history of our species - from 
98 1 NewScientist: The Collection | The Human Brain 
 the earliest Palaeolithic stone tools to the  internet - its primary function was that of a  pathway instead of a boundary. Through the  “stick”, the human species, much like the  blind man in our example, feels, discovers,  and makes sense of the environment, but also  enacts the way forward. 
Let’s not forget that from an evolutionary  point of view, the main reason we have a brain  is to move, not to contemplate. And it seems  fair to say that the reason we came to have  our sophisticated capacities for thought and  language is that, unlike any other animal,  we gave our movement purpose, direction 
"It often goes unnoticed that  much of our thinking takes  place outside our heads" 
and meaning. We had to use a “stick” to  accomplish that; something concrete, a  material scaffold to think through, with  and about. We came to have a sapient mind  because we are Homofaber- a concept  developed by the French philosopher Henri  Bergson in his 1907 book Creative Evolution,  which holds that human intelligence was  originally a facility to create artificial objects.  Tool-making and tool use was just the beginning  in a series of prostheses and material signs.  Indeed, things do much of our thinking. 
That is also why a stick used by a monkey  in captivity to retrieve food is of a different  kind. For humans, “sticks” are also used for  sight - in the Aristotelian sense in which  “seeing” is intimately associated with our  desire to know. In contrast, for non-human  animals, sticks are basically for eating. That’s 
a difference that makes a difference. 
This unique human predisposition for  engagement with material culture explains  why we humans, more than any other species,  make things, and how those things, in return,  make our minds what they are. I call that  metaplasticity- we have plastic minds that  develop and change as they interact with the  material world. 
I want to put materiality back into the  cognitive equation. MET offers a new way  of understanding how different forms of  material culture, from the stone hand-axe  to the iPhone, may have provided a powerful  mechanism of defining, but also transforming,  what we are and how we think. Mind-changing  technology has a futuristic, sci-fi ring to it, but  what most people don’t realise is that humans  have used it since they first evolved. ■ 
The Human Brain | NewScientist: The Collection! 99 

It's not just your mind that does the thinking, David Robson discovers 
T ake a minute out of the hustle and 
bustle of your busy life and sit very still.  Now, place your hands on the arms of  the chair or the desk in front of you, and try  to focus your attention on counting your  heartbeats. Can you feel a throbbing drum  roll, a slight murmur or nothing at all? How  does your bladder feel - is it empty or will you  need to dash for the bathroom within the next  half hour? You may be surprised to learn that  these bodily sensations are helping you think. 
We tend to view the mind as an aloof,  disembodied entity, but it is becoming  increasingly clear that the whole body is  involved in the thinking process. Without  input from your body, your mind would be  unable to generate a sense of self or process  emotions properly. Your body even plays a role  in thinking about language and mathematics.  And physiological sensations, such as those  from your heart and bladder, influence such  diverse personal attributes as the strength of  your tendency to conform, your willpower  and whether you are swayed by your  intuitions or governed by rational thought. 
In the past few years, discoveries about  mind-body connections have overturned the  long-held view of the body as a passive vehicle  driven by the brain. Instead there is more of a  partnership, with bodily experiences playing  an active role in your mental life. “The brain  cannot act independently of the body,” says  Arthur Glenberg at the University of  Wisconsin-Madison. Tune in to the body’s  signals, and you can exploit this to improve  your creativity, memory and self-control. 
Rene Descartes must be turning in his  grave at these findings. In his Meditations  on First Philosophy, published in 1641, he  famously argued that the mind and body are,  in essence, two separate entities that could  theoretically exist entirely independently of  one another. The book sparked a fierce debate  into the exact nature of the mind-body  connection - a debate that continues to this  day. At the centre of the modern discussion is  the puzzling sensation of embodiment. The  feeling that we own the flesh and blood that  stretches from the tips of our toes to the crown  of our head is the essence of our sense of self. 
As such, embodiment is central to  consciousness, yet, until recently, we knew  little about it. 
The first hint of the answer came from  an eerie illusion discovered in the late 1990s  by Matthew Botvinick, then a doctoral  student at Carnegie Mellon University in  Pittsburgh, Pennsylvania. He had the idea that  embodiment emerges from the brain’s need  to integrate the information it is receiving  from various senses. A Halloween party gave  him the perfect opportunity to test this, when  he discovered that someone had brought a  rubber arm along as part of their costume.  Placing the fake arm where he could see it on  a table, while hiding his real arm from view,  Botvinick asked an accomplice to stroke both  rubber and real arms at equivalent places and  in time with one another. As he suspected, in  an attempt to reconcile the tactile and visual  stimuli, he began to feel as if the stroking  sensation was coming from the arm he could  see. It was as if his brain had forgotten about the  real arm and now felt it owned the fake one. He  was suitably spooked by the sensation: “I was  so unsettled I threw the arm across the room.” 
Subsequent lab experiments confirmed the  result wasn’t just the product of a hard night’s  partying. Importantly, Botvinick also found  that the illusion did not occur when brush  strokes on the real and fake arm were out of  sync, because then the brain was not receiving  confused messages that it had to resolve. 
Soon, other groups saw the potential of the  rubber-hand illusion for unlocking the secrets  of embodiment. Brain scans taken as people  fell for the trick showed that we have a crude  body map in the brain’s right temporoparietal  junction. When our senses provide information  about our bodies, this is compared and  integrated with the map in the premotor  and parietal cortices (see diagram, page 103).  Any mismatch must be resolved at this stage,  leading to illusions such as the rubber hand.  However, it is only when the integrated  information reaches another area called the  insular cortex that the feeling of embodiment  pops into conscious awareness. 
That’s not all. The insular cortex also  processes our internal bodily signals. 
 100 1 NewScientist: The Collection | The Human Brain 
CREATIVE 
POSTURING 
 Truman Capote once described himself as a  "horizontal author", saying: "I can't think unless  I'm lying down, either in bed or stretched on a  couch and with a cigarette and coffee handy."  Vladimir Nabokov was a similarly supine writer. 
They might have had a point. In 2005, Darren  Lipnicki and Don Byrne, both then at the  Australian National University in Canberra,  found that people solved anagrams in about  10 per cent less time when lying down  compared with standing. The mechanism is  fairly simple. Stress is well known to be the  enemy of creativity, and we feel more relaxed  on our backs than on our feet. 
If you can't persuade your boss to buy you a  chaise longue for the office, there are some  more discreet ways to get those creative juices  flowing. For instance, Joel Cretenet and Vincent  Dru from Paris West University Nanterre La  Defense suggest that you extend your left arm  out in front of you or bend your right arm at the  elbow, so you resemble Auguste Rodin's iconic  statue The Thinker. Volunteers who made  these moves performed much better on a  creative thinking task, in which they had to find  innovative uses for an everyday object, such as  a brick. The explanation is complicated, but it  seems the movements are tied to our instincts  to approach or distance ourselves from a  situation. This helps broaden our outlook on  the problem, which is known to be crucial for  flexible thinking. 
Even simple eye movements left and right  across your field of vision can help you to think  more laterally. It is thought that this  temporarily encourages communication  between the right and left hemispheres of the  brain, which boosts creativity. 
'Mathematical thinking  seems to piggyback on  our experience of  movement and space" 
including the throb of our pulse and rumble  of our gut. And it turns out that people vary  greatly in how good they are at detecting  these, an ability known as interoception. A  team led by Manos Tsakiris at Royal Holloway,  University of London, found that around a  quarter of volunteers were able to count  their own heartbeats with an accuracy of at  least 8o per cent without taking their pulse,  while another quarter had little conscious  awareness of it, missing the actual number  by 50 per cent or more. Intriguingly, the team  also found that those who were particularly  good at interoception were less susceptible to  embodiment illusions, perhaps because these  internal sensations override the contradictory  information from their eyes. '‘If you have a  strong sense of self from the inside, you don’t  rely so much on external information like  vision and touch,” says Tsakiris. 
Since those pioneering experiments,  all kinds of related illusions have surfaced,  each unveiling more about the mind-body  connection and the way it moulds our  thinking. Henrik Ehrsson and his colleagues  at the Karolinska Institute in Stockholm,  Sweden, for example, recently used a set-up  similar to Botvinick’s to persuade volunteers  to embody plastic bodies of various sizes,  including the diminutive figure of a Barbie  doll. Ehrsson noticed that the subjects  perceived things as being much bigger when  they were under the illusion that they were  just 30 centimetres tall. “When we sat next to  them, they had the sense that a giant body was  nearby,” he says. This suggests that our body  awareness affects how we interpret the raw  information hitting our eyes. 
Tsakiris, meanwhile, has found that if he  strokes someone’s face in sync with a random  face being stroked on a screen, he can persuade  them to feel as if the image is their own  reflection. This illusion is particularly  intriguing, since it indicates that the body’s  influence might reach beyond sensory  perception to determine how we relate to  other people. Tsakiris believes it could explain  why we warm to people who subtly copy our  facial expressions and body language. He  thinks that seeing a reflection of our > 
The Human Brain I NewScientist: The Collection 1 101 
 FLEXING YOUR  WILLPOWER 
 Before you make your next important  decision, try to hold off from visiting the  bathroom for a few hours, says Mirjam Tuk  at Imperial College London. "It could give you  a little more self-control." She's not joking. 
Tuk's discovery came after she read that  just one neural circuit determines our  self-control in lots of different areas. 
She wondered whether flexing willpower  in one domain might therefore bolster  resolve in another. Just then, nature started  to call, suggesting the perfect way to test  her idea. In her subsequent experiment, 
Tuk, who was at the time working at the  University of Twente in the Netherlands,  asked half of her volunteers to drink a few  glasses of flavoured water, under the ruse  that they were taking a taste test; the rest  just took a sip of each sample. They waited  a while before trying numerous tasks,  including a classic test of self-control -  considering whether they would prefer to  receive a small amount of money now, or a  larger sum at a later date. The subjects who  had downed the drinks were more likely to  choose to wait. 
Fortunately, a bursting bladder is not the  only way your body can help to increase your  willpower. Walking backwards, or tensing your  muscles, can strengthen your resolve, too.  Folding your arms, meanwhile, seems to make  you more persistent at a task in hand. 
movements in someone else may evoke a faint  version of the face-swap illusion, prompting  us to act towards them rather as if we are  admiring ourselves in the mirror. 
An experiment by Maria-Paola Paladino at  the University of Trento in Italy lends some  support to this idea. She asked volunteers  experiencing the face-swap illusion to rate  their own personality, and to guess at the  personality of the person on the screen in  front of them. They considered themselves  to be strikingly similar to the person on the  screen. In a subsequent perceptual test, they  were asked to estimate the number of letters  flashed on a screen, while being told how the  virtual person had answered. They were more  likely to chose a figure around that answer than  were people who had not been subjected to the  illusion. Since people who are naturally  sensitive to their internal signals are not as  easily hoodwinked by body illusions, they may  be less affected by this kind of social  manipulation, and less empathic as a result. 
If the simple feeling of the heart beating  in our chest is somehow connected to our 
subconscious reactions to a person, how  might the body’s myriad other processes be  shaping our thinking? That is exactly what  researchers studying “embodied cognition”  would like to know. Running against  Descartes’s philosophy, this school of thought  maintains that many, if not all, aspects of our  mental lives are inextricably linked to the 
'Before you make your  next important decision,  try to hold off from visiting  the bathroom" 
experiences of our flesh and blood. 
Emotional experience is perhaps the best-  studied area of embodied cognition. As a  simple example, you may think that you smile  because you are happy, but in fact happy  feelings arise in a large part from the physical  sensation of smiling. Even very subtle facial  expressions appear to be essential for us to  process emotions. In one particularly elegant 
example, Glenberg’s team showed that  people whose frown muscles had been  frozen with botox took longer to read sad  or angry sentences than they did before  receiving the treatment. Emotions are also  linked to physical sensations. We tend to  feel colder when we feel lonely, for example,  and we associate warmth with friendliness  and inclusion. 
Such findings suggest that people who are  in tune with their bodies are more sensitive  to their own feelings. The jury is out on this  one, though the fact that the same brain  region - the insular cortex - handles both  interoception and emotional processing  supports the idea. There is also evidence that  you can tap into this connection to improve  your intuitive decision-making (see “How to  follow your heart”, right). 
Touchy-feely emotions are one thing,  but embodied cognition might stretch  even further to abstract thought processes.  Mathematical thinking, for example, seems  somehow to piggyback on our experience of  movement and space. When people are asked 
102 1 NewScientist: The Collection | The Human Brain 

HOWTO FOLLOW  YOUR HEART 
The feeling of being you 
Your sense of self, a key aspect of consciousness, is created by both your body and your brain 
 PARIETAL CORTEX  Processing of sensory information 
PREMOTOR CORTEX  ^ Processing of sensory information 
INSULAR CORTEX 
Processing of internal bodily  signals (interoception). Integration  of mental map and sensory  information to create sense of self 
to think of random numbers, they are more  likely to come up with smaller ones if they  look down and to the left, and bigger ones if  they look up and to the right. Other studies  show that language is also deeply embodied.  Every time we hear a word, the brain seems to  simulate the actions associated with its  meaning. When someone says the word  '‘climb”, for example, it activates the same  neural regions that trigger our muscles to pull  our weight up a tree. What’s more, appropriate  hand gestures can help our understanding of  these words. 
The field of embodied cognition is only  just beginning to blossom, although it has  had a relatively long history. Many questions  remain, including where these mind-body  associations come from. Are they innate or  learned in infancy? “When we’re cuddled up  with mum, we might learn to associate  warmth with feelings of social closeness,”  says Glenberg. But the link could be hardwired.  It also remains to be seen exactly how much  our mental and physical lives intertwine. 
“My personal belief is that all cognition is  embodied,” says Glenberg, “and the evidence  is slowly inching towards this view.” 
None of this detracts from the many  exciting applications of mind-body research.  Tsakiris is looking at the clinical uses of  illusions. A version of the rubber-hand  illusion might help the brain to accept a  prosthetic limb, while something akin to the  face illusion could ease the rehabilitation of  anyone receiving a face transplant. Face-swap  illusions could also be used to help us better  understand empathy and prejudice. 
Experiments in which white and black faces  are interchanged using a virtual environment,  for example, have been shown to help people  deal with implicit biases, while other  experiments have shown that giving people  superhero powers in a virtual reality can make  them behave in a more helpful manner in real  life. On a more frivolous note, body illusion  techniques could help players embody virtual  avatars in immersive video games. 
Education should benefit, too. Glenberg  has found that young children learn much  more quickly, and understand more, if they  are encouraged to play-act what they are  reading. Their memory of the words seems  to attach itself to the sensory experiences  involved, he thinks. Susan Goldin-Meadow  at the University of Chicago noticed  something very similar in children learning  simple equations. Those encouraged to  gesture tended to understand the material  more quickly and remember what they had  learned for longer. The mechanism remains  murky, although it is clear that the  movements somehow activate an implicit  understanding of the material. 
You might be able to make use of these  discoveries in your own life. Whether you  want to increase your willpower, creativity or  memory, there are numerous ways to exploit  the mind-body connection for your own  benefit (see “Creative posturing”, page loi, and  “Flexing your willpower”, page 102). The  effects may be moderate, but sometimes that’s  all you need. With your body helping you to  think more effectively, you never know what  you might achieve. ■ 
We often use metaphors involving the  body to describe the process of intuition - we  talk about going with our "gut instincts" or  "following our hearts". Perhaps we should  take these phrases more literally. Barnaby  Dunn, then at the Medical Research Council  Cognition and Brain Sciences Unit in  Cambridge, UK, and his colleagues have  found that people who take notice of subtle  physiological changes tend to be more  intuitive. 
The team first asked volunteers to sit  quietly and try to count out their heartbeats  without feeling for their pulse. All the  while, an ECG machine took an accurate  measurement. Comparing these two  results gives a good indication of a person's  "interoception", their ability to read their  body's internal signals. Then, to test their  intuition, the participants played a simple  computer game. The computer offered them  four decks of cards and on each round they  had to guess which deck would present a card  of a certain colour. Unbeknownst to the  players, the set-up was rigged - two of the  decks were always slightly more likely to  have the winning cards than the other two. 
The results were surprising. Those with the  best interoception tended to be either the  best, or the worst, at this card game. Those  who were bad at reading their body's signals  came right in the middle. 
Why could this be? Dunn suspects it is down  to the way we process our emotions. In another  experiment, he asked the same subjects to rate  their emotional reactions as they looked at a  series of emotive pictures. The better they were  at interoception, the more these ratings  correlated with physiological change, such as  a shift in heart rate. Dunn suggests that having  a hunch might create a flicker of excitement or  interest that is reflected in subtle changes in  physiology. Because people with good  interoception are more sensitive to these  signals, their perception of the hunch is  stronger, making it more likely that they will  act on it. "Their bodies are driving what they  decide," he says. That doesn't mean the hunch is  right, though - which would explain why these  people did the best, and worst, of the group. 
If you would like to tap into the signals that  your subconscious mind is sending your body,  you might want to take up meditation. Jocelyn  Sze and colleagues at the University of  California, Berkeley, have found that  meditating improves bodily awareness and  results in the same kind of link between  physiological and emotional reactions that  Dunn found to be crucial for intuition. 
The Human Brain I NewScientist: The Collection 1 103 
 104 1 NewScientist: The Collection | The Human Brain 
 CHAPTER EIGHT 
MEMORY 
We are all collections of memories. They dictate how we  think, act and make decisions, and even define our identity. 
Yet memory, with its many virtues and flaws, has puzzled us  for centuries. How are memories made and stored in the  brain? Why do we remember some events but not others? 
What do other animals remember? And how can we  improve the flawed instrument handed to us by evolution? 
Over the following pages we answer these questions and  many more, starting with a revolutionary new  understanding of memory’s purpose. 
Memon" 
The ultimate guide ^ 
W HEN thinking about the workings  of the mind, it is easy to imagine  memory as a kind of mental  autobiography - the private book of you. 
To relive the trepidation of your first day at  school, say, you simply dust off the cover  and turn to the relevant pages. But there is a  problem with this idea. Why are the contents  of that book so unreliable? It is not simply our  tendency to forget key details. We are also  prone to “remember” events that never  actually took place, almost as if a chapter from  another book has somehow slipped into our  autobiography. Such flaws are puzzling if  you believe that the purpose of memory is  to record your past - but they begin to make  sense if it is for something else entirely. 
That is exactly what memory researchers  are now starting to realise. They believe  that human memory didn’t evolve so that  we could remember but to allow us to imagine  what might be. This idea began with the  work of Endel Tulving, now at the Rotman  Research Institute in Toronto, Canada, who 
discovered a person with amnesia who could  remember facts but not episodic memories  relating to past events in his life. Crucially,  whenever Tulving asked him about his plans  for that evening, the next day or the summer,  his mind went blank - leading Tulving to  suspect that foresight was the flipside of  episodic memory. 
Subsequent brain scans supported the  idea, suggesting that every time we think  about a possible future, we tear up the pages  of our autobiographies and stitch together  the fragments into a montage that represents  the new scenario. This process is the key to  foresight and ingenuity, but it comes at the  cost of accuracy, as our recollections become  frayed and shuffled along the way. “It’s not  surprising that we confuse memories and  imagination, considering that they share  so many processes,” says Daniel Schacter,  a psychologist at Harvard University. 
Over the next lo pages, we will show how  this theory has brought about a revolution  in our understanding of memory. Given the 
many survival benefits of being able to  imagine the future, for instance, it is not  surprising that other creatures show a  rudimentary ability to think in this way  (page 106). Memory’s role in planning and  problem solving, meanwhile, suggests that  problems accessing the past may lie behind  mental illnesses like depression and post-  traumatic stress disorder, offering a new  approach to treating these conditions  (page 110). Equally, a growing understanding  of our sense of self can explain why we are  so selective in the events that we weave  into our life story - again showing definite  parallels with the way we imagine the future  (page 108). The work might even suggest  some dieting tips (page 112). 
It is still early days, but what’s clear is  that we are at the beginning of a long and  exciting journey. “The one thing that we  really have learned is that memory is  extraordinarily more complicated than  anyone would have thought 10 or 20 years  ago,” says Tulving. David Robson > 
The Human Brain | NewScientist: The Collection! 105 
1 • • • 
Memory in its simplest form is as ancient  as life itself. But do other creatures remember  like we do, asks Emma Young 
E very morning, you take a walk in the  park, bringing some bread to feed the  pigeons. As the days wear on, you begin  to see the birds as individuals; you even start  to name them. But what do the pigeons  remember of you? Do they think kindly of you  as they drop off to sleep at night, or is your face  a blank, indistinguishable from the others  strolling through the park? 
These questions may seem whimsical, but  knowing what other creatures recall is crucial  if we are to understand their inner lives. It  turns out that the range of mnemonic feats  in the wild is nearly as varied as life itself. 
If you take memory to mean any ability  to store and respond to past events, even  the simplest organisms meet the grade. 
Blobs of slime mould, for instance, which  can slowly crawl across a surface, seem to  note the timing of changes to their climate,  slowing their movement in anticipation of  an expected dry spell - even when it never  actually arrives. 
With the emergence of the first neurons  about half a billion years ago, memories  became more intricate as information  could be stored in the patterns of electrical  connections within the nervous system (see  “The making of a memory”, right). This type of  learning may have been behind the Cambrian  explosion - the sudden appearance and rapid  evolution of more complex species about  530 million years ago - because it enabled  animals to exploit new niches, say Eva  lablonka at Tel Aviv University and Simona  Ginsburg at the Open University of Israel. 
Over the following few hundred million  years, increasingly advanced skills could  emerge with different forces driving the  evolution of each creature’s mind. The result  is a surprising range of mnemonic feats  throughout the animal kingdom. Migratory  cardinal fish, for instance, can remember  where they laid their eggs during the breeding 
Hey, it's that  guy with the  bread again 
THE MAKING OF A MEMORY 
When we talk about memory, we can mean  many things. In the short term, we use our  working memory to juggle small lists of  information, such as a round of drinks. 
These are held in fleeting changes in the  brain's electrical or chemical activity that  quickly fade as the mind wanders. 
Long-term memories, in contrast, can last  a lifetime. They can be classed as semantic  memories of facts, or episodic memories  of events. Psychologists also refer to  autobiographical memories, which include  the episodic and semantic memories that  relate to our life story. 
All these different kinds of long-term  memories are woven into the webs of  connections between brain cells. By the  creation of new receptors at the end of a  neuron, by a surge in the production of a  neurotransmitter, or by the forging of new  ion channels that allows a brain cell to boost  the voltage of its signals, the brain alters the  communication between networks of cells. 
As a result, the same pattern of neurons  will fire when we recall the memory,  bringing the thought back into our  consciousness. Many brain regions  are involved in this process, but the  hippocampus, near the base of the brain,  is considered to be especially important  in consolidating our memories. 
Ultimately, these changes to the  neural network are probably stored  semi-permanently through epigenetic  changes, which involve small alterations  to the structure of a gene and determine  its activity within the cell. Certain genes  linked to the formation of memories have  been shown to have fewer methyl groups  attached to their DNA after learning,  for instance - a clear example of an  epigenetic change. 
But the brain is not like a video camera.  Every time we recall a memory, new proteins  are made and the epigenetic markers will  alter - changing it in subtle ways. 
 106 1 NewScientist: The Collection I The Human Brain 
 season and, after over-wintering in deep  water, return to within half a metre of the  same spot. Animals as diverse as lizards,  bees and octopuses can learn the way out of  a maze, and pigeons have an excellent visual  recognition, learning to recognise more than  a thousand different images. They can even  recognise individual humans and aren’t  fooled by a change of clothes. 
Such skills, although impressive,  don’t match our experiences of episodic  memory, in which we immerse ourselves  in specific events. A pigeon might learn to  associate your face with food, but it probably  can’t remember your last meeting in the  way you might be able to recall details of  your last trip to the park. 
"Santino the chimp  collects and hides  rocks to later throw  at visitors" 
It is an important distinction, because  episodic memory is thought to allow us to  imagine and plan for the future. This skill,  known as mental time travel, was long  thought to be unique to humans, but there are  now some signs that a handful of other species  might also be able to escape the present. 
Some of the most convincing evidence  comes from Nicola Clayton and Sergio Correia  at the University of Cambridge, who have  shown that western scrub jays can learn from  their experiences to anticipate the actions  of other birds. If one bird knows that another  is watching it bury its food, for instance, it  will later move the stash, presumably to  prevent it from being stolen. But they will  only do this if they have previously stolen  food themselves - suggesting that they were  drawing on their memories while forming  the plan. Similar studies have suggested that  bonobos and orang-utans are also capable of  mental time travel. 
Initially, the work attracted a lot of  scepticism from researchers like Michael  Corballis at the University of Auckland in  New Zealand, who believed that the results  could be explained by a complex kind of  classical conditioning, for instance. But some  recent work has begun to change his mind. He  points to a study of activity in the hippocampi  of rats, which suggests that they replay their  movements through a maze, and may even  imagine future paths that they could take. He  is also impressed by Santino, a chimp at  Furuvik Zoo in Sweden that collects and hides  rocks to throw at visitors, using premeditation  that would rely on episodic memory. 
Unfortunately, so few animals have been  studied that it is difficult to pinpoint exactly  when this skill emerged, although the  researchers suspect that it evolved separately  in the different lineages, rather than emerging  in one of our common ancestors. 
Thomas Suddendorf at the University of  Queensland in Australia is less willing to  accept that animal memories rival our own. 
He proposes that episodic memory depends  on a host of different components, and  although some animals may be able to use  limited foresight when it comes to food, for  instance, only humans demonstrate the kind  of capacity and flexibility that can allow us to  imagine all kinds of futures. 
“These simulations allow us to plan, prepare  for and deliberately shape the future, like no  other animal appears to do,” Suddendorf says.  Santino might be able to plan a rock attack -  but he could not plan anything so conniving  as a bid for freedom. - 
 Us vs the machines 
Just how do our memories compare  to today's PCs? 
Speed and motivation are probably our  biggest limits. Memorising a substantial  work of literature word for word can take 
years or decades 
^ even 
lOm 2.5m 
If the brain processed binary information like a  computer, with each synapse holding a single bit  of information, we could store roughly 
12,000GB 
You could hold a 700-page book like Moby Dick  nearly 10 million times, or 2.5 million songs 

a A mid-range computer may 
holdUUU  in its random access ^  memory (RAM), many  million times more than  human short-term memory 
A computer hard drive stores data by  magnetising sections of a ferromagnetic disk.  On a computer with a 
500GB 
a hard drive, you could  § store Moby Dick 400,000 times 
i 
" 101011100°01110H 
A computer can lay down memories  astonishingly quickly - absorbing  Moby D/ck in about 
0.5 seconds 
The Human Brain I NewScientist: The Collection 1 107 

 How do we piece together  an autobiography from the  many events in our lives,  wonders Kirsten Weir 
G raduation day. The first concert  you attended. Your first kiss. These  personal recollections stand apart from  memories of shopping lists or the world’s  capital cities. Autobiographical memories  define us; they are who we are. 
Yet they are far from complete, with some  periods of our lives producing heaps of  recollections while others receive relatively  patchy coverage. What forces lead us to  remember one event but forget another? Until  recently, the subject had largely been a black  box to researchers, but they have now begun to  make huge strides towards an understanding  of the way our minds write our life story. 
Our brains certainly start remembering  at a young age, learning simple associations  before we are born. One small study even  found that newborns tend to stop crying  when they hear the theme tune of a TV show  their mother often watched while pregnant,  perhaps because it reminds them of the  comfort of the womb. But we cannot  consciously remember specific events  from before the age of 2 or 3, when our  autobiographical memory begins to develop.  Even then, we are hard-pressed to remember  much from before our sixth birthday (for more  on this, see “Our forgotten years”, page 70). 
So far, three different factors have  emerged that might explain this hazy  recall. One possibility is that the neural  pathways are not mature enough between  the hippocampus - where memories are  consolidated - and the rest of the brain, so  our experiences from this period may never  be cemented into long-term storage. Our  burgeoning language skills also play a key  role, says Martin Conway at City University  London, because words provide a kind of  scaffold on which we hang our memories  for future retrieval. His experiments have  shown that children don’t tend to remember 
 "Your personality  determines the  events that you  remember" 
an event until they have learned the  words to describe it. 
A sense of our own identity is also crucial  for our memory of particular experiences.  Experiments show that children who can  recognise themselves in a mirror - a sign that  they have developed a sense of self - are able  to recall certain events when tested a week  later, while toddlers who fail the mirror test  draw a blank. 
As we get older, our identities and  recollections develop together in an  intimate dance. While the events in your  life shape your opinion of yourself, your  personality also determines what you  remember; someone who thinks they are  courageous might fail to remember a time  when they acted in a cowardly manner, for  example. “Your sense of who you are and  how you enact your personality traits is  very tied up in autobiographical memory,” 
108 1 NewScientist: The Collection | The Human Brain 
 Your memory  is made from the  scraps of your life 
 SHARED 
RECALL 
Autobiographical memories are, by  definition, personal. But that doesn't  mean they are all our own, says  Amanda Barnier, a cognitive scientist  at Macquarie University in Sydney,  Australia. She and her colleagues  interviewed couples that had been  married for decades. Not surprisingly,  couples who remembered together,  rather than independently, were able  to recall significantly more than those  who took a solo approach. 
Much research focuses on the  downsides of this process, including  the risk of false memories: it is not  uncommon for people to absorb their  siblings' or spouse's recollections into  their own life stories, for example. 
But Barnier argues that  collaborative recall's benefits have  long been overlooked. Understanding  the cues that couples use to prompt  one another could offer new ways to  shore up memory in elderly people  facing dementia, for instance. 
"We often hear about this idea  of someone losing their long-term  partner, and all of a sudden they  experience a rapid decline," she says.  "It must be like they've lost a part of  their mind." 
says Robyn Fivush at Emory University  in Atlanta, Georgia. 
Guiding all of this are our parents, who  form our identities and cement our  memories with their storytelling. When  families discuss personal events in an  elaborate way, children develop more  detailed narratives of their own by the time  they reach school age than those whose  parents weave less intricate stories.  Psychologist Qi Wang at Cornell University  in Ithaca, New York, believes this may  explain the influence of culture on the way  we reminisce. Chinese parents tend to focus  less on individual experiences and emotions  when discussing the past, with fewer details,  than Americans, for instance. As a result,  Wang has found that Chinese people’s  memories, even during adulthood, tend to be  less personal, focusing instead on events of  social or historical significance. 
 Events like family holidays are moulded to fit  the flowing narrative of our life story 
As we venture further from the safety  of our parents’ embrace, our autobiographical  memories continue to mature. The difference  is quite noticeable, says Conway; a lo-year-old  cannot relay a coherent life story, but a  20-year-old can go on for hours. '‘Something  happens over that adolescent period.” But  what? So far, studies to tackle that question are  lacking. “There’s a big lacuna between about  age 7 to late adolescence where we don’t really  know what’s going on,” he says. 
The cultural script 
We do know, however, that we are more  likely to remember events from the end of this  period, in young adulthood, than from any  other period in our lives. This “reminiscence  bump” may be the result of anatomical  changes to the still developing brain.  Alternatively, it may be that our brains feel  emotions more keenly during adolescence  and early adulthood - and memories linked to  intense feelings stick in the mind for longer. 
Or perhaps it is simply down to the fact  that many important landmarks in our lives -  learning to drive, graduating and falling in  love for the first time - tend to fall within this  period. “Those distinct events are more likely  to be remembered, because they’re culturally  marked,” Fivush says. 
This idea is supported by work conducted by  Annette Bohn and Dorthe Berntsen at Aarhus  University in Denmark. They found that when  young children were asked to write their  future life stories, most of the events they  imagined took place in young adulthood,  mirroring the reminiscence bump. So it seems  that we are aware of the “cultural life script”  from a young age, which may mould our  recollections of events as they occur. 
The finding dovetails with the idea that  memory and foresight share the same  machinery in the brain. A child’s ability to  imagine the future seems to develop in  tandem with his or her autobiographical  memory, for instance. Wang, meanwhile,  has found that the cultural differences that  shape our personal narratives can also  influence our planning abilities, showing that  Chinese people are less likely to give specific,  personal details than Americans when they  talk about events to come. 
Our autobiographical memories  aren’t perfect, to be sure. But whether we are  looking forward or gazing back into the  past, our personal narratives are central to  understanding our place in the world. That’s  a point worth remembering. 
TheHumanBrain I NewScientist: The Collection 1 109 

W HAT pushes someone to try to  take their own life? That’s what  psychologist Mark Williams was  trying to find out as he visited people  recovering from attempted suicide in the  UK’s Addenbrookes Hospital in the 1980s.  Williams knew he had to tread carefully:  the patients had been hospitalised for an  attempted overdose in the past 48 hours.  “These people had done dangerous things to  themselves,” he recalls. “You can’t ask them  to do complicated tests.” 
Williams was there because he suspected  there was something different about the  long-term memories of people who are  depressed or suicidal, and had devised a  simple exercise to test his theory on the  patients at Addenbrookes. Sitting at their  bedside, he would read out a cue word, such  as “happy” or “clumsy”, before asking them  to describe a past event it brought to mind.  Perhaps not surprisingly, they were quicker  to tell him about negative experiences than  positive ones, but Williams was struck by  something more subtle. 
While his comparison group - other  hospital patients who weren’t depressed -  tended to focus on specific events, the  overdose patients were noticeably vaguer.  One responded to the word “happy” with 
“the first years of my marriage”, for instance;  another person given the word “safe” said:  “when Tm in bed”. Even when Williams  encouraged them to be more specific, they  were less likely to dig out a single incident -  such as a particular film, or an insult that had  upset them. 
It was as if the depressed patients were  merely skimming the chapter headings of  their autobiographies, without reading the  text that followed. It might seem a minor  detail compared with the desperation that  leads to a suicide attempt. But Williams’s  findings, which are now supported by a  host of studies from other groups, have  emphasised just how important our  memories are in shaping our well-being,  offering a new perspective on depression  and perhaps other mental illnesses too. 
Holding on 
According to this theory, our memories act  as a kind of ballast that holds us steady during  times of stress; they can suggest ways to solve  problems and offer comfort when we are  feeling wounded. When people find it hard to  recall specific events from their past, however,  they feel overwhelmed by life’s challenges,  which slowly pushes them into depression. 
to 
Subtle memory  losses may lead us to  an unexpectedly dark  place. David Robson  investigates 
110 1 NewScientist: The Collection | The Human Brain 
 “In the right circumstances, the effect can be  striking,” says Williams, who is now at the  University of Oxford. If the theory is right,  there may be new ways of treating depression  that directly target the underlying memory  problems. 
A new approach would certainly be  welcome. Depression is the commonest  form of mental illness, affecting somewhere  between lo and 20 per cent of us at some  point in our lives. Antidepressants help  some people, particularly the most severely  affected, but these drugs can bring side effects,  including weight gain and loss of libido.  Meanwhile, talking therapies such as cognitive  behavioural therapy can be costly and often  take weeks or months to make an impact. 
Williams is by no means the first to suggest  that memory plays a part in mental illness;  Sigmund Freud once suggested that the  repression of unpleasant memories from  childhood could lead to hysteria. In the case  of Williams’s suicidal patients, however,  theirs was a more general difficulty. When  questioned, they painted their past in  broad brush strokes - “I always enjoyed a  good party” that missed the details of specific  events - “my brother’s 30th birthday”. 
Williams’s paper, published in 1986 in  the Journal of Abnormal Psychology,  triggered a trickle, then a torrent, of similar  studies. They revealed that “over-general  memory”, as the phenomenon came to be  known, was not limited to people who had  tried to commit suicide, but was linked to  depression in general. 
Further studies found it to be present  before the low mood developed, lending 
weight to the idea that the memory problems  led to depression and not the other way  around. For instance, one team examined the  memories and well-being of 74 women who  had undergone IVF and failed to get pregnant.  Those who had the least specific recall  before the treatment were most likely to  develop symptoms of depression after the  disappointment. Another study, published  in April, found that teenagers judged to have  over-general memory were more likely to  develop depression in the 12 months after  they first met the researchers. And a recent  small study has also linked this kind of poor  recall to bipolar disorder. 
As the body of evidence supporting this  idea has grown, various theories have  emerged about just how memory problems  could send our mood into a downward spiral.  One idea is that remembering the good times  is important for chasing the blues away.  “Thinking of better times gives you more  hope for the future,” says Jennifer Sumner of  Northwestern University in Evanston, Illinois. 
Given the role of memory in imagination  and foresight (see page 105), poor access to  our past may also impair our problem-solving  skills, which are known to be weaker in  people with depression. When asked how  you might make friends after moving to a new 
"Memories act as  a kind of ballast that  holds us steady during  times of stress" 
neighbourhood, for example, most people  can come up with good ideas, like inviting the  neighbours round for drinks. Those with  depression, in contrast, tend to be stumped by  these questions. Importantly, people with  over-general memory also seem to fare poorly  at this kind of task. “When you face problems  in your life, you don’t have an analogy to help  you solve the current situation,” says Rachel  Anderson at the University of Ffull, UK. It is  easy to imagine how, with your difficulties  mounting, you may then begin to feel  desperate and helpless, trapped by your  circumstances with no obvious escape. 
Flashbacks 
That might explain her finding that people  with less specific recall only develop  depression when they face long-term stresses,  such as ongoing quarrels with their partner;  those with fewer hassles show few ill-effects. 
As well as depression, over-general  memories could make people more  vulnerable to post-traumatic stress disorder. 
It may seem counter-intuitive, because PTSD  involves vivid memories of a traumatic  incident. But these flashbacks appear to be  the exception rather than the rule because  people with PTSD tend to have trouble  recalling other events from their past. Once  again, these difficulties seem to be present  long before the onset of the disorder -  firefighters with hazy recall are often the  first to develop the symptoms of PTSD, for  instance. Perhaps a poor memory just  weakens our mental fortress - and when  the defences are down, it’s easier for anxiety,  fear and painful flashbacks to intrude into  our thoughts. 
Why do people lose access to their  recollections in the first place? Given the  complexity of the human mind, it’s probably  the outcome of many interlinked processes.  Williams thinks we may learn the over-general  style of thinking from our parents, if they  tend to talk in broad terms about the past. 
It could also begin as a coping mechanism,  helping people to retreat from the pain of  a difficult experience. 
Tim Brennen at the University of Oslo in  Norway probed the memories of Bosnian  teenagers who had been young children  during the Bosnian war in the 1990s. “They  had seen people being killed, villages burnt  down. They were kept in a state of terror for  years,” says Brennen. The teens found it  harder to remember specific events in their  past than their Norwegian peers. > 
 The Human Brain I NewScientist: The Collection I 111 
"Poor memory  weakens our  mental fortress,  allowing fear to  intrude into our  thoughts" 
By the time Brennen met the Bosnians in  the late 2000s, many were living a relatively  peaceful life and hadn’t yet developed signs  of mental illness as a result of their experiences,  over-general memories or otherwise. That  doesn’t necessarily contradict the theory. 
As Anderson has found, the weaknesses in  our defences only show during times of  stress. Brennen suspects that the  consequences might kick in once they  face the challenges of adult life. 
Although this theory of depression is  gaining converts among researchers, it still  has plenty of critics. Mark Howe at City  University London points to a contradictory  study showing that people with depression  simply take longer to access their  recollections. If you give them enough time,  they can usually summon specific incidences  for a cue word, he says. Perhaps they are just  less keen than other people on sharing  personal recollections with a stranger. “I don’t  think their memory has fundamentally  changed,” he says. 
While the theory’s merits are still being  debated, its proponents are already exploring  whether a kind of memory training can be  used to improve people’s recall and so reduce  their symptoms of depression. Tim Dalgleish  at the MRC Cognition and Brain Sciences  Unit in Cambridge, UK, for instance, has  investigated a technique called Memory  Specificity Training (MeST), which encourages  people to practise delving into their  memories. In effect, they are asked to repeat  a similar version of Williams’s memory test  over and over again, recalling detailed specific  incidents for different cue words. Crucially,  the events need not have anything to do with  the person’s current anxieties. People can be  taught MeST in groups and may only need five  weekly sessions to see improvement if early  results are anything to go by. 
One of the first trials took place in Iran,  carried out by Hamid Neshat-Doost at the  University of Isfahan, who worked with  Dalgleish in Cambridge before returning to his 
home country. It involved 23 Afghani refugees  with depression, living in a community with  little access to cognitive behavioural  therapists. The 11 people who received five  group sessions of MeST improved  significantly, unlike the others, who went  untreated. Importantly, those with the most  improvements in their ability to recall  specifics reported the greatest improvements  in their mood. 
Admittedly that was a small, unblinded  trial and memory training would have to be  compared with traditional talking therapies  in a head-to-head trial before any conclusions  could even begin to be drawn about their  relative merits. After all, cognitive behavioural  therapy is also becoming more widely and  cheaply available through online programmes  and group therapy. But Williams, who has  worked on a similar form of memory training,  says MeST could be another useful option  for those who don’t respond well to cognitive  behavioural therapy or antidepressants.  ‘‘What’s nice is that it brings the patient on  board in a collaborative way,” he says. “It isn’t  stigmatising.” 
Sumner agrees that memory training looks  promising, having tried to encourage her own  patients to reminisce more specifically, with  positive results. “They don’t see their past  and future as [uniformly] negative,” she says.  “It gives them something to latch on to,  motivating them to make changes.” ■ 

^ "j 
People who are lost in  the here and now reveal a  strange interplay between  memory and body, says 
Catherine de Lange 
 IIZ I NewScientist: The Collection | The Human Brain 
 T o THE casual observer, there would  have been nothing unusual about  Henry Molaison as he tucked into  dinner at his usual slow-and-steady pace. 
But to the group of psychologists at the  Massachusetts Institute of Technology who  were observing him, his behaviour was  astonishing: just 6o seconds earlier, he had  polished off an identical three-course meal.  Yet Molaison was no glutton. Instead, part of  his brain had been removed in an attempt to  cure his epilepsy. From then on, he was unable  to form new memories and became stuck in  the present for perpetuity. 
Scientists usually consider feelings of  hunger to arise from hormonal signals in the  gut, but Molaison’s behaviour suggested that  our memories of what we have just eaten may  be more important in curbing our appetite.  The idea found further support a decade later,  in 1998, when Morris Moscovitch at the  University of Toronto, Canada, replicated this  experiment using two people with a similar 
memory condition. Not only did these people  eat a second meal, just 15 minutes after  finishing the first, but in some trials they  unquestioningly ate a third. 
There is always the possibility that the  brain damage may have brought on  complications besides the memory loss  that interfered with the gut’s signals to the  brain, but a recent experiment by Suzanne  Higgs at the University of Birmingham in  the UK suggests otherwise. She tapped into  “sensory specific satiety” - the familiar  sensation that our liking for a given food 
"Just 60 seconds  earlier, Henry  Molaison had  polished off an  identical three-  course meal" 
decreases the more we eat of it, whereas a  different dish will feel more appetising; it is  the reason that we can find extra space for  pudding. Higgs found that people with  amnesia retain such preferences. After a  hearty lunch of sandwiches they will prefer  crisps or cookies to further sandwiches, even  though they couldn’t tell you what  they had just eaten. She concludes that the  digestive signals are reaching the brain, and  that the amnesiacs’ lack of memory lies  behind their seemingly insatiable appetite. 
Incredible endurance 
The unexpected effects of memory on our  feelings and behaviour might not stop with  food. Diane Van Deren is one of the world’s  elite ultra runners. In one recent race she ran  more than 1500 kilometres over 22 days. 
On some of those days, she ran for as long  as 20 hours. Van Deren had always been good  at sport, but her incredible endurance seems  to be down in part to her poor short-term  memory, again the result of brain surgery  for epilepsy. 
Often, she just cannot remember how long  she has been running for, underestimating the  time by as much as 8 hours. “Most people with  amnesia suffer a tyranny of the present,” says  Adam Zeman, a neurologist studying memory  and epilepsy at the University of Exeter, UK,  but Van Deren’s inability to remember how  long she has been running seems to free her  from the feelings of fatigue that plague other  runners. Perhaps, while others get caught up  in the details of where they have been and  where they are going. Van Deren gets into a  more zen-like state that lets her run for longer  without feeling so much strain. Of course, it  could also be that after the challenges in her  life Van Deren has a higher threshold for  discomfort than most people. 
For the rest of us, losing track of time on a  long run is difficult, but there are certainly  ways in which these findings affect us all.  Higgs has found that simple distractions such  as watching TV can stop people from forming  good memories of what they are eating. As a  result, they tend to snack more after the meal  than control groups who were not distracted. 
Imagination can play a powerful role too.  Thanks perhaps to its close link to memory,  simply imagining the process of eating  something can lead people to feel more  satiated, causing them to eat less. Which all  goes to show that in the fight against overeating,  memory could be your biggest ally, even if at  times it would be more palatable to forget. 
The Human Brain I NewScientist: The Collection 1 113 
CHAPTER NINE 
MAKE THE MOST OF IT 
A USER'S GUIDE TD 
THE MIND 
The human mind is the most complex information processing  system we know. It has all sorts of useful design features but  also many glitches and weaknesses. The problem is, it doesn't  come with a user's manual. You just have to plug and play. 
But if anyone knows how to get the best out of our brains, it's  neuroscientists. So we asked some of the best to explain how  the human brain performs many of its most useful functions  and how to use them to the max. By Caroline Williams 
 114 1 NewScientist: The Collection | The Human Brain 
1 AHENTION 
 1 
2 
3 
4 
5  B  7 
ATTENTION 
WORKING 
MEMORY 
LOGICAL  & RATIONAL  THOOGHT 
LEARNING 
KNOWLEOGE 
CREATIVITY 
INTELLIGENCE 
Almost every useful feature of your  brain begins with attention. Attention  determines what you are conscious  of at any given moment, and so  controlling it is just about the most  important thing that the brain can do. 
To make any sense of the world  around us we need to filter out  almost everything and focus solely  on what is relevant. Not only that,  but focused attention is essential  for learning or memorising. So it  follows that if you can boost your  ability to pay attention, you can  improve at almost anything. 
In simple terms, the brain has two  attention systems. One, the  “bottom-up” system, automatically  snaps awareness to potentially  important new information, such  as moving objects, sudden noises  or sensations of touch. This system  is fast, unconscious and always on  (at least when you are awake). 
The other, the “top down” system,  is deliberate, focused attention, which  zooms in on whatever we need to think  about and, hopefully, stays there long  enough to get the job done. This is the  form of attention that is useful for  doing tasks that require concentration. 
Unfortunately distractibility comes  as both a bug and a design feature. Top-  down attention requires effort and so  is prone to losing focus, or being rudely  interrupted by the bottom-up system. 
The good news is that we can tweak  our attention settings to stay focused  more easily. As well as cutting down 
“Top-down  attention is prone  to tosing focus,  or being rudeiy  interrupted” 
on bottom-up distractions by turning  off email notifications, putting your  phone on silent and so on, Nilli Lavie, a  cognitive neuroscientist at University  College London, suggests actually  giving your brain more to do. 
Lavie’s work has shown that better  control of top-down attention comes  not by reducing the number of inputs,  but by increasing them. Her load theory  says that once the brain reaches its  limit of sensory processing, it can’t take  anything else in, including distractions. 
This seems to work for both  distractions and mind wandering, says  Lavie. In real life, she suggests thinking  about adding visual aspects to a task  that make it more attention-grabbing  without making it more difficult -  putting a colourful border around a  blank document and making the bit  you are working on purple, perhaps. 
It works with all the senses, she says,  so choosing somewhere with a bit of  background noise might also help. 
There are also signs that cognitive  training might help. Researchers  working with people with attention-  deficit hyperactivity disorder (ADHD)  and brain injuries have found that  cognitive training, combined with non-  invasive magnetic brain stimulation,  can improve focus on a task that needs  sustained attention. 
Wider studies are under way, and  initial results seem to suggest that the  right kind of brain training could help  more or less anyone. 
While we wait, the next best option  is learning to chill out in exactly the  right way. Long-term meditators have  been shown to have thicker parts of the  brain associated with attention, while  other studies have found that attention  test scores improved after a short  course of meditation. So learning  to focus better may be as simple as  making time to sit still and focus on  not very much. > 
The Human Brain I NewScientist: The Collection 1 115 
 L ike attention, working 
memory is one of the brain's  most crucial front-line  functions. Everything you  know and remember,  whether it's an event, a skill  or a fascinating fact, started  its journey into storage by going  through your working memory. 
But working memory is much  more than just a clearing house for  long-term memories. It has been  described as the brain's scratch pad:  the place where information is held  and manipulated. If you are doing  anything that requires effortful,  focused thought, you are using your  working memory. 
In the 1970s, Alan Baddeley and  Graham Hitch of the University of  York, UK, came up with an influential  model to explain how the system  works. The main component is the  executive controller, which runs the  show by focusing your attention on  the relevant information. 
It also kicks "slave" systems into  action. One of these holds up to four  pieces of visual information at a  time; another can memorise about  Z seconds of sound, especially  spoken words, which it loops over  and over again (think of mentally  repeating a phone number while you  search for a pen). The third is the  episodic buffer, which adds relevant  information from long-term memory. 
A weakness of this model is that  working memory doesn't occupy  a discrete brain area that can be  watched in action in a brain scanner.  Because of this, some cognitive  neuroscientists have suggested that  it might not be a separate system  at all, but just the part of long-term  memory that we are currently paying  attention to. 
Whatever it is, working memory  comes as standard in the human 
2 
L WORKING MEMORY 
brain, but some people have better  working memories than others.  Working memory capacity is a  better predictor of academic success  than IQ, so getting the most out of  it is useful. 
The good news is that the system  can probably be upgraded. Some  studies have shown that brain  training programmes aimed  specifically at working memory  can produce improvements, and  there are even a handful of training  packages on the market. But it's not  clear whether they make you better  at anything other than working  memory tests. 
Cognitive neuroscientist Jason  Chein of Temple University in  Philadelphia, Pennsylvania, who  studies working memory, says  there seems to be evidence of  improvements in other cognitive  skills, although any changes are  quite small. "A small effect may  still be important in the sense  that even modest gains can have  a meaningful impact on everyday  cognition," he says. 
“Even modest  gains in working  memory can  improve generai  cognition” 
 116 1 NewScientist: The Collection | The Human Brain 

LOGICAL AND  RATIONAL THOOGHT 
We like to think of ourselves as rational  and logical creatures. And so we can  be - but not without some effort. 
Logical thought requires us to behave  like a microprocessor, executing stepwise  operations on information using the rules  of logic. This doesn't come naturally to  most people, requiring outside instruction  to learn and lengthy training to master.  Even then, we struggle to maintain a  purely rational perspective. 
It turns out that there is a kernel of  truth in the popular wisdom that "left  brain equals logic". Imaging studies have  shown that the left prefrontal cortex is  needed to make logical trains of thought  happen and, a lot of the time, no input i^  needed from the right. 
But when there is conflict between |  what seems logical and beliefs we already  hold, the right side of the prefrontal  cortex kicks in to help sort out the  confusion. Unfortunately, the right  hemisphere usually wins. Study after  study has shown that where new  information conflicts with existing  beliefs, our brains bend over backwards to  keep beliefs intact rather than revise  them. 
Another surprise is that, contrary  to popular wisdom, emotions aren't  necessarily the enemy of rationality.  People who have damage to the part  of the prefrontal cortex that processes  emotions struggle to make decisions  at all, especially when there is no logical  advantage to either option (for more on  the mind-body connection, see "Your  clever body", page 100). 
So embracing our not-particularly  logical gut feelings about decisions might  actually help us make more rational 
choices. But not always: other studies  have shown that strong emotions can  interfere with making rational decisions,  particularly when they concern people  we love. 
Other than hard graft - and an  appreciation of the role of belief and  emotion - is there anything we can do  to become more logical? 
Vinod Goel, a cognitive psychologist  at York University in Toronto, Canada,  says that a zap to the head might one  day help. "Brain stimulation techniques  may eventually offer a route to improving 
‘There is a kernel  of truth in the  popular wisdom  that left brain  equals logic” 
reasoning," he says. His team recently  used a similar approach to enhance  creative thought and, he says, "one  can imagine the same techniques being  used to enhance our ability for logical  reasoning". As yet, though, there is no  shortcut. For now, he says, practice is your  best option. Recent studies have shown  that a few months' training in rational  thought, as part of law degree training,  increased the number of connections  between frontal and parietal lobes and  between the two hemispheres. The catch  is, without regular practice this effect  would almost certainly fade a few  months after the course ended. > 
 The Human Brain I NewScientist: The Collection 1 117 

 L earning is what your brain does naturally. 
In fact it has been doing it every waking  minute since about a month before you were  born. It is the process by which you acquire  and store useful (and useless) information  and skills. Can you make it more efficient?  The answer lies in what happens  physically as we learn. As it processes information,  the brain makes and breaks connections, growing  and strengthening the synapses that connect  neurons to their neighbours, or shrinking them  back. When we are actively learning, the making  of new connections outweighs the breaking of old  ones. Studies in rats have shown that this rewiring  process can happen very quickly - within hours of  learning a skill such as reaching through a hole to  get a food reward. And in some parts of the brain,  notably the hippocampus, the brain grows new  brain cells as it learns. 
But once a circuit is in place, it needs to be used  if it is going to stick. This largely comes down to  myelination - the process whereby a circuit that is  stimulated enough times grows a coat of fatty  membrane. This membrane increases conduction  speed, making the circuit work more efficiently. 
What, then, is the best way to learn things and  retain them? The answer won't come as a huge  surprise to anyone who has been to school: focus  attention, engage working memory and then, a bit  later, actively try to recall it. 
Alan Baddeley of the University of York, UK, says  it is a good idea to test yourself in this way as it  causes your brain to strengthen the new connection.  He also suggests consciously trying to link new bits  of information to what you already know. That makes 
the connection more stable in the brain and less  likely to waste away through underuse. 
The learning process carries on for life, so why is  it so much harder to learn when we reach adulthood?  The good news is that there seems to be no  physiological reason for the slowdown. Instead, it  seems to be a lot to do with the fact that we simply  spend less time learning new stuff, and when  we do, we don't do it with the same potent mix  of enthusiasm and attention as the average child. 
Part of the problem seems to be that adults  know too much. Research by Gabriele Wulf at the  University of Nevada, Las Vegas, has shown that  adults tend to learn a physical skill, like hitting a  golf ball, by focusing on the details of the movement.  Children, however, don't sweat the details, but  experiment in getting the ball to go where they  want. When Wulf taught adults to learn more like  kids, they picked up skills much faster. 
This also seems to be true for learning information.  As adults we have a vast store of mental shortcuts  that allow us to skip over details. But we still have the  capacity to learn new things in the same way as  children, which suggests that if we could resist the  temptation to cut corners, we would probably learn a  lot more. 
A more tried-and-tested method is to keep active.  Ageing leads to the loss of brain tissue, but this  may have a lot to do with how little we hare about  compared with youngsters. With a little exercise, the  brain can spring back to life. In one study, 40 minutes  of exercise three times a week for a year increased the  size of the hippocampus - which is crucial for learning  and memory. It also improved connectivity across the  brain, making it easier for new things to stick. 
One of the brain’s most useful  features is the ability to absorb  pieces of information and make  connections between them.  Knowledge really is power: a little  can be a dangerous thing and  the more you know the better  equipped you are to deal with life. 
But what exactly is knowledge?  How are facts stored, organised  and recalled when needed? 
Knowledge obviously relies 
“There seems  to be no limit to  the knowledge  that can fit into  a brain” 
on memory - in particular the  type of memory that stores  general information about  objects, places, facts and people,  known as semantic memory.  This is the part of memory which  knows that Paris is the capital of  France, a constitutional republic  in western Europe - but not the  part which stores memories of a  weekend break there. 
 118 1 NewScientist: The Collection | The Human Brain 

 Knowledge isn’t so much about  what information you store as  how you organise it to create a  rich and detailed understanding  of the world that connects  everything you know. 
The sight of a dog, for example,  automatically activates other  bits of information about dogs:  how they look, smell, sound  and move, the fact that they are  domesticated wolves, the names  of similar dogs you know, and  your feelings about dogs. 
How the brain achieves this  gargantuan feat is far from clear.  A recent proposal is that it has  a “hub” that tags categories  to everything we know and  encounter, allowing us to connect  related things. 
In 2003, Tim Rogers, a  cognitive psychologist now at  the University of Wisconsin-  Madison, proposed the anterior  temporal lobe (ATL) as the hub.  The ATL is badly affected in  people with semantic dementia,  who progressively lose their  knowledge of the meanings of  words and objects but retain  their skills and autobiographical  memories. Experiments since  then have backed this up - when  the ATL is temporarily knocked 
out by a small electromagnetic  pulse, people lose the ability to  name objects and understand the  meanings of words. 
Rogers says that without  this system we would spend a  lot of time being confused about  how things fit together. “How  would you infer, for instance,  that when making a collage with  your kids, if you run out of sticky  tape you can use the glue stick  instead?” he says. “The tape is  not similar to the glue stick in  its shape, colour or how you use  it. You need a representation  that specifies similarity of kind.” 
The good news is that there  seems to be no limit to the  knowledge that can fit into a  brain. As far as we know no one  has ever run out of storage space. 
But it seems you can know  too much. Michael Ramscar  at Tubingen University in  Germany reckons that anyone  who lives long enough eventually  hits that point just by virtue of  a lifetime’s knowledge. He  suggests that cognitive skills  slow down with age not because  the brain withers but because it  is so full. And that - like an  overused hard drive - takes  longer to sift through. 
 6 CREATIVITY 
J. K. Rowling has said that the  idea for Harry Potter popped  into her head while she was  stuck on a very delayed train.  We have all had similar -  although probably less  lucrative - "aha" moments,  where a flash of inspiration  comes along out of the blue.  Where do they come from?  And is there any way to order  them on demand? 
Experiments led byjohn  Kounios, a neuroscientist  at Drexel University in  Philadelphia, suggest that  the reason we aren't all  millionaire authors is that  some brains come better set  up for creativity than others.  EEC measurements taken  while people were thinking  about nothing in particular  revealed naturally higher  levels of right hemisphere  activity in the temporal lobes  of people who solved  problems using insight rather  than logic. Kounios says  recent work hints that this  brain feature might be  inherited, but even if you  happen to have a more  focused, less creative brain,  there are plenty of general  tips on how to get it into  creative mode. 
Boringly, the first is to  put in the groundwork to  build up a good store of  information so that the  unconscious has something  to work with. Studies on  subliminal learning have  poured cold water on the  idea that knowledge can drift  into the brain without any  conscious effort, so it pays to  focus intently on the details 
of the problem until all the  facts are safely stored. At  this stage, anything that  helps with focus, such as  caffeine, should help. 
Once that's taken care of,  it's time to cultivate a more  relaxed, positive mood by  taking a break to do  something completely  different - like watching a  few entertaining cat videos.  Studies where people have  either watched a comedy  film or a thriller before  coming up with new ideas  have shown that a relaxed  and happy mood is far more  conducive to ideas than a  tense and anxious one. Not  only that, but it pays to turn  down the focus knob a little,  and the easiest way to do  that is to look for ideas when  your brain is too tired to  focus properly. A ZOll study  showed that morning  people had their most  creative ideas late at night,  while night owls had theirs  early in the morning. 
Mental exhaustion  might be a more realistic  state of mind than relaxation  when an important deadline  is looming, but if the ideas  are still refusing to come  there may one day be an  easier solution. Brain  stimulation studies, in which  activity was boosted in the  right temporal lobe and  suppressed in the left,  increased the rate of  problem-solving by 40 per  cent. So the stressed creative  of the future might be able to  pop on a "thinking cap" to  help those juices flow. > 
The Human Brain I NewScientist: The Collection 1 119 

INTELLIGENCE 
I ntelligence has 
always been tricky to  quantify, not least  because it seems to  involve most of the  brain and so is  almost certainly not  one ‘‘thing”. Even so, scores  across different kinds of IQ  tests have long shown that  people who do particularly  well - or badly - on one  seem to do similarly on all.  This can be crunched into a  single general intelligence  factor, or “g”, which  correlates pretty well with  academic success, income,  health and lifespan. 
So more intelligence is  clearly a good thing, but  where does it come from? 
A large part of the answer  seems to be genetics. In  1990, the first twin studies  showed that the IQ scores  of identical twins raised  apart are more similar to  each other those of non-  identical twins raised  together. Since then, a few  genes have been linked to  IQ, but all of them seem to  have a tiny effect and there  are probably thousands  involved (for more on this,  see Chapter 3, page 42). 
That doesn’t mean the  environment plays no  part, at least in childhood.  While the brain is  developing, everything  from diet to education and  stimulation plays a huge  part in developing the  brain structures needed  for intelligent thought.  Children with a bad diet  and poor education may 
never fulfil their genetic  potential. 
But even for educated  and well-fed children, the  effects of environment  wear off over time. By  adulthood, genes account  for 60 to 80 per cent of the  variance in intelligence  scores, compared with less  than 30 per cent in young  children. Like it or not, we 
“Like it ornot,  we get more iike  our dose famiiy  members as we  getotder" 
get more like our close  family members the older  we get. 
So if genes play such a  big part, is there anything  adults can do to improve  IQ? The good news is that  one type of intelligence  keeps on improving  throughout life. Most  researchers distinguish  between fluid intelligence,  which measures the ability  to reason, learn and spot  patterns, and crystallised  intelligence, the sum of all  our knowledge so far. Fluid  intelligence slows down  with age, but crystallised  intelligence doesn’t. So  while we all get a little  slower to the party as  we get older, we can rest  assured that we are still  getting cleverer. 
 IZO I NewScientist: The Collection | The Human Brain 
AND FINALY:  THE RIGHT TIME? 
 The brain is a fickle beast - at some times  as sharp as a tack, at others like a fuzzy ball  of wool. At least some of that variation can  be explained by fluctuations in circadian  rhythms, which means that, in theory, if you  do the right kind of task at the right time of day,  life should run a little more smoothly. 
The exact timing of these fluctuations  varies by about Z hours between morning  and evening types, so it is difficult to give any  one-size-fits-all advice. Nevertheless there are  a few rules that it's worth bearing in mind  whatever your natural waking time. 
It's an idea not to do too much that  involves razor-sharp focus in the first couple  of hours after waking up. Depending on how  much sleep you have had it can take anything  from 30 minutes to 4 hours to shake off  sleep inertia - also known as morning  grogginess. If you want to think creatively,  though, groggy can be good (see "Creativity",  page 119). 
If hard work can't wait, though, the good  news is that researchers have backed up  what most of us already know - a dose of  caffeine helps you shake off sleep inertia and  get on with some work. 
Another tip is to time your mental  gymnastics to coincide with fluctuations in  body temperature. Studies measuring variation  in everything from attention and verbal  reasoning to reaction times have shown that  when our core temperature dips below 37 °C  the brain isn't at its best. 
By this measure, the worst time to do  anything involving thinking is, unsurprisingly,  between midnight and 6am. It is almost as bad  in the afternoon slump between Zpm and 4pm,  which has more to do with body temperature  than lunch - studies of people who have no  lunch or just a small one have the same  problem. All in all, the best time to get stuck in  is between mid-morning and noon and then again  between 4pm and 10pm. 
There may be a way to hack the system,  though. Studies have shown that body  temperature changes and alertness also  work independently of the internal clock, so  a well-timed bit of exercise or hot shower can  work wonders. 
Competitive sports, though, are worth  leaving until the end of the day. Studies have  shown that reaction times and hand-eye  coordination get progressively better throughout  the day, reaching a peak at around 8pm. 
After that, there's time for a little more  focused energy before the body cools down,  the brain slows and there's nothing more to do  with it but dream. ■ 

“If you want to  think creatively,  morning  grogginess  can be good” 
The Human Brain I NewScientist: The Collection I IZl 

 122 1 NewSciEntistThetaiertbnlTheHi 
The origins and  purpose of sieep 
Sleep has fascinated philosophers, writers and scientists for centuries, but  research into it only began in earnest in the 1950s, Our slumbers are now much  less mysterious to us, say Derk-Jan Dijk and Raphaelle Winsky-Sommerer 
WHAT IS  SLEEP? 
Strictly speaking, the term "sleep" only applies to  animals with complex nervous systems. Nevertheless  it is possible to identify sleep-like states in  invertebrates, which allows us define sleep more  broadly. These include cycles of rest and activity, a  stereotypical body position, lack of responsiveness  and compensatory rest after sleep deprivation.  Insects in particular have a state very similar to  sleep, as do scorpions and some crustaceans. 
Even microorganisms, which lack a nervous  system, have daily cycles of activity and inactivity 
driven by internal body clocks known as circadian  clocks. The origins of sleep might therefore date  back to the dawn of life about 4 billion years ago,  when microorganisms changed their behaviour in  response to night and day. 
Some researchers consider sleep to be part of a  continuum of inactive states found throughout the  animal kingdom. Once we understand exactly what  aspects of an organism benefit from these states,  we may be able to provide a meaningful answer to  the question of whether simple organisms sleep. 


HALF AWAKE 
Sleep feels like an on-or-off condition, but  brains can be awake and asleep at the same  time. This phenomenon is well known in  dolphins and seals - animals that can sleep  "uni-hemisphehcally": one half of their brain  is asleep while the other half shows electrical  activity characteristic of wakefulness. 
A study in rats found that after prolonged  wakefulness, some neurons go offline and display  sleep-like activity. Tellingly, this mosaic brain state is  accompanied by occasional lapses in attention. 
Sleep researchers are investigating if human  and other animal sleep is a "global" state or  whether the process of sleep can, to some extent,  be regulated locally. There is mounting evidence  for the latter. For example, the most active brain  regions during wakefulness subsequently undergo  deeper sleep for longer. 
This localised view of sleep could lead to a better  understanding of cases when wakefulness intrudes  into sleep, such as in sleep-talking, sleepwalking  and episodes of insomnia in which people report 
being awake all night even though recording  brainwaves (see "Slumber cycles", below) from a  single location suggests they have been asleep. 
It also promises to explain how sleep can intrude  into wakefulness, such as during lapses of attention  when we are sleep-deprived. These "micro sleeps"  can be particularly dangerous when driving and  various ways to detect them have been developed,  for instance by monitoring how a car moves relative  to white lines on roads or analysing the movements  of the eyes for signs of sleepiness. 
SLUMBER CYCLES 
During sleep, complex changes occur in the  brain. These can be observed with an  electroencephalogram (EEC), which measures the  brain's electrical activity and associated brainwaves. 
After lying awake for 10 minutes or so we enter  non-rapid eye movement sleep or NREM sleep. NREM  sleep is divided into three stages, NREMl, NREM2 and  NREM3, based on subtle differences in EEC patterns.  Each stage is considered progressively "deeper". 
After cycling through the NREM stages we enter  rapid-eye-movement or REM sleep. The EEC during  REM sleep is similar to wakefulness or drowsiness. 
It is during this stage that many of our dreams occur. 
Each cycle lasts for about 1.5 hours and a night's  sleep usually consists of five or six cycles. 
In addition to changes in brain activity, sleep is  also characterised by a reduction in heart rate of  about 10 beats per minute, a fall in core body  temperature of 1 °C to 1.5 °C as well as a reduction  in movement and sensation. 
REASONS FOR REST 
There are many explanations for sleep, ranging  from keeping us out of harm's way to saving energy,  regulating emotions, processing information and  consolidating memory. Each has strengths - and  weaknesses too. Rather than seek a single,  universal function of sleep we might do better to  study its influence at each level of biological  organisation. 
At the level of the whole organism, a primary  function of sleep may be the regulation of  autonomic nervous activity such as heart rate - sleep  disorders are often associated with dysfunction of  § the autonomic nervous system, such as an abnormal  i heartbeat. At the level of the brain, it may support  I memory consolidation by reducing the amount of  I information travelling through the central nervous 
I system. Studies in mice have found that sleep 
< 
< promotes the formation of new connections  ^ between brain cells, which might be how sleep 
Sleep scientists break sleep into four distinct stages. 
A typical night's sleep involves several cycles, which includes both REM and non-REM (NREM) sleep 
EEG TRACE 
Alpha brainwaves (relaxation) 
First 
CYCLE 
Second Third Fourth Fifth 
AWAKE 
Rapid brain activity 
Light sleep, irregular, shallow brainwaves 
Bursts of rhythmic brain activity . 
called sleep spindles and k-complexes ^ 
Slow, deep "delta" brainwaves ^ 
REM 
NREMl 
NREM 2 
NREM 3 
 2 3 4 5 6 
Time (hours) 
helps to consolidate memory. However, memory  consolidation occurs when we are awake too. 
At the level of nerve cells, sleep alters firing rates  of neurons and also changes the temporal  distribution and synchronisation of firing across  networks of cells, which may alter their connectivity.  The regulation of nerve-cell connectivity, called  synaptic homeostasis, can help prevent the nervous  system from becoming overloaded. Support for this  idea has come from studies of fruit flies. 
One neglected role of sleep in humans is social  isolation. As social animals, we may need sleep to  consolidate the rules and insights of our complex  social lives. 
PET scans show differences in brain activity  between wakefulness and various sleep  states. Activity is in red, inactivity in blue  (also see "Slumber cycles", above) 
Awake REM 
 Non-REMl Non-REM3 
 The Human Brain I NewScientist: The Collection 1 123 

 124 1 NewScientist: The Collection | The Human Brain 
 interpretation of dreams is the 
I royal road to a knowledge of the 
I unconscious activities of the mind.” 
So wrote Sigmund Freud in his 1900 classic  The Interpretation of Dreams. He saw this idea  as a ‘‘once in a lifetime” insight, and for much  of the 20th century the world agreed. Across  the globe, and upon countless psychoanalysts'  couches, people recounted their dreams in  the belief that they contained coded messages  about repressed desires. Dreams were no  longer supernatural communications or  divine interventions - they were windows  into the hidden self. 
Today we interpret dreams quite differently,  and use far more advanced techniques than  simply writing down people's recollections. 
In sleep laboratories, dream researchers hook  up volunteers to EEGs and fMRl scanners and  awaken them mid-dream to record what they  were dreaming. Still tainted by association  with psychoanalysis, it is not a field for the  faint-hearted. “To say you're going to study  dreams is almost academic suicide,” says  Matt Walker at the University of California,  Berkeley. Nevertheless, what researchers are  finding will make you see your dreams in a  whole new light. 
Modern neuroscience has pushed Freud's  ideas to the sidelines and has taught us  something far more profound about  dreaming. We now know that this peculiar  form of consciousness is crucial to making  us who we are. Dreams help us to consolidate  our memories, make sense of our myriad  experiences and keep our emotions in check. 
Changing patterns of electrical activity tell  us that the sleeping brain follows 90-minute  cycles, each consisting of five stages - two  of light sleep at the start, then two of deep  sleep, followed by a stage of REM, or rapid eye  movement sleep (see diagram, page 127). There  is no characteristic pattern of brain activity  Memory traces from our corresponding to dreaming, but as far as we  waking experiences are know all healthy people do it. And while  replayed in our dreams dreaming is commonly associated with REM  sleep, during which it occurs almost all of the  time, researchers have known since the late  1960s that it can also occur in non-REM sleep -  though these dreams are different. Non-REM  dreams tend to be sparse and more thought-  like, often without the complexity, length and  vivid hallucinatory quality of REM dreams. 
Despite their differences, both types of  dreams seem to hold a mirror to our waking  lives. Dreams often reflect recent learning  experiences and this is particularly true at  the start of a night's sleep, when non-REM  dreaming is very common. Someone who 
has just been playing a skiing arcade game  may dream of skiing, for example. The link  between waking experience and non-REM  sleep has also been observed in brain  scanning studies. Pierre Maquet at the  University of Liege, Belgium, looked at the  later stages of non-REM sleep and found  that the brains of volunteers replayed the  same patterns of neural activity that had  earlier been elicited by waking experiences.  Many REM-sleep dreams also reflect elements  of experiences from the preceding day, but  the connection is often more tenuous - so  someone who has been playing a skiing game  might dream of rushing through a forest or  falling down a hill. 
Sleep on it 
But we do not simply replay events while we  dream, we also process them, consolidating  memories and integrating information for  future use. Robert Stickgold of Harvard Medical  School in Boston recently found that people  who had non-REM dreams about a problem  he had asked them to tackle subsequently  performed better on it. Likewise, REM sleep  has been linked with improved abilities on  video games and visual perception tasks, and  in extracting meaning from a mass of  information. 
“It's clear that the brain does an immense  amount of memory processing while we  sleep - and it certainly isn't mere coincidence  that while our brain is sorting out these  memories and how they fit together, we're  dreaming,” says Stickgold. He suspects that  the two types of dream states have different  functions for memory, although what these  functions are is a matter of debate. Non-REM  dreaming might be more important for  stabilising and strengthening memories,  Stickgold suggests, while REM dreaming  reorganises the way a memory is stored  in the brain, allowing you to compare and  integrate a new experience with older ones. 
Ian Born and Susanne Diekelmann, now  both at the University of Tubingen in  Germany, however, have looked at the same  evidence and come to the opposite  conclusion - that REM sleep supports  the strengthening of a new memory,  while non-REM sleep is for higher-level  consolidation of memories. “I think this  means that we're still lost when it comes to  understanding the role of different sleep  stages in memory,” says Stickgold. 
Also unclear is how central is the role  of dreams in memory formation. During > 
The Human Brain | NewScientist: The Collection! 1Z5 
"While you are dreaming, your brain is literally  reshaping itself, so dreams play a key role in  making you who you are" 
dreaming is certainly not the only time  our brains consolidate memories. For  example, when we daydream certain areas  of the brain, called the default network,  become active. We now know this network  is involved in memory processing and many  of the same brain regions are active during  REM sleep. What’s more, daydreaming, like  REM dreaming, can improve our ability to  extract meaning from information and have  creative insights. 
Does this mean we don’t actually need  dream sleep to process memories? Not  necessarily, says Walker, who points out  that the way new memories are replayed in  the brain is different in daydreaming and  dreaming. Rat studies show that the reruns  happen in reverse when the animals are awake  and forwards when they are sleeping. No one  is quite sure what this difference means for  memory processing, but Walker believes  it shows that daydreaming is not simply a 
diluted version of sleep dreaming. Maquet  agrees. “Different brain states may all have  somewhat different functions for memory.  Memory consolidation is probably organised  in a cascade of cellular events that have to  occur serially,” he says - some while you are  awake, and then some while you are asleep. 
Even if dreaming is crucial for memory.  Walker for one does not see this as its main  function. “1 think the evidence is mounting in  favour of dream sleep acting as an emotional  homeostasis: basically, rebalancing the  emotional compass in a good way at the  biological level,” he says. Everyone knows  how a short nap can transform a  cantankerous 2-year-old and Walker has  shown something similar in adults. He  found that a nap that includes REM dreaming  mitigates a normal tendency in adults to  become more sensitive to angry or fearful  faces over the course of a day, and makes  people more receptive to happy faces. 
The interpretation  of nightmares 
Antii Revonsuo enjoys his  nightmares. "At least in  hindsight " he qualifies, "as  though they were good horror  movies where you don't know  it's a movie until it's over." But  then Revonsuo, at the University  of Turku in Finland, thinks  that nightmares are the main  biological reason for why we  dream - they allow us to simulate  scary encounters, and so be  better prepared for them in  our waking life. 
"The theory predicts correctly  several features of our dream  content," says Revonsuo. For  example, he and his colleagues  have found that about two-thirds  of the dreams of healthy adults  involve at least one threat. About  40 per cent of these take the  form of aggressive encounters -  running away from an attacker  or getting into a fight. Such  encounters are higher among  children, accounting for over half  of threat dreams in Finnish kids  and three-quarters among 
traumatised Palestinian children. 
Revonsuo argues that  children's dreams are closer to  our evolutionarily original form  of dreaming because children  haven't yet had a chance to adjust  to the modern environment. He  has found that between 40 and  50 per cent of children's dreams  contain animal characters, often  as enemies, which is similar  to the instance among adult  hunter-gatherers. The figure is  just 5 per cent in Western adults.  "I don't think any other dream  theory has made such specific  predictions and shown that  they hold," he says. 
It is a neat idea, but Robert  Stickgold at Harvard Medical  School in Boston cannot believe  that's all there is to dreaming. 
"I think Revonsuo has made the  same mistake as Freud - which is  to limit dreaming's functionality. 
I think dreaming is absolutely  about threat rehearsal some  of the time. But it's absolutely  about other things, too." 
Walker has also found that sleep, and  REM sleep in particular, strengthens  negative emotional memories. This might  sound like a bad thing - but if you don’t  remember bad experiences you cannot  learn from them. In addition, both he and  Stickgold think that reliving the upsetting  experience in the absence of the hormonal  rush that accompanied the actual event  helps to strip the emotion from the memory,  making it feel less raw as time goes on. So  although dreams can be highly emotional.  Walker believes they gradually erode the  emotional edges of memories. In this way,  REM dreams act as a kind of balm for the  brain, he says. In people with post-traumatic  stress disorder, this emotion-stripping  process seems to fail for some reason, so  that traumatic memories are recalled in all  their emotional detail - with crippling  psychological results. 
As with memory processing, REM and  non-REM dreaming may play different  psychological roles. Patrick McNamara of  Boston University has found that people  woken at different sleep stages give different  reports of their dreams. REM dreams contain  more emotion, more aggression and more  unknown characters, he says, while non-REM  dreams are more likely to involve friendly  encounters. This has led him to speculate that  non-REM dreams help us practise friendly  encounters while REM dreams help us to  rehearse threats (see “The interpretation  of nightmares”, left). 
So what do they mean? 
All this suggests that we couldn’t function  properly without dreaming, but it doesn’t  answer the perennially intriguing question:  what do dreams actually mean? 
For some sleep researchers, the answer is  simple - and disappointing. Born argues that  dreams themselves have no meaning, they  are just an epiphenomenon, or side effect, of  brain activity going on during sleep, and it is  this underlying neuronal activity, rather than  the actual dreams, that is important. Walker  finds it hard to disagree. “I don’t want to  believe it. But I don’t see large amounts of  evidence to support the idea [that dreams  themselves are significant],” he says. 
Those researchers who refuse to accept  the notion that the content of dreams is  unimportant point to work by Rosalind  Cartwright of Rush University, Chicago. In  a long series of studies starting in the 1960s,  she followed people who had gone through 
1Z6 1 NewScientist: The Collection I The Human Brain 
 A strange form of  consciousness we  cannot live without 
Dream on 
A typical night's sleep involves five cycles and during each one we pass through several stages of varying  sleep depth. Almost all REM sleep is filled with dreams but dreaming can also occur in the other stages 
1st cycle 2nd cycle 3rd cycle 4th cycle 5th cycle 
 012345678 
Time (hours) 
divorces, separations and bereavements.  Those who dreamed most about these events  later coped better, suggesting that their  dreams had helped. “Cartwright’s work  provides some of the most solid evidence  that dreaming serves a function,” says Erin  Wamsley at Furman University in Greenville,  South Carolina. There is no hard data showing  that dreaming is not an epiphenomenon, she  admits, but the same could be said about  waking consciousness. 
In fact, Wamsley’s own research hints  that the form and function of a dream are  connected. She worked with Stickgold on  the study which found that non-REM dreams  boost people’s performance on a problem.  Their volunteers were given an hour’s training  on a complex maze then either allowed a  90-minute nap or kept awake. The dreamers 
subsequently showed bigger improvements,  but the biggest gains of all were in people who  dreamed about the maze. It didn’t seem to  matter that the content of these dreams was  obtuse. One volunteer, for example, reported  dreaming about the maze with people at  checkpoints - although there were no people  or checkpoints in the real task - and then  about bat caves that he had visited a few years  earlier. Stickgold didn’t expect this to improve  the volunteer’s ability to navigate the maze,  “and yet this person got phenomenally  better”. 
He points out that the dream content is  consistent with the idea that during dreaming  memories are filed with other past experiences  for future reference. “Dreams have to be  connected in a meaningful, functional way to  improvements in memory - not just be an 
epiphenomenon,” he says. “1 say this with  fervent emotion, which is what I use when I  don’t have hard data.” 
Such evidence may one day be forthcoming,  though. In the past, there has been no  objective way to record what someone is  dreaming, but that is changing. In 2008,  Yukiyasu Kamitani at the ATR Brain  Information Communication Research  Laboratory in Kyoto, japan, and colleagues  used fMRI scans to decode and then recreate  scenes that volunteers were picturing in  their mind while awake. To see if they could  do the same thing with people’s dreams, in  a later study the team repeatedly woke  volunteers as they slept in a scanner and  asked them to describe their dreams. Using  that information, they were able to categorise  what certain patterns on fMRI scans meant  and tell with 60 per cent accuracy what kinds  of things people were dreaming about - for  instance whether they were dreaming about a  man or woman, or certain types of objects,  such as a car. 
Some may think all this peering and  prodding at our dream world is taking away its  magic, but the researchers don’t see it that  way. While you are dreaming, your brain  literally reshapes itself by rewiring and  strengthening connections between neurons.  So although dreams do not reveal the secret  you, they do play a key role in making you who  you are. “The mystery and the wonder of  dreams is untouched by the science,” says  Stickgold. “It just helps us appreciate better  how amazing they really are.” ^ 
The Human Brain I NewScientist: The Collection 1 127 
 COMING SOON 
 NewSclentist 
THE COLLECTION 
VOLUME TWO / ISSUE TWO 
MEDIC A 
FROM 
THERE'S A REVOEUTION GOING ON 
FIND OUT ABOUT TOMORROW'S  MEDICAL BREAKTHROUGHS TODAY 
ON SALE 13 MAY 
To buy back issues of New Scientist: The Collection,  visit newscientist. com/TheCollection 
One: The Big Questions / Two: The Unknown Universe 
Three: The Scientific Guide to a Better You / Four: The Human Story 
Morning pick me up Lasting pick me up 
 ikM M ^ l4Wt  4^KwrniB 
Hu* la Ld 
mmmi 
t4if HHMtNa iAfSt^r 
tiqyi 
. K-"\.V • ■/' ■( ^ 
 A USER'S GUIDE 
ukl:biiGjnsat«o 
t ^WMlV 
 A thirst for  knowledge? 
Each week New Scientist provides everything  you need to satisfy a curious mind 
Covering the entire spectrum of science and  technology and why they matter, you’ll find a whole  world of knowledge - including, of course, why your  mind is curious in the first place 
Subscribe and save 
Visit newscientist.com/8046 or call  +61 2 9422 8559 and quote offer 8046 
NewScientist 
 